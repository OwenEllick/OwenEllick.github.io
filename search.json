[
  {
    "objectID": "Beermarkets.html",
    "href": "Beermarkets.html",
    "title": "Beer Markets Blog",
    "section": "",
    "text": "Decoding Beer Prices: A Linear Regression Journey into Market Dynamics\n\n\n\nimage.png\n\n\n\n\nIntroduction\nEver wondered what truly dictates the price of your favorite beer? Is it the brand, the size, or perhaps the market you‚Äôre in? In this post, we‚Äôll embark on a journey into the world of beer markets, using linear regression to unravel the complex factors that influence beer prices. We‚Äôll explore how different models capture the nuances of this market, from simple additive effects to intricate interactions between brands, sizes, and promotions.\n\n\nLoading the Data\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when, log\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.sql.functions import pow\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler,\n    and inserts a dashed horizontal line after the Intercept row. The table includes separate columns\n    for the 95% confidence interval lower and upper bounds for each coefficient (computed at the 5% significance level)\n    and an \"Observations\" row (using model.summary.numInstances) above the R¬≤ row.\n    The RMSE row is placed as the last row.\n\n    The columns are ordered as:\n        Metric | Value | Significance | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    For the \"Value\", \"Std. Error\", \"95% CI Lower\", and \"95% CI Upper\" columns, commas are inserted every three digits,\n    with 3 decimal places (except for Observations which is formatted as an integer with commas).\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Extract coefficients and standard errors as NumPy arrays\n    coeffs = model.coefficients.toArray()\n\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element)\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Compute t-statistics for feature coefficients (t = beta / SE(beta))\n    # t_stats = coeffs / std_errors\n    t_stats = model.summary.tValues\n\n    # Degrees of freedom: number of instances minus number of predictors minus 1 (for intercept)\n    df = model.summary.numInstances - len(coeffs) - 1\n\n    # Compute the t-critical value for a 95% confidence interval (two-tailed, 5% significance)\n    t_critical = stats.t.ppf(0.975, df)\n\n    # Compute two-tailed p-values for each feature coefficient\n    # p_values = [2 * (1 - stats.t.cdf(np.abs(t), df)) for t in t_stats]\n    p_values = model.summary.pValues\n\n    # Function to assign significance stars based on p-value\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build the table rows.\n    # Order: Metric, Value, Significance, Std. Error, p-value, 95% CI Lower, 95% CI Upper.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n        table.append([\n            \"Beta: \" + feature,       # Metric name\n            beta,                     # Beta estimate (Value)\n            significance_stars(p),    # Significance stars\n            se,                       # Standard error\n            p,                        # p-value\n            ci_lower,                 # 95% CI lower bound\n            ci_upper                  # 95% CI upper bound\n        ])\n\n    # Compute and add the intercept row with its SE, p-value, significance, and CI (if available)\n    if intercept_se is not None:\n        intercept_t = model.intercept / intercept_se\n        intercept_p = 2 * (1 - stats.t.cdf(np.abs(intercept_t), df))\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_se = \"\"\n        intercept_p = \"\"\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n\n    table.append([\n        \"Intercept\",\n        model.intercept,\n        intercept_sig,\n        intercept_se,\n        intercept_p,\n        ci_intercept_lower,\n        ci_intercept_upper\n    ])\n\n    # Append overall model metrics:\n    # Insert an Observations row using model.summary.numInstances,\n    # then an R¬≤ row, and finally the RMSE row as the last row.\n    table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table.\n    # For the \"Value\" (index 1), \"Std. Error\" (index 3), \"95% CI Lower\" (index 5), and \"95% CI Upper\" (index 6) columns,\n    # format with commas and 3 decimal places, except for Observations which should be an integer with commas.\n    # For the p-value (index 4), format to 3 decimal places.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                # Format Observations as integer with commas, no decimals.\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if i in [1, 3, 5, 6]:\n                    formatted_row.append(f\"{item:,.3f}\")\n                elif i == 4:\n                    formatted_row.append(f\"{item:.3f}\")\n                else:\n                    formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Generate the table string using tabulate.\n    table_str = tabulate(\n        formatted_table,\n        headers=[\"Metric\", \"Value\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"],\n        tablefmt=\"pretty\",\n        colalign=(\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n    )\n\n    # Insert a dashed line after the Intercept row for clarity.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(MODEL, ASSEMBLER))\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\nbeer_markets = pd.read_csv(\n  'https://bcdanl.github.io/data/beer_markets_all_cleaned.csv'\n)\nprint(beer_markets)\n\n\n\nfiltered_df = beer_markets[\n    beer_markets['container'].isin(['CAN', 'NON REFILLABLE BOTTLE'])]\nprint(filtered_df)\n\n\ndf = spark.createDataFrame(filtered_df)\n\ndtrain, dtest= df.randomSplit([0.67, 0.33], seed=42)\n\n\n\nModel Setups\n\ndtrain = dtrain.withColumn('log_price_floz', log(col('price_floz'))).withColumn('log_beer_floz', log(col('beer_floz')))\ndtest = dtest.withColumn('log_price_floz', log(col('price_floz'))).withColumn('log_beer_floz', log(col('beer_floz')))\n\n\nmarkets = [row.market for row in dtrain.select('market').distinct().collect()]\nmarkets.remove('BUFFALO-ROCHESTER')\nfor market in markets:\n    dtrain = dtrain.withColumn(f'market_{market}', when(col('market') == market, 1).otherwise(0))\n    dtest = dtest.withColumn(f'market_{market}', when(col('market') == market, 1).otherwise(0))\n\n\nbrands = [row.brand for row in dtrain.select('brand').distinct().collect()]\nbrands.remove('BUD_LIGHT')\nfor brand in brands:\n    dtrain = dtrain.withColumn(f'brand_{brand}', when(col('brand') == brand, 1).otherwise(0))\n    dtest = dtest.withColumn(f'brand_{brand}', when(col('brand') == brand, 1).otherwise(0))\n\n\ndtrain = dtrain.withColumn('container_CAN', when(col('container') == 'CAN', 1).otherwise(0))\ndtest = dtest.withColumn('container_CAN', when(col('container') == 'CAN', 1).otherwise(0))\n\ndtrain = dtrain.withColumn('promo', col('promo').cast('integer'))\ndtest = dtest.withColumn('promo', col('promo').cast('integer'))\n\n\n\nModel 1: The Basic Additive Model\n\nmarket_cols = [col_name for col_name in dtrain.columns if col_name.startswith('market_') and col_name != 'market_BUFFALO-ROCHESTER']\nbrand_cols = [col_name for col_name in dtrain.columns if col_name.startswith('brand_') and col_name != 'brand_BUD_LIGHT']\n\nassembler1 = VectorAssembler(inputCols=market_cols + brand_cols + ['container_CAN', 'log_beer_floz'], outputCol='features')\ndtrain1 = assembler1.transform(dtrain)\ndtest1 = assembler1.transform(dtest)\n\nmodel1 = LinearRegression(featuresCol='features', labelCol='log_price_floz').fit(dtrain1)\npredictions1 = model1.transform(dtest1)\n\nprint(\"Model 1 Summary:\")\nprint(f\"  Coefficients: {model1.coefficients}\")\nprint(f\"  Intercept: {model1.intercept}\")\nprint(f\"  R-squared: {model1.summary.r2}\")\nprint(f\"  RMSE: {model1.summary.rootMeanSquaredError}\")\n\n\nInterpretation: This model estimates the baseline log(price_per_floz) for a specific reference market and brand. Each coefficient represents the average change in log(price_per_floz) for a one-unit change in the corresponding variable.\nLimitations: It doesn‚Äôt account for interactions between variables, like how beer size might affect different brands differently.\n\n\n\nModel 2: Interaction with Brand and Size\n\ninteraction_cols_beer_floz = [f'{brand}_log_beer_floz' for brand in brand_cols] # Change here to use the complete name\nfor brand in brand_cols:\n    dtrain = dtrain.withColumn(f'{brand}_log_beer_floz', col(brand) * col('log_beer_floz')) # Use col(brand) instead\n    dtest = dtest.withColumn(f'{brand}_log_beer_floz', col(brand) * col('log_beer_floz')) # Use col(brand) instead\n\nassembler2 = VectorAssembler(inputCols=market_cols + brand_cols + ['container_CAN', 'log_beer_floz'] + interaction_cols_beer_floz, outputCol='features')\ndtrain2 = assembler2.transform(dtrain)\ndtest2 = assembler2.transform(dtest)\n\nmodel2 = LinearRegression(featuresCol='features', labelCol='log_price_floz').fit(dtrain2)\npredictions2 = model2.transform(dtest2)\n\nprint(\"\\nModel 2 Summary:\")\nprint(f\"  Coefficients: {model2.coefficients}\")\nprint(f\"  Intercept: {model2.intercept}\")\nprint(f\"  R-squared: {model2.summary.r2}\")\nprint(f\"  RMSE: {model2.summary.rootMeanSquaredError}\")\n\n\nInterpretation: This model allows the effect of beer size on price to vary across brands, capturing potential differences in price elasticity.\n\n\n\nModel 3: The Full Interaction Model\n\ninteraction_cols_promo_beer_floz = [f'{brand}_promo_log_beer_floz' for brand in brand_cols]\ninteraction_cols_promo = [f'{brand}_promo' for brand in brand_cols]\n\ndtrain = dtrain.withColumn('promo_log_beer_floz', col('promo') * col('log_beer_floz'))\ndtest = dtest.withColumn('promo_log_beer_floz', col('promo') * col('log_beer_floz'))\n\nfor brand in brand_cols:\n    dtrain = dtrain.withColumn(f'{brand}_promo', col(brand) * col('promo')) # Use the complete brand column name\n    dtest = dtest.withColumn(f'{brand}_promo', col(brand) * col('promo')) # Use the complete brand column name\n    dtrain = dtrain.withColumn(f'{brand}_promo_log_beer_floz', col(brand) * col('promo') * col('log_beer_floz')) # Use the complete brand column name\n    dtest = dtest.withColumn(f'{brand}_promo_log_beer_floz', col(brand) * col('promo') * col('log_beer_floz')) # Use the complete brand column name\n\nassembler3 = VectorAssembler(inputCols=market_cols + brand_cols + ['container_CAN', 'log_beer_floz', 'promo_log_beer_floz'] + interaction_cols_beer_floz + interaction_cols_promo + interaction_cols_promo_beer_floz, outputCol='features')\ndtrain3 = assembler3.transform(dtrain)\ndtest3 = assembler3.transform(dtest)\n\nmodel3 = LinearRegression(featuresCol='features', labelCol='log_price_floz').fit(dtrain3)\npredictions3 = model3.transform(dtest3)\n\nprint(\"\\nModel 3 Summary:\")\nprint(f\"  Coefficients: {model3.coefficients}\")\nprint(f\"  Intercept: {model3.intercept}\")\nprint(f\"  R-squared: {model3.summary.r2}\")\nprint(f\"  RMSE: {model3.summary.rootMeanSquaredError}\")\n\n\nInterpretation: This model captures the most complex relationships, including how promotions affect price across different brands and sizes.\nChallenges: The increased complexity can make interpretation more difficult and increase the risk of overfitting.\n\n\n\nConclusion\n\nThis analysis demonstrates the power of linear regression in decoding the intricate factors that drive beer prices. By building and comparing different models, we gain a deeper understanding of the beer market, revealing the nuanced interplay between brands, sizes, markets, and promotions."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "DANL Project",
    "section": "",
    "text": "Link to Written Report HERE"
  },
  {
    "objectID": "project.html#summary-statistics",
    "href": "project.html#summary-statistics",
    "title": "DANL Project",
    "section": "2.1 Summary Statistics üèÄ",
    "text": "2.1 Summary Statistics üèÄ\n\nNBAsal &lt;- read_csv(\"nba_salaries.csv\")\n\n\n  \n\n\nskim(NBAsal) %&gt;% \n  select(-n_missing)\n\n\nData summary\n\n\nName\nNBAsal\n\n\nNumber of rows\n467\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n28\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nPlayer Name\n1\n7\n24\n0\n467\n0\n\n\nPosition\n1\n1\n5\n0\n9\n0\n\n\nTeam\n1\n3\n7\n0\n75\n0\n\n\nPlayer-additional\n1\n7\n9\n0\n467\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n‚Ä¶1\n1.00\n233.00\n134.96\n0.0\n116.50\n233.00\n349.50\n466.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nSalary\n1.00\n8416598.75\n10708118.05\n5849.0\n1782621.00\n3722040.00\n10633543.50\n48070014.0\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nAge\n1.00\n25.82\n4.28\n19.0\n23.00\n25.00\n29.00\n42.0\n‚ñá‚ñá‚ñÉ‚ñÇ‚ñÅ\n\n\nGP\n1.00\n48.23\n24.81\n1.0\n31.00\n55.00\n68.50\n83.0\n‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñá\n\n\nGS\n1.00\n22.65\n27.09\n0.0\n1.00\n8.00\n45.50\n83.0\n‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n\n\nMP\n1.00\n19.87\n9.55\n1.8\n12.50\n19.20\n28.30\n41.0\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÉ\n\n\nFG\n1.00\n3.35\n2.46\n0.0\n1.60\n2.70\n4.30\n11.2\n‚ñá‚ñá‚ñÇ‚ñÇ‚ñÅ\n\n\nFGA\n1.00\n7.12\n5.02\n0.0\n3.30\n5.80\n9.40\n22.2\n‚ñá‚ñá‚ñÉ‚ñÇ‚ñÇ\n\n\nFG%\n1.00\n0.47\n0.11\n0.0\n0.42\n0.46\n0.51\n1.0\n‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñÅ\n\n\n3P\n1.00\n1.00\n0.88\n0.0\n0.30\n0.80\n1.50\n4.9\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\n3PA\n1.00\n2.79\n2.26\n0.0\n1.00\n2.40\n4.15\n11.4\n‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ\n\n\n3P%\n0.97\n0.33\n0.13\n0.0\n0.29\n0.34\n0.39\n1.0\n‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\n2P\n1.00\n2.36\n1.99\n0.0\n0.90\n1.70\n3.30\n10.5\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n\n\n2PA\n1.00\n4.33\n3.57\n0.0\n1.80\n3.20\n5.85\n17.8\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\n2P%\n0.99\n0.53\n0.14\n0.0\n0.49\n0.54\n0.59\n1.0\n‚ñÅ‚ñÅ‚ñá‚ñÇ‚ñÅ\n\n\neFG%\n1.00\n0.53\n0.10\n0.0\n0.49\n0.54\n0.58\n1.0\n‚ñÅ‚ñÅ‚ñá‚ñÇ‚ñÅ\n\n\nFT\n1.00\n1.44\n1.57\n0.0\n0.50\n0.90\n1.85\n10.0\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nFTA\n1.00\n1.85\n1.90\n0.0\n0.60\n1.20\n2.35\n12.3\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nFT%\n0.95\n0.75\n0.15\n0.0\n0.69\n0.77\n0.84\n1.0\n‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÜ\n\n\nORB\n1.00\n0.87\n0.75\n0.0\n0.40\n0.70\n1.10\n5.1\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nDRB\n1.00\n2.66\n1.71\n0.0\n1.40\n2.30\n3.50\n9.6\n‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\nTRB\n1.00\n3.53\n2.28\n0.0\n1.90\n3.00\n4.50\n12.5\n‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ\n\n\nAST\n1.00\n2.11\n1.96\n0.0\n0.80\n1.40\n2.90\n10.7\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nSTL\n1.00\n0.61\n0.40\n0.0\n0.30\n0.60\n0.80\n3.0\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\n\n\nBLK\n1.00\n0.38\n0.36\n0.0\n0.10\n0.30\n0.50\n2.5\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nTOV\n1.00\n1.11\n0.83\n0.0\n0.50\n0.90\n1.50\n4.1\n‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ\n\n\nPF\n1.00\n1.70\n0.78\n0.0\n1.20\n1.70\n2.20\n5.0\n‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÅ\n\n\nPTS\n1.00\n9.13\n6.91\n0.0\n4.10\n7.10\n11.70\n33.1\n‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÅ"
  },
  {
    "objectID": "project.html#top-players-and-points-per-game",
    "href": "project.html#top-players-and-points-per-game",
    "title": "DANL Project",
    "section": "2.2 Top Players and Points Per Game üèÄ",
    "text": "2.2 Top Players and Points Per Game üèÄ\nThe following shows the top 10 players during the NBA 2022-2023 season based on the number of points per game.\n\ntop_10_players &lt;- NBAsal %&gt;% \n  group_by(PTS) %&gt;% \n  arrange(desc(PTS)) %&gt;% \n  head(10) %&gt;% \n  relocate(PTS,.before = Position)\n  \ntop_10_players\n\n# A tibble: 10 √ó 32\n# Groups:   PTS [10]\n    ...1 `Player Name` Salary   PTS Position   Age Team     GP    GS    MP    FG\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    27 Joel Embiid   3.36e7  33.1 C           28 PHI      66    66  34.6  11  \n 2     9 Damian Lilla‚Ä¶ 4.25e7  32.2 PG          32 POR      58    58  36.3   9.6\n 3    34 Shai Gilgeou‚Ä¶ 3.09e7  31.4 PG          24 OKC      68    68  35.5  10.4\n 4     8 Giannis Ante‚Ä¶ 4.25e7  31.1 PF          28 MIL      63    63  32.1  11.2\n 5    37 Jayson Tatum  3.04e7  30.1 SF          24 BOS      74    74  36.9   9.8\n 6     0 Stephen Curry 4.81e7  29.4 PG          34 GSW      56    56  34.7  10  \n 7     4 Kevin Durant  4.41e7  29.1 PF          34 BRK/‚Ä¶    47    47  35.6  10.3\n 8     3 LeBron James  4.45e7  28.9 PF          38 LAL      55    54  35.5  11.1\n 9    33 Donovan Mitc‚Ä¶ 3.09e7  28.3 SG          26 CLE      68    68  35.8  10  \n10    25 Devin Booker  3.38e7  27.8 SG          26 PHO      53    53  34.6   9.9\n# ‚Ñπ 21 more variables: FGA &lt;dbl&gt;, `FG%` &lt;dbl&gt;, `3P` &lt;dbl&gt;, `3PA` &lt;dbl&gt;,\n#   `3P%` &lt;dbl&gt;, `2P` &lt;dbl&gt;, `2PA` &lt;dbl&gt;, `2P%` &lt;dbl&gt;, `eFG%` &lt;dbl&gt;, FT &lt;dbl&gt;,\n#   FTA &lt;dbl&gt;, `FT%` &lt;dbl&gt;, ORB &lt;dbl&gt;, DRB &lt;dbl&gt;, TRB &lt;dbl&gt;, AST &lt;dbl&gt;,\n#   STL &lt;dbl&gt;, BLK &lt;dbl&gt;, TOV &lt;dbl&gt;, PF &lt;dbl&gt;, `Player-additional` &lt;chr&gt;\n\n\nComments:\n\nJoel Embiid, Damian Lillard, Shai Gilgeous-Alexander, Giannis Antetokounmpo, Jayson Tatum, Stephen Curry, Kevin Durant, Lebron James, Donovan Mitchell, and Devin Booker are the top 10 players based on points per game for the 2022-2023 NBA season"
  },
  {
    "objectID": "project.html#points-per-game-and-salary",
    "href": "project.html#points-per-game-and-salary",
    "title": "DANL Project",
    "section": "2.3 Points Per Game and Salary ‚õπÔ∏è‚Äç‚ôÇÔ∏è",
    "text": "2.3 Points Per Game and Salary ‚õπÔ∏è‚Äç‚ôÇÔ∏è\nProvide both ggplot codes and a couple of sentences to describe the relationship between salary and points per game.\n\nggplot(NBAsal, aes(x= PTS, y= Salary))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\")\n\n\n\n\n\n\n\n\nComments:\n\nThe plot above shows us that during 2022-2023 season players that had a higher average points per game generally had a higher salary than players that had a lower average points per game.\nThis implies that players that score more points per game are generally considered more valuable to a franchise which is why they are paid more."
  },
  {
    "objectID": "project.html#top-fg",
    "href": "project.html#top-fg",
    "title": "DANL Project",
    "section": "2.4 Top FG% ‚õπÔ∏è‚Äç‚ôÇÔ∏è",
    "text": "2.4 Top FG% ‚õπÔ∏è‚Äç‚ôÇÔ∏è\n\nFind the ten highest FG% Values of players who have attmpted more than 15 Field Goal Attempts per game\n\nFG% is the ratio of field goals made to field goals attempted\nGenerally a higher field goal percentage denotes higher efficiency.\nA FG% of .500 (50%) or above is considered a good percentage, although it depends on the position.\n\nWho are the players for those top 10 FG% values?\n\n\ntop_FG_Percentage &lt;- NBAsal %&gt;% \n  group_by(`FG%`) %&gt;%\n  filter(FGA &gt; 15) %&gt;% \n  arrange(desc(`FG%`)) %&gt;% \n  head(10) %&gt;% \n  relocate(`FG%`, .before = Position) %&gt;% \n  relocate(FGA, .before = Position)\n\ntop_FG_Percentage\n\n# A tibble: 10 √ó 32\n# Groups:   FG% [9]\n    ...1 `Player Name` Salary `FG%`   FGA Position   Age Team     GP    GS    MP\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    94 Zion William‚Ä¶ 1.35e7 0.608  16.2 PF          22 NOP      29    29  33  \n 2    14 Anthony Davis 3.80e7 0.563  17.2 C           29 LAL      56    54  34  \n 3     4 Kevin Durant  4.41e7 0.56   18.3 PF          34 BRK/‚Ä¶    47    47  35.6\n 4     8 Giannis Ante‚Ä¶ 4.25e7 0.553  20.3 PF          28 MIL      63    63  32.1\n 5    27 Joel Embiid   3.36e7 0.548  20.1 C           28 PHI      66    66  34.6\n 6     6 Kawhi Leonard 4.25e7 0.512  16.8 SF          31 LAC      52    50  33.6\n 7    39 De'Aaron Fox  3.04e7 0.512  18.2 PG          25 SAC      73    73  33.4\n 8    34 Shai Gilgeou‚Ä¶ 3.09e7 0.51   20.3 PG          24 OKC      68    68  35.5\n 9     5 Bradley Beal  4.33e7 0.506  17.6 SG          29 WAS      50    50  33.5\n10    45 DeMar DeRozan 2.73e7 0.504  17.6 SF          33 CHI      74    74  36.2\n# ‚Ñπ 21 more variables: FG &lt;dbl&gt;, `3P` &lt;dbl&gt;, `3PA` &lt;dbl&gt;, `3P%` &lt;dbl&gt;,\n#   `2P` &lt;dbl&gt;, `2PA` &lt;dbl&gt;, `2P%` &lt;dbl&gt;, `eFG%` &lt;dbl&gt;, FT &lt;dbl&gt;, FTA &lt;dbl&gt;,\n#   `FT%` &lt;dbl&gt;, ORB &lt;dbl&gt;, DRB &lt;dbl&gt;, TRB &lt;dbl&gt;, AST &lt;dbl&gt;, STL &lt;dbl&gt;,\n#   BLK &lt;dbl&gt;, TOV &lt;dbl&gt;, PF &lt;dbl&gt;, PTS &lt;dbl&gt;, `Player-additional` &lt;chr&gt;\n\n\nComments:\n\nZion Williamson, Anthony Davis, Kevin Durant, Giannis Antetokounmpo, Joel Embiid, Kawhi Leonard, De‚ÄôAaron Fox, Shai Gilgeous-Alexander, Bradley Beal, Demar DeRozan have the highest FG% for players with more than 15 attempts per game.\nAll these players have a FG% above a .50 which means they are considered highly efficient"
  },
  {
    "objectID": "project.html#mp-minutes-per-game-vs-tov-turnovers",
    "href": "project.html#mp-minutes-per-game-vs-tov-turnovers",
    "title": "DANL Project",
    "section": "2.5 MP (minutes per game) Vs TOV (turnovers) üóëÔ∏è",
    "text": "2.5 MP (minutes per game) Vs TOV (turnovers) üóëÔ∏è\nProvide both ggplot codes and a couple of sentences to describe the relationship between Minutes Played Per Game and Turnovers for the top 10 players based on Points Per Game.\n\nggplot(top_10_players, aes(x= MP, y= TOV))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\")+\n  facet_wrap(~`Player Name`, ncol = 2)\n\n\n\n\n\n\n\n\nComments:\n\nGiannis Antetokounmpo played the least amount of minutes per game, but had the most turnovers per game. This makes sense as Giannis is the primary ball handler for his team, therfore there is more opportunity for him to turnover the ball\nDevin Booker, Donovan Mitchell, Jayson Tatum, and Shai Gilgeous-Alexander all play around 35-37 minutes per game and turnover the ball the least out of those 10 players\nStephen Curry, Lebron James, and Kevin Durant all play similar minutes and turnover the ball at a similar rate\n\nThey are all older in age and play similar minutes due to load management. They turnover the ball at an almost identical rate per game."
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Owen‚Äôs Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNYC Dogs\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\nOwen Ellick\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nIce Cream\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nOwen Ellick\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nPySpark Basics\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nOwen Ellick\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nggplot Basics\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nOwen Ellick\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nFavorite Artists\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nOwen Ellick\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nPython Basics\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nOwen Ellick\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify All\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nOwen Ellick\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nNFL in 2022\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nOwen Ellick\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBeer Markets\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nOwen Ellick\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nRestaurant Inspection\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nOwen Ellick\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nOwen Ellick\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Project210.html",
    "href": "Project210.html",
    "title": "DANL 210 Project",
    "section": "",
    "text": "The integration of Environmental, Social, and Governance (ESG) factors have become increasingly important for investors, businesses, and policymakers due to their implications for sustainability, ethical practices, and long-term financial performance. Understanding the relationship between ESG metrics and financial/accounting data is crucial for making informed investment decisions, developing sustainable business strategies, and shaping public policy.\n\n\n\nThis project seeks to analyze the interplay between ESG metrics, stock performance, and financial health by integrating data from multiple sources. The specific problem addressed is the lack of comprehensive insights into how ESG factors influence stock market performance and financial outcomes. By conducting data analysis, we aim to uncover patters, correlations, and trends that shed light on the relationship between sustainable practices, market valuation, and corporate financial performance. This analysis will provide valuable insights for investors, business, and policymakers seeking to incorporate ESG considerations into their decision-making processes and drive positive social and environmental impact."
  },
  {
    "objectID": "Project210.html#background",
    "href": "Project210.html#background",
    "title": "DANL 210 Project",
    "section": "",
    "text": "The integration of Environmental, Social, and Governance (ESG) factors have become increasingly important for investors, businesses, and policymakers due to their implications for sustainability, ethical practices, and long-term financial performance. Understanding the relationship between ESG metrics and financial/accounting data is crucial for making informed investment decisions, developing sustainable business strategies, and shaping public policy."
  },
  {
    "objectID": "Project210.html#statement-of-the-problem",
    "href": "Project210.html#statement-of-the-problem",
    "title": "DANL 210 Project",
    "section": "",
    "text": "This project seeks to analyze the interplay between ESG metrics, stock performance, and financial health by integrating data from multiple sources. The specific problem addressed is the lack of comprehensive insights into how ESG factors influence stock market performance and financial outcomes. By conducting data analysis, we aim to uncover patters, correlations, and trends that shed light on the relationship between sustainable practices, market valuation, and corporate financial performance. This analysis will provide valuable insights for investors, business, and policymakers seeking to incorporate ESG considerations into their decision-making processes and drive positive social and environmental impact."
  },
  {
    "objectID": "Project210.html#descriptive-statistics-for-yfinance-history",
    "href": "Project210.html#descriptive-statistics-for-yfinance-history",
    "title": "DANL 210 Project",
    "section": "3.1 Descriptive Statistics for Yfinance History",
    "text": "3.1 Descriptive Statistics for Yfinance History\n\n3.1.1 Ungrouped\n\nprint(\"\\nYfinance History Data Descriptive Statistics:\")\nprint(finance_history.describe())\n\n\nYfinance History Data Descriptive Statistics:\n                Open           High            Low          Close  \\\ncount  197796.000000  197796.000000  197796.000000  197796.000000   \nmean      149.641065     151.227362     148.152789     149.747446   \nstd       329.775746     333.343590     326.743053     330.150447   \nmin         1.030000       1.060000       0.780000       0.980000   \n25%        40.284990      40.740002      39.843464      40.310957   \n50%        82.321109      83.207870      81.400002      82.359852   \n75%       155.430508     156.976242     153.974345     155.511169   \nmax      8022.919922    8158.990234    8010.000000    8099.959961   \n\n             Volume      Dividends   Stock Splits  \ncount  1.977960e+05  197796.000000  197796.000000  \nmean   4.134482e+06       0.006791       0.000104  \nstd    9.289442e+06       0.099606       0.014192  \nmin    0.000000e+00       0.000000       0.000000  \n25%    8.323000e+05       0.000000       0.000000  \n50%    1.707600e+06       0.000000       0.000000  \n75%    3.854425e+06       0.000000       0.000000  \nmax    3.160112e+08      15.000000       4.000000  \n\n\n\n\n3.1.2 Grouped/Selected\n\nfinance_history['Volume'].describe()\n\ncount    1.977960e+05\nmean     4.134482e+06\nstd      9.289442e+06\nmin      0.000000e+00\n25%      8.323000e+05\n50%      1.707600e+06\n75%      3.854425e+06\nmax      3.160112e+08\nName: Volume, dtype: float64"
  },
  {
    "objectID": "Project210.html#descriptive-statistics-for-balance-sheets",
    "href": "Project210.html#descriptive-statistics-for-balance-sheets",
    "title": "DANL 210 Project",
    "section": "3.2 Descriptive Statistics for Balance Sheets",
    "text": "3.2 Descriptive Statistics for Balance Sheets\n\n3.2.1 Ungrouped\n\nprint(\"\\nFinance Balance Sheets Data Descriptive Statistics:\")\nprint(finance_balance_sheets.describe())\n\n\nFinance Balance Sheets Data Descriptive Statistics:\n       Ordinary Shares Number  Share Issued      Net Debt    Total Debt  \\\ncount            2.658000e+03  2.658000e+03  2.317000e+03  2.626000e+03   \nmean             6.405716e+08  7.568339e+08  1.291833e+10  1.922311e+10   \nstd              1.296627e+09  1.442190e+09  2.240450e+10  4.221775e+10   \nmin              3.167625e+06  9.589239e+06  2.200000e+07  1.404000e+06   \n25%              1.420809e+08  1.716074e+08  2.174900e+09  3.349737e+09   \n50%              2.720531e+08  3.142580e+08  5.714136e+09  7.261294e+09   \n75%              5.952015e+08  7.113500e+08  1.322700e+10  1.758525e+10   \nmax              1.572341e+10  1.572341e+10  2.238890e+11  4.421400e+11   \n\n       Tangible Book Value  Invested Capital  Working Capital  \\\ncount         2.659000e+03      2.658000e+03     2.314000e+03   \nmean          6.140413e+09      3.453963e+10     1.977094e+09   \nstd           2.906924e+10      6.710119e+10     7.778440e+09   \nmin          -9.834700e+10      1.221960e+08    -4.443000e+10   \n25%          -1.823000e+09      7.054684e+09    -2.506682e+08   \n50%           2.157460e+09      1.482450e+10     9.074155e+08   \n75%           8.880100e+09      3.350450e+10     2.709800e+09   \nmax           2.636610e+11      7.488770e+11     9.313100e+10   \n\n       Net Tangible Assets  Capital Lease Obligations  Common Stock Equity  \\\ncount         2.659000e+03               1.651000e+03         2.659000e+03   \nmean          6.475658e+09               1.730649e+09         1.661968e+10   \nstd           3.034589e+10               5.731350e+09         3.221102e+10   \nmin          -9.834700e+10              -6.100000e+07        -1.723300e+10   \n25%          -1.827950e+09               1.591580e+08         3.229000e+09   \n50%           2.120019e+09               3.918690e+08         7.500000e+09   \n75%           9.090150e+09               1.001500e+09         1.753100e+10   \nmax           2.722630e+11               7.729700e+10         3.067370e+11   \n\n       ...  Trading Securities  Investmentsin Subsidiariesat Cost  \\\ncount  ...        1.060000e+02                       3.600000e+01   \nmean   ...        8.437230e+10                       2.777855e+09   \nstd    ...        1.371783e+11                       3.849279e+09   \nmin    ...        0.000000e+00                       0.000000e+00   \n25%    ...        6.462450e+07                       4.755000e+08   \n50%    ...        1.174500e+09                       1.388500e+09   \n75%    ...        1.357220e+11                       1.962875e+09   \nmax    ...        5.742130e+11                       1.228100e+10   \n\n       Total Partnership Capital  Limited Partnership Capital  \\\ncount               1.000000e+01                 1.000000e+01   \nmean                1.964380e+10                 1.963130e+10   \nstd                 1.568801e+10                 1.567458e+10   \nmin                 4.479000e+09                 4.479000e+09   \n25%                 4.826250e+09                 4.826250e+09   \n50%                 1.910150e+10                 1.908800e+10   \n75%                 3.345025e+10                 3.343100e+10   \nmax                 3.668200e+10                 3.665600e+10   \n\n       Dueto Related Parties Non Current  Duefrom Related Parties Non Current  \\\ncount                       1.400000e+01                         1.100000e+01   \nmean                        9.710000e+08                         1.903109e+07   \nstd                         8.538153e+08                         4.242806e+07   \nmin                         5.000000e+06                         0.000000e+00   \n25%                         2.992500e+08                         0.000000e+00   \n50%                         6.075000e+08                         1.805000e+06   \n75%                         1.882750e+09                         3.529500e+06   \nmax                         2.376000e+09                         1.350000e+08   \n\n       Fixed Assets Revaluation Reserve  Current Deferred Taxes Liabilities  \\\ncount                      3.000000e+00                        2.000000e+00   \nmean                       3.001667e+09                        9.285000e+07   \nstd                        5.158336e+09                        9.192388e+05   \nmin                        2.300000e+07                        9.220000e+07   \n25%                        2.350000e+07                        9.252500e+07   \n50%                        2.400000e+07                        9.285000e+07   \n75%                        4.491000e+09                        9.317500e+07   \nmax                        8.958000e+09                        9.350000e+07   \n\n       Current Deferred Taxes Assets  General Partnership Capital  \ncount                   5.000000e+00                          5.0  \nmean                    3.234000e+08                   -2000000.0  \nstd                     6.618761e+07                          0.0  \nmin                     2.550000e+08                   -2000000.0  \n25%                     2.820000e+08                   -2000000.0  \n50%                     3.100000e+08                   -2000000.0  \n75%                     3.440000e+08                   -2000000.0  \nmax                     4.260000e+08                   -2000000.0  \n\n[8 rows x 142 columns]\n\n\n\n\n3.2.2 Grouped/Selected\n\nfinance_balance_sheets[['Total Assets','Total Debt']].describe()\n\n\n\n\n\n\n\n\n\nTotal Assets\nTotal Debt\n\n\n\n\ncount\n2.659000e+03\n2.626000e+03\n\n\nmean\n9.075012e+10\n1.922311e+10\n\n\nstd\n2.940459e+11\n4.221775e+10\n\n\nmin\n1.891360e+08\n1.404000e+06\n\n\n25%\n1.131355e+10\n3.349737e+09\n\n\n50%\n2.523760e+10\n7.261294e+09\n\n\n75%\n6.313150e+10\n1.758525e+10\n\n\nmax\n4.090727e+12\n4.421400e+11"
  },
  {
    "objectID": "Project210.html#descriptive-statistics-income-statements",
    "href": "Project210.html#descriptive-statistics-income-statements",
    "title": "DANL 210 Project",
    "section": "3.3 Descriptive Statistics Income Statements",
    "text": "3.3 Descriptive Statistics Income Statements\n\n3.3.1 Ungrouped\n\nprint(\"\\nFinance Income Statement Data Descriptive Statistics:\")\nprint(finance_income_statement.describe())\n\n\nFinance Income Statement Data Descriptive Statistics:\n       Tax Effect Of Unusual Items  Tax Rate For Calcs  Normalized EBITDA  \\\ncount                 2.659000e+03         2659.000000       2.314000e+03   \nmean                 -1.263741e+07            0.203667       1.463302e+09   \nstd                   1.516601e+08            0.080287       3.230278e+09   \nmin                  -5.202400e+09            0.000000      -8.430000e+09   \n25%                  -3.259879e+06            0.167080       2.814000e+08   \n50%                   0.000000e+00            0.210000       5.805740e+08   \n75%                   0.000000e+00            0.240000       1.345750e+09   \nmax                   4.056840e+08            0.400000       4.322100e+10   \n\n       Total Unusual Items  Total Unusual Items Excluding Goodwill  \\\ncount         1.981000e+03                            1.981000e+03   \nmean         -5.804262e+07                           -5.804262e+07   \nstd           9.154493e+08                            9.154493e+08   \nmin          -1.300600e+10                           -1.300600e+10   \n25%          -3.670000e+07                           -3.670000e+07   \n50%          -2.500000e+06                           -2.500000e+06   \n75%           6.900000e+06                            6.900000e+06   \nmax           2.802600e+10                            2.802600e+10   \n\n       Net Income From Continuing Operation Net Minority Interest  \\\ncount                                       2.659000e+03            \nmean                                        7.352532e+08            \nstd                                         2.157545e+09            \nmin                                        -1.191100e+10            \n25%                                         1.109640e+08            \n50%                                         2.810000e+08            \n75%                                         7.211725e+08            \nmax                                         3.391600e+10            \n\n       Reconciled Depreciation  Reconciled Cost Of Revenue        EBITDA  \\\ncount             2.565000e+03                2.282000e+03  2.314000e+03   \nmean              4.015389e+08                4.538171e+09  1.408229e+09   \nstd               8.570225e+08                1.112775e+10  3.235936e+09   \nmin              -4.460000e+08               -7.980000e+08 -1.236200e+10   \n25%               6.100000e+07                5.786600e+08  2.706172e+08   \n50%               1.510000e+08                1.522100e+09  5.706500e+08   \n75%               3.812620e+08                3.339823e+09  1.333000e+09   \nmax               1.382000e+10                1.406860e+11  4.322100e+10   \n\n               EBIT  ...  Salaries And Wages   Other Taxes  \\\ncount  2.463000e+03  ...        5.830000e+02  2.250000e+02   \nmean   9.869213e+08  ...        8.517112e+08  3.458729e+08   \nstd    2.556520e+09  ...        2.054952e+09  1.040171e+09   \nmin   -1.297600e+10  ...       -4.000000e+08 -9.000000e+05   \n25%    1.823420e+08  ...        2.000000e+06  5.899200e+07   \n50%    4.150000e+08  ...        5.600000e+07  1.270000e+08   \n75%    1.000600e+09  ...        6.220000e+08  2.660000e+08   \nmax    4.037300e+10  ...        1.311800e+10  7.712000e+09   \n\n       Provision For Doubtful Accounts  Total Other Finance Cost  \\\ncount                     7.600000e+01              2.800000e+02   \nmean                      9.453408e+06              3.437941e+07   \nstd                       2.172056e+07              1.057812e+08   \nmin                      -2.300000e+07             -8.500000e+07   \n25%                       0.000000e+00              1.000000e+06   \n50%                       2.816500e+06              5.022000e+06   \n75%                       1.100000e+07              2.607500e+07   \nmax                       9.800000e+07              6.080000e+08   \n\n       Depreciation Income Statement  Insurance And Claims  \\\ncount                   2.180000e+02          8.800000e+01   \nmean                    2.192255e+08          1.080127e+08   \nstd                     3.660190e+08          1.843105e+08   \nmin                     3.000000e+06          4.643000e+06   \n25%                     2.825000e+07          1.282075e+07   \n50%                     1.030000e+08          2.868200e+07   \n75%                     2.727500e+08          7.576450e+07   \nmax                     2.264000e+09          7.570000e+08   \n\n       Rent And Landing Fees  Excise Taxes  Depletion Income Statement  \\\ncount           6.400000e+01  2.700000e+01                8.000000e+00   \nmean            6.088209e+07  2.281630e+08                2.342250e+08   \nstd             4.800665e+07  2.792101e+09                1.093543e+08   \nmin             1.700000e+07 -4.122000e+09                1.214000e+08   \n25%             2.800000e+07  1.728500e+08                1.631000e+08   \n50%             3.541850e+07  5.423000e+08                1.830000e+08   \n75%             6.842500e+07  7.565000e+08                3.432500e+08   \nmax             1.800000e+08  1.129900e+10                3.870000e+08   \n\n       Securities Amortization  \ncount             5.000000e+00  \nmean              2.670120e+07  \nstd               2.412841e+07  \nmin               4.581000e+06  \n25%               1.011000e+07  \n50%               1.320700e+07  \n75%               4.969400e+07  \nmax               5.591400e+07  \n\n[8 rows x 75 columns]\n\n\n\n\n3.3.2 Grouped/Selected\n\nfinance_income_statement['Net Income'].describe()\n\ncount    2.659000e+03\nmean     7.498470e+08\nstd      2.219866e+09\nmin     -1.191100e+10\n25%      1.116675e+08\n50%      2.825000e+08\n75%      7.290780e+08\nmax      3.391600e+10\nName: Net Income, dtype: float64"
  },
  {
    "objectID": "Project210.html#descriptive-statistics-esg-project-data",
    "href": "Project210.html#descriptive-statistics-esg-project-data",
    "title": "DANL 210 Project",
    "section": "3.4 Descriptive Statistics ESG Project Data",
    "text": "3.4 Descriptive Statistics ESG Project Data\n\n3.4.1 Ungrouped\n\nprint(\"\\nESG Data Descriptive Statistics:\")\nprint(esg_proj_data.describe())\n\n\nESG Data Descriptive Statistics:\n       Total ESG Risk Score  Environmental Risk Score  Social Risk Score  \\\ncount            635.000000                612.000000         612.000000   \nmean              21.639843                  5.819281           9.016176   \nstd                7.112836                  5.318299           3.566610   \nmin                6.400000                  0.000000           0.800000   \n25%               16.300000                  1.775000           6.700000   \n50%               21.200000                  4.000000           8.900000   \n75%               26.100000                  9.000000          11.125000   \nmax               52.000000                 27.300000          22.500000   \n\n       Governance Risk Score  Controversy Level  \ncount             612.000000         577.000000  \nmean                6.826961           1.967071  \nstd                 2.395565           0.786578  \nmin                 2.400000           1.000000  \n25%                 5.200000           1.000000  \n50%                 6.300000           2.000000  \n75%                 7.900000           2.000000  \nmax                19.400000           5.000000  \n\n\n\n\n3.4.2 Grouped/Selected\n\nesg_proj_data['Total ESG Risk Score'].describe()\n\ncount    635.000000\nmean      21.639843\nstd        7.112836\nmin        6.400000\n25%       16.300000\n50%       21.200000\n75%       26.100000\nmax       52.000000\nName: Total ESG Risk Score, dtype: float64\n\n\n\nesg_proj_data['Controversy Level'].describe()\n\ncount    577.000000\nmean       1.967071\nstd        0.786578\nmin        1.000000\n25%        1.000000\n50%        2.000000\n75%        2.000000\nmax        5.000000\nName: Controversy Level, dtype: float64"
  },
  {
    "objectID": "Project210.html#distribution-plots",
    "href": "Project210.html#distribution-plots",
    "title": "DANL 210 Project",
    "section": "3.5 Distribution Plots",
    "text": "3.5 Distribution Plots\n\n3.5.1 ESG Project Data\n\ndef plot_distributions(df, title_prefix):\n    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col].dropna(), bins=20, kde=True)\n        plt.title(f'Distribution of {title_prefix} {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.show()\n\n# Plot distributions for ESG data\nplot_distributions(esg_proj_data, 'ESG Data')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Yfinance Balance Sheet Data\n\nbalance_sheet_columns = ['Total Assets', 'Common Stock', 'Current Liabilities', 'Accounts Payable', 'Total Debt']\n\n# Function to plot distributions for specified columns\ndef plot_distributions(df, title_prefix, columns):\n    for col in columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col].dropna(), bins=20, kde=True)\n        plt.title(f'Distribution of {title_prefix} {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.show()\n\n# Plot distributions for financial balance sheets data\nplot_distributions(finance_balance_sheets, 'Finance Balance Sheets Data', columns=balance_sheet_columns)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.3 Yfinance Income Statement Data\n\nincome_statement_columns = ['Net Income', 'Total Revenue', 'Gross Profit', 'Total Expenses']\n\n# Function to plot distributions for specified columns\ndef plot_distributions(df, title_prefix, columns):\n    for col in columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col].dropna(), bins=20, kde=True)\n        plt.title(f'Distribution of {title_prefix} {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.show()\n\n# Plot distributions for financial income statement data\nplot_distributions(finance_income_statement, 'Finance Income Statement Data', columns=income_statement_columns)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.4 Yfinance History Data\n\nhistory_columns = ['Open', 'High', 'Low', 'Close']\n\n# Function to plot distributions for specified columns\ndef plot_distributions(df, title_prefix, columns):\n    for col in columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col].dropna(), bins=20, kde=True)\n        plt.title(f'Distribution of {title_prefix} {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.show()\n\n# Plot distributions for financial historical data\nplot_distributions(finance_history, 'Finance History Data', columns=history_columns)"
  },
  {
    "objectID": "Project210.html#correlation-heat-maps",
    "href": "Project210.html#correlation-heat-maps",
    "title": "DANL 210 Project",
    "section": "3.6 Correlation Heat Maps",
    "text": "3.6 Correlation Heat Maps\n\n3.6.1 ESG Project Data Heatmap\n\nnumeric_data = esg_proj_data.select_dtypes(include='number')\ncorr_matrix = numeric_data.corr()\nplt.figure(figsize=(8, 6))\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n\nplt.title('Correlation Heatmap with Varied Correlations')\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.6.2 Yfinance Historical Data Heatmap\n\nnumeric_data = finance_history.select_dtypes(include='number')\ncorr_matrix = numeric_data.corr()\nplt.figure(figsize=(8, 6))\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n\nplt.title('Correlation Heatmap with Varied Correlations')\n\n\nplt.show()"
  },
  {
    "objectID": "Project210.html#a.-esg-data-analysis",
    "href": "Project210.html#a.-esg-data-analysis",
    "title": "DANL 210 Project",
    "section": "4.1 A. ESG Data Analysis",
    "text": "4.1 A. ESG Data Analysis\n\nFind the top 10 companies by Total ESG Risk Score\n\n\ntop_10_companies = esg_proj_data.sort_values(by='Total ESG Risk Score', ascending=False).head(10)\n\nprint(top_10_companies)\n\n                               Comapny Name  Total ESG Risk Score  \\\n211                 Energy Transfer LP (ET)                  52.0   \n492       Range Resources Corporation (RRC)                  44.9   \n119     Chesapeake Energy Corporation (CHK)                  44.0   \n149       Crescent Point Energy Corp. (CPG)                  42.4   \n436  Occidental Petroleum Corporation (OXY)                  41.7   \n629           Exxon Mobil Corporation (XOM)                  41.6   \n244           General Electric Company (GE)                  40.5   \n217              Eagle Materials Inc. (EXP)                  40.5   \n64                  The Boeing Company (BA)                  39.6   \n161               Cenovus Energy Inc. (CVE)                  39.3   \n\n     Environmental Risk Score  Social Risk Score  Governance Risk Score  \\\n211                       NaN                NaN                    NaN   \n492                      25.3                9.7                    9.9   \n119                      23.8               10.9                    9.3   \n149                      27.3                7.9                    7.2   \n436                      25.0                9.7                    7.0   \n629                      23.1               10.0                    8.5   \n244                      14.2               15.4                   10.9   \n217                      22.3                9.2                    9.0   \n64                        8.8               22.5                    8.3   \n161                      21.6                9.6                    8.1   \n\n     Controversy Level  \n211                5.0  \n492                2.0  \n119                2.0  \n149                NaN  \n436                2.0  \n629                3.0  \n244                3.0  \n217                1.0  \n64                 4.0  \n161                2.0  \n\n\n\nComment: This code provides the top 10 companies in the esg_proj_data data frame by Total ESG Risk Score.\n\nAs you can see Energy Transfer LP is the top company by Total ESG Risk Score\n\nCreate a plot to visualize the Total ESG Risk Scores for the top 10 companies\n\n\nplt.figure(figsize=(10, 6))\n\nsns.barplot(data=top_10_companies, x='Comapny Name', y='Total ESG Risk Score', palette='viridis')\n\nplt.title('Top 10 Companies by Total ESG Risk Score')\nplt.xlabel('Company')\nplt.ylabel('Total ESG Risk Score')\nplt.xticks(rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nComment: This bar plot visualizes the top 10 companies by Total ESG Risk Score"
  },
  {
    "objectID": "Project210.html#b.-financial-income-statement-data-analysis",
    "href": "Project210.html#b.-financial-income-statement-data-analysis",
    "title": "DANL 210 Project",
    "section": "4.2 B. Financial Income Statement Data Analysis",
    "text": "4.2 B. Financial Income Statement Data Analysis\n\nWhat is the distribution of net income across a small subset of 5 companies within the data frame finance_income_statement?\n\n\nsubset_companies = finance_income_statement['Company Name'].unique()[:5] \n\nsubset_data = finance_income_statement[finance_income_statement['Company Name'].isin(subset_companies)]\n\n\nplt.figure(figsize=(10, 6))\n\n\nsns.boxplot(data=subset_data, x='Net Income', y='Company Name')\n\n\nplt.title('Distribution of Net Income Across Selected Companies')\nplt.xlabel('Net Income')\nplt.ylabel('Company Name')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nComment: This code shows boxplots regarding net income for the first 5 companies in the data frame\n\nThe box plot summarizes the central tendency and spread of net income for each selected company\nIt reveals variations in net income distribution among the companies, with Apple being far ahead in every aspect.\nPotential outliers in net income are indicated by points beyond the whiskers of the box plot\nThis visualization helps identify differences in net income distribution across companies and highlights areas for further analysis"
  },
  {
    "objectID": "Project210.html#c.-financial-history-data-analysis",
    "href": "Project210.html#c.-financial-history-data-analysis",
    "title": "DANL 210 Project",
    "section": "4.3 C. Financial History Data Analysis",
    "text": "4.3 C. Financial History Data Analysis\n\nWhat is the average daily trading volume for a specific company over the given time period spanning from January 1,2023, to March 31, 2024?\n\n\ncompany_name = 'Tesla, Inc.'\n\ncompany_data = finance_history[finance_history['Company Name'] == company_name]\naverage_volume = company_data['Volume'].mean()\nprint(f\"The average daily trading volume for {company_name} is: {average_volume}\")\n\nThe average daily trading volume for Tesla, Inc. is: 130524607.07395498\n\n\n\nHow does the closing price of Zillow Group, Inc.¬†vary over time?\n\n\ncompany_name = 'Zillow Group, Inc.'\n\ncompany_data = finance_history[finance_history['Company Name'] == company_name]\ncompany_data['Date'] = pd.to_datetime(company_data['Date'])\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=company_data, x='Date', y='Close', marker='o')\n\nplt.title(f'Closing Price Over Time for {company_name}')\nplt.xlabel('Date')\nplt.ylabel('Closing Price')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nComment: The closing price for Zillow based on the visualization was trending upward in the first and second quarter.\n\nIn the third quarter the close price dropped very low within 35 and 40.\nDuring the fourth quarter the closing price willed its way back up\nLastly, in the fifth quarter it remained volatile going up and down ultimately ending trending downward"
  },
  {
    "objectID": "Project210.html#d.-financial-balance-sheet-data-analysis",
    "href": "Project210.html#d.-financial-balance-sheet-data-analysis",
    "title": "DANL 210 Project",
    "section": "4.4 D. Financial Balance Sheet Data Analysis",
    "text": "4.4 D. Financial Balance Sheet Data Analysis\n\nFind the top 15 companies in the finance_balance_sheets Data Frame by Inventory\n\n\ngrouped_inventory = finance_balance_sheets.groupby('Company Name')['Inventory'].sum().reset_index()\n\ntop_companies_inventory = grouped_inventory.sort_values(by='Inventory', ascending=False)\n\ntop_15_companies_inventory = top_companies_inventory.head(15)\n\ncolumn_order = ['Company Name', 'Inventory'] + [col for col in top_15_companies_inventory.columns if col not in ['Company Name', 'Inventory']]\ntop_15_companies_inventory = top_15_companies_inventory[column_order]\n\nprint(top_15_companies_inventory)\n\n                     Company Name     Inventory\n455            The Boeing Company  3.990090e+11\n28               Amazon.com, Inc.  1.706280e+11\n188       Exxon Mobil Corporation  1.209520e+11\n146             D.R. Horton, Inc.  1.168188e+11\n279            Lennar Corporation  1.056539e+11\n307          McKesson Corporation  1.053050e+11\n97         CVS Health Corporation  8.787900e+10\n110                       Cencora  8.754558e+10\n215        General Motors Company  8.740400e+10\n201            Ford Motor Company  8.652400e+10\n108              Caterpillar Inc.  8.647700e+10\n105         Cardinal Health, Inc.  8.527500e+10\n209                  GE Aerospace  8.413800e+10\n139  Costco Wholesale Corporation  8.413200e+10\n449                   Tesla, Inc.  7.211100e+10\n\n\n\nComment: These are the top 15 companies in the data frame by inventory\nHow does total assets value vary across different companies?\n\n\ngrouped_assets = finance_balance_sheets.groupby('Company Name')['Total Assets'].sum().reset_index()\n\ntop_companies_assets = grouped_assets.sort_values(by='Total Assets', ascending=False)\n\ntop_N = 10\ntop_N_companies_assets = top_companies_assets.head(top_N)\nplt.figure(figsize=(10, 6))\nsns.barplot(data=top_N_companies_assets, x='Company Name', y='Total Assets')\n\nplt.title(f'Top {top_N} Companies by Total Assets')\nplt.xlabel('Company Name')\nplt.ylabel('Total Assets')\nplt.xticks(rotation=90,)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nComment: This code shows how the variable total assets varys across the top 10 companies by total assets. I reduced the data to top 10 to avoid overcrowding on the x axis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Owen Ellick",
    "section": "",
    "text": "Owen Ellick majors in Data Analytics at SUNY Geneseo. When not working on data analytics, Owen enjoys playing sports and hanging out with his friends."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Owen Ellick",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Data Analytics | Aug 2022 - May 2026  Minor in Accounting"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Owen Ellick",
    "section": "Experience",
    "text": "Experience\nTown of Smithtown | Beach Laborer | May 2023 - Present\nNassau/Suffolk P.A.L Long Island Lacrosse League | Referee | March 2022- Present"
  },
  {
    "objectID": "Housing.html",
    "href": "Housing.html",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "Housing.html#introduction",
    "href": "Housing.html#introduction",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Introduction",
    "text": "Introduction\nIn today‚Äôs blog, we dive deep into an exciting regression analysis using data from the 2004 American Housing Survey. This dataset includes various factors like home values, demographics, neighborhood characteristics, and more. In particular, we focus on the relationships between home features, such as the number of bedrooms and bathrooms, and house prices, as well as a logistic regression model examining down payments for first-time homebuyers.\nLet‚Äôs explore how we can use regression to understand the impact of these factors on house prices and down payments."
  },
  {
    "objectID": "Housing.html#loading-the-data",
    "href": "Housing.html#loading-the-data",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, log_loss, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nhomes = pd.read_csv(\n  'https://bcdanl.github.io/data/american_housing_survey.csv'\n)\nhomes.head()\n\n\n\n  \n    \n\n\n\n\n\n\nLPRICE\nVALUE\nSTATE\nMETRO\nZINC2\nHHGRAD\nBATHS\nBEDRMS\nPER\nZADULT\n...\nEABAN\nHOWH\nHOWN\nODORA\nSTRNA\nAMMORT\nINTW\nMATBUY\nDWNPAY\nFRSTHO\n\n\n\n\n0\n85000\n150000\nGA\nrural\n15600\nNo HS\n2\n3\n1\n1\n...\n0\ngood\ngood\n0\n0\n50000\n9\n1\nother\n0\n\n\n1\n76500\n130000\nGA\nrural\n61001\nHS Grad\n2\n3\n5\n2\n...\n0\ngood\nbad\n0\n1\n70000\n5\n1\nother\n1\n\n\n2\n93900\n135000\nGA\nrural\n38700\nHS Grad\n2\n3\n4\n2\n...\n0\ngood\ngood\n0\n0\n117000\n6\n0\nother\n1\n\n\n3\n100000\n140000\nGA\nrural\n80000\nNo HS\n3\n4\n2\n2\n...\n0\ngood\ngood\n0\n1\n100000\n7\n1\nprev home\n0\n\n\n4\n100000\n135000\nGA\nrural\n61000\nHS Grad\n2\n3\n2\n2\n...\n0\ngood\ngood\n0\n0\n100000\n4\n1\nother\n1\n\n\n\n\n5 rows √ó 29 columns"
  },
  {
    "objectID": "Housing.html#relationships",
    "href": "Housing.html#relationships",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Relationships",
    "text": "Relationships\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='BEDRMS', y='VALUE', data=homes)\nplt.title('Relationship between Number of Bedrooms and House Value')\nplt.xlabel('Number of Bedrooms')\nplt.ylabel('House Value')\nplt.show()\n\n\n\n\n\n\n\n\n\nAs expected, there‚Äôs a general trend showing that houses with more bedrooms tend to have higher values. However, there‚Äôs significant variability, indicating other factors also play a role.\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(homes['VALUE'], bins=50, kde=True)\nplt.title('Distribution of House Values')\nplt.xlabel('House Value')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nThe distribution of house values is right-skewed, indicating that there are many lower-valued homes and a smaller number of very high-valued homes. This is typical in real estate markets.\n\nplt.figure(figsize=(10,6))\nsns.scatterplot(x='BATHS', y='VALUE', data=homes)\nplt.title('Relationship between Number of Bathrooms and House Value')\nplt.xlabel('Number of Bathrooms')\nplt.ylabel('House Value')\nplt.show()\n\n\n\n\n\n\n\n\nSimilar to bedrooms, the number of bathrooms positively correlates with house value. The more bathrooms, the more valuable the house tends to be."
  },
  {
    "objectID": "Housing.html#linear-regression-model",
    "href": "Housing.html#linear-regression-model",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nFit a linear regression model with the following specifications:\n\nOutcome variable: log(VALUE)\nPredictors: all but AMORT and LPRICE\n\n\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\nhomes_copy = homes.copy()\n\nhomes_copy = homes_copy.dropna(subset=['VALUE', 'AMMORT', 'LPRICE'])\n\nhomes_copy['log_VALUE'] = np.log(homes_copy['VALUE'])\n\ncategorical_cols = homes_copy.select_dtypes(include=['object']).columns\n\nencoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse=False for compatibility with LinearRegression\nencoded_data = encoder.fit_transform(homes_copy[categorical_cols])\nencoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n\nhomes_copy = homes_copy.drop(columns=categorical_cols)\nhomes_copy = pd.concat([homes_copy, encoded_df], axis=1)\n\npredictors = [col for col in homes_copy.columns if col not in ['AMMORT', 'LPRICE', 'VALUE','log_VALUE']]\n\nX = homes_copy[predictors]\ny = homes_copy['log_VALUE']\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(model.coef_)\nmodel.intercept_\n\n[ 6.24368078e-07  2.11688048e-01  8.74019493e-02  9.65115807e-03\n -1.86439843e-02 -9.32413595e-04 -4.34737453e-02 -2.56800854e-02\n -8.64473930e-02  9.39133675e-03 -1.26476483e-01  2.86960819e-02\n  2.94471054e-01 -1.51545979e-02 -1.62135211e-01  1.02559846e-02\n -3.61815653e-02 -4.63687515e-02 -2.96611588e-02 -8.39847366e-02\n  6.17984043e-01  3.25853542e-01  2.71615116e-01 -3.70717924e-02\n -2.43773279e-01 -1.61255350e-01 -1.01567117e-01 -4.64994801e-02\n -5.56818280e-02 -3.80200590e-01 -2.53658960e-01 -4.30915507e-01\n  4.95171204e-01 -4.30521312e-02  4.30521312e-02 -1.48478092e-02\n  1.17280711e-01  1.82412176e-01 -7.54623696e-02 -2.09382709e-01\n -6.47717629e-02  6.47717629e-02 -5.96320624e-02  5.96320624e-02\n -6.04477439e-02  6.04477439e-02]\n\n\nnp.float64(11.219066790661115)"
  },
  {
    "objectID": "Housing.html#linear-regression-refit",
    "href": "Housing.html#linear-regression-refit",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Linear Regression Refit",
    "text": "Linear Regression Refit\n\nRefit the linear regression model, retaining only statistically significant predictors from Question 1.\nCompare the revised model to the initial model from Question 2 using:\n\nB estimates\nR^2\nRMSE\nResidual plots\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nX = sm.add_constant(X)\n\nmodel_sm = sm.OLS(y, X).fit()\n\nprint(model_sm.summary())\n\nsignificant_predictors = [predictor for predictor, p_value in zip(X.columns, model_sm.pvalues) if p_value &lt; 0.05]\nprint(\"Significant predictors:\", significant_predictors)\n\n\nX_significant = X[significant_predictors]\nmodel_significant = sm.OLS(y, X_significant).fit()\n\nprint(model_significant.summary())\n\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport numpy as np\n\n\nprint(\"\\nOriginal Model Coefficients:\")\nprint(model_sm.params)\n\nprint(\"\\nRefitted Model Coefficients:\")\nprint(model_significant.params)\n\n\n\nprint(\"\\nOriginal Model R-squared:\", model_sm.rsquared)\nprint(\"Refitted Model R-squared:\", model_significant.rsquared)\n\nprint(\"\\nOriginal Model RMSE:\", np.sqrt(mean_squared_error(y, model_sm.predict(X))))\nprint(\"Refitted Model RMSE:\", np.sqrt(mean_squared_error(y, model_significant.predict(X_significant))))\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.residplot(x=model_sm.fittedvalues, y=model_sm.resid)\nplt.title('Original Model Residual Plot')\n\nplt.subplot(1, 2, 2)\nsns.residplot(x=model_significant.fittedvalues, y=model_significant.resid)\nplt.title('Refitted Model Residual Plot')\n\nplt.tight_layout()\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              log_VALUE   R-squared:                       0.305\nModel:                            OLS   Adj. R-squared:                  0.304\nMethod:                 Least Squares   F-statistic:                     170.6\nDate:                Wed, 16 Apr 2025   Prob (F-statistic):               0.00\nTime:                        23:39:27   Log-Likelihood:                -18921.\nNo. Observations:               15565   AIC:                         3.792e+04\nDf Residuals:                   15524   BIC:                         3.824e+04\nDf Model:                          40                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst                3.4237      0.016    215.508      0.000       3.393       3.455\nZINC2             6.244e-07   5.54e-08     11.273      0.000    5.16e-07    7.33e-07\nBATHS                0.2117      0.012     18.271      0.000       0.189       0.234\nBEDRMS               0.0874      0.010      8.690      0.000       0.068       0.107\nPER                  0.0097      0.006      1.543      0.123      -0.003       0.022\nZADULT              -0.0186      0.011     -1.714      0.086      -0.040       0.003\nNUNITS              -0.0009      0.001     -1.792      0.073      -0.002    8.74e-05\nEAPTBL              -0.0435      0.023     -1.853      0.064      -0.089       0.003\nECOM1               -0.0257      0.019     -1.335      0.182      -0.063       0.012\nECOM2               -0.0864      0.048     -1.799      0.072      -0.181       0.008\nEGREEN               0.0094      0.014      0.671      0.502      -0.018       0.037\nEJUNK               -0.1265      0.051     -2.478      0.013      -0.227      -0.026\nELOW1                0.0287      0.023      1.241      0.215      -0.017       0.074\nESFD                 0.2945      0.030      9.963      0.000       0.237       0.352\nETRANS              -0.0152      0.025     -0.598      0.550      -0.065       0.034\nEABAN               -0.1621      0.036     -4.506      0.000      -0.233      -0.092\nODORA                0.0103      0.033      0.310      0.757      -0.055       0.075\nSTRNA               -0.0362      0.016     -2.251      0.024      -0.068      -0.005\nINTW                -0.0464      0.004    -10.518      0.000      -0.055      -0.038\nMATBUY              -0.0297      0.014     -2.168      0.030      -0.056      -0.003\nFRSTHO              -0.0840      0.017     -4.870      0.000      -0.118      -0.050\nSTATE_CA             0.8813      0.021     41.053      0.000       0.839       0.923\nSTATE_CO             0.5892      0.020     29.992      0.000       0.551       0.628\nSTATE_CT             0.5350      0.022     24.497      0.000       0.492       0.578\nSTATE_GA             0.2263      0.022     10.279      0.000       0.183       0.269\nSTATE_IL             0.0196      0.050      0.394      0.694      -0.078       0.117\nSTATE_IN             0.1021      0.021      4.820      0.000       0.061       0.144\nSTATE_LA             0.1618      0.028      5.713      0.000       0.106       0.217\nSTATE_MO             0.2169      0.025      8.829      0.000       0.169       0.265\nSTATE_OH             0.2077      0.023      8.841      0.000       0.162       0.254\nSTATE_OK            -0.1168      0.024     -4.922      0.000      -0.163      -0.070\nSTATE_PA             0.0097      0.025      0.389      0.697      -0.039       0.059\nSTATE_TX            -0.1676      0.026     -6.540      0.000      -0.218      -0.117\nSTATE_WA             0.7585      0.022     35.075      0.000       0.716       0.801\nMETRO_rural          1.6688      0.011    145.656      0.000       1.646       1.691\nMETRO_urban          1.7549      0.013    139.495      0.000       1.730       1.780\nHHGRAD_Assoc         0.6699      0.017     39.387      0.000       0.637       0.703\nHHGRAD_Bach          0.8020      0.013     61.847      0.000       0.777       0.827\nHHGRAD_Grad          0.8671      0.016     54.110      0.000       0.836       0.899\nHHGRAD_HS Grad       0.6093      0.012     52.521      0.000       0.587       0.632\nHHGRAD_No HS         0.4753      0.022     21.651      0.000       0.432       0.518\nHOWH_bad             1.6471      0.016    101.889      0.000       1.615       1.679\nHOWH_good            1.7766      0.015    122.239      0.000       1.748       1.805\nHOWN_bad             1.6522      0.014    116.080      0.000       1.624       1.680\nHOWN_good            1.7715      0.013    138.550      0.000       1.746       1.797\nDWNPAY_other         1.6514      0.012    140.565      0.000       1.628       1.674\nDWNPAY_prev home     1.7723      0.012    145.976      0.000       1.748       1.796\n==============================================================================\nOmnibus:                    23661.757   Durbin-Watson:                   1.909\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         11331196.532\nSkew:                          -9.561   Prob(JB):                         0.00\nKurtosis:                     133.790   Cond. No.                     2.14e+19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 8.02e-25. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\nSignificant predictors: ['const', 'ZINC2', 'BATHS', 'BEDRMS', 'EJUNK', 'ESFD', 'EABAN', 'STRNA', 'INTW', 'MATBUY', 'FRSTHO', 'STATE_CA', 'STATE_CO', 'STATE_CT', 'STATE_GA', 'STATE_IN', 'STATE_LA', 'STATE_MO', 'STATE_OH', 'STATE_OK', 'STATE_TX', 'STATE_WA', 'METRO_rural', 'METRO_urban', 'HHGRAD_Assoc', 'HHGRAD_Bach', 'HHGRAD_Grad', 'HHGRAD_HS Grad', 'HHGRAD_No HS', 'HOWH_bad', 'HOWH_good', 'HOWN_bad', 'HOWN_good', 'DWNPAY_other', 'DWNPAY_prev home']\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              log_VALUE   R-squared:                       0.304\nModel:                            OLS   Adj. R-squared:                  0.303\nMethod:                 Least Squares   F-statistic:                     234.4\nDate:                Wed, 16 Apr 2025   Prob (F-statistic):               0.00\nTime:                        23:39:27   Log-Likelihood:                -18932.\nNo. Observations:               15565   AIC:                         3.792e+04\nDf Residuals:                   15535   BIC:                         3.815e+04\nDf Model:                          29                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nconst                3.4174      0.017    202.550      0.000       3.384       3.450\nZINC2             6.206e-07   5.51e-08     11.253      0.000    5.13e-07    7.29e-07\nBATHS                0.2137      0.012     18.492      0.000       0.191       0.236\nBEDRMS               0.0916      0.009      9.840      0.000       0.073       0.110\nEJUNK               -0.1298      0.051     -2.552      0.011      -0.230      -0.030\nESFD                 0.2942      0.029     10.083      0.000       0.237       0.351\nEABAN               -0.1727      0.036     -4.825      0.000      -0.243      -0.103\nSTRNA               -0.0440      0.016     -2.791      0.005      -0.075      -0.013\nINTW                -0.0470      0.004    -10.672      0.000      -0.056      -0.038\nMATBUY              -0.0284      0.014     -2.087      0.037      -0.055      -0.002\nFRSTHO              -0.0838      0.017     -4.878      0.000      -0.117      -0.050\nSTATE_CA             0.8707      0.032     27.224      0.000       0.808       0.933\nSTATE_CO             0.5807      0.030     19.086      0.000       0.521       0.640\nSTATE_CT             0.5223      0.032     16.498      0.000       0.460       0.584\nSTATE_GA             0.2134      0.032      6.638      0.000       0.150       0.276\nSTATE_IN             0.0935      0.032      2.937      0.003       0.031       0.156\nSTATE_LA             0.1476      0.038      3.930      0.000       0.074       0.221\nSTATE_MO             0.2038      0.034      5.954      0.000       0.137       0.271\nSTATE_OH             0.1866      0.033      5.644      0.000       0.122       0.251\nSTATE_OK            -0.1227      0.034     -3.619      0.000      -0.189      -0.056\nSTATE_TX            -0.1758      0.036     -4.933      0.000      -0.246      -0.106\nSTATE_WA             0.7467      0.032     23.306      0.000       0.684       0.809\nMETRO_rural          1.6704      0.011    146.036      0.000       1.648       1.693\nMETRO_urban          1.7469      0.013    134.232      0.000       1.721       1.772\nHHGRAD_Assoc         0.6690      0.017     39.340      0.000       0.636       0.702\nHHGRAD_Bach          0.8017      0.013     61.779      0.000       0.776       0.827\nHHGRAD_Grad          0.8666      0.016     54.170      0.000       0.835       0.898\nHHGRAD_HS Grad       0.6079      0.012     52.167      0.000       0.585       0.631\nHHGRAD_No HS         0.4722      0.022     21.520      0.000       0.429       0.515\nHOWH_bad             1.6447      0.016     99.753      0.000       1.612       1.677\nHOWH_good            1.7726      0.015    120.566      0.000       1.744       1.801\nHOWN_bad             1.6475      0.014    113.724      0.000       1.619       1.676\nHOWN_good            1.7699      0.013    135.416      0.000       1.744       1.796\nDWNPAY_other         1.6475      0.012    136.534      0.000       1.624       1.671\nDWNPAY_prev home     1.7699      0.012    141.839      0.000       1.745       1.794\n==============================================================================\nOmnibus:                    23659.822   Durbin-Watson:                   1.908\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         11333403.763\nSkew:                          -9.560   Prob(JB):                         0.00\nKurtosis:                     133.804   Cond. No.                     3.66e+17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.74e-21. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\nOriginal Model Coefficients:\nconst               3.423659e+00\nZINC2               6.243681e-07\nBATHS               2.116880e-01\nBEDRMS              8.740195e-02\nPER                 9.651158e-03\nZADULT             -1.864398e-02\nNUNITS             -9.324136e-04\nEAPTBL             -4.347375e-02\nECOM1              -2.568009e-02\nECOM2              -8.644739e-02\nEGREEN              9.391337e-03\nEJUNK              -1.264765e-01\nELOW1               2.869608e-02\nESFD                2.944711e-01\nETRANS             -1.515460e-02\nEABAN              -1.621352e-01\nODORA               1.025598e-02\nSTRNA              -3.618157e-02\nINTW               -4.636875e-02\nMATBUY             -2.966116e-02\nFRSTHO             -8.398474e-02\nSTATE_CA            8.813424e-01\nSTATE_CO            5.892119e-01\nSTATE_CT            5.349735e-01\nSTATE_GA            2.262866e-01\nSTATE_IL            1.958510e-02\nSTATE_IN            1.021030e-01\nSTATE_LA            1.617913e-01\nSTATE_MO            2.168589e-01\nSTATE_OH            2.076765e-01\nSTATE_OK           -1.168422e-01\nSTATE_PA            9.699415e-03\nSTATE_TX           -1.675571e-01\nSTATE_WA            7.585296e-01\nMETRO_rural         1.668777e+00\nMETRO_urban         1.754882e+00\nHHGRAD_Assoc        6.698840e-01\nHHGRAD_Bach         8.020125e-01\nHHGRAD_Grad         8.671440e-01\nHHGRAD_HS Grad      6.092694e-01\nHHGRAD_No HS        4.753491e-01\nHOWH_bad            1.647058e+00\nHOWH_good           1.776601e+00\nHOWN_bad            1.652197e+00\nHOWN_good           1.771462e+00\nDWNPAY_other        1.651382e+00\nDWNPAY_prev home    1.772277e+00\ndtype: float64\n\nRefitted Model Coefficients:\nconst               3.417360e+00\nZINC2               6.205976e-07\nBATHS               2.137264e-01\nBEDRMS              9.156783e-02\nEJUNK              -1.298173e-01\nESFD                2.941677e-01\nEABAN              -1.726983e-01\nSTRNA              -4.397253e-02\nINTW               -4.695362e-02\nMATBUY             -2.844721e-02\nFRSTHO             -8.381413e-02\nSTATE_CA            8.706690e-01\nSTATE_CO            5.807213e-01\nSTATE_CT            5.222900e-01\nSTATE_GA            2.133941e-01\nSTATE_IN            9.354597e-02\nSTATE_LA            1.476382e-01\nSTATE_MO            2.037749e-01\nSTATE_OH            1.866253e-01\nSTATE_OK           -1.227290e-01\nSTATE_TX           -1.758373e-01\nSTATE_WA            7.466948e-01\nMETRO_rural         1.670410e+00\nMETRO_urban         1.746950e+00\nHHGRAD_Assoc        6.689516e-01\nHHGRAD_Bach         8.016565e-01\nHHGRAD_Grad         8.666233e-01\nHHGRAD_HS Grad      6.079105e-01\nHHGRAD_No HS        4.722182e-01\nHOWH_bad            1.644713e+00\nHOWH_good           1.772647e+00\nHOWN_bad            1.647458e+00\nHOWN_good           1.769902e+00\nDWNPAY_other        1.647465e+00\nDWNPAY_prev home    1.769895e+00\ndtype: float64\n\nOriginal Model R-squared: 0.30530297069979406\nRefitted Model R-squared: 0.30433932172621814\n\nOriginal Model RMSE: 0.8160229055289854\nRefitted Model RMSE: 0.8165886824730757"
  },
  {
    "objectID": "Housing.html#fit-a-logistic-regression-model",
    "href": "Housing.html#fit-a-logistic-regression-model",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Fit a Logistic Regression Model",
    "text": "Fit a Logistic Regression Model\n\nFit a logistic regression model with the following specifications:\n\nOutcome variable: (indicating whether the buyer made a down payment of 20% or more)\nPredictors: All available variables except AMORT and LPRICE\n\nThe outcome variable is defined as: \\[\\begin{align}\n\\text{GT20DWN} \\,=\\,\\begin{cases}\n1 & \\text{if}\\; \\frac{\\text{LPRICE} - \\text{AMMORT}}{\\text{LPRICE}} &gt; 0.2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\\]\nAnalyze and interpret the following relationships:\nThe association between first-time homeownership ( ) and the probability of making a 20%+ down payment.\nThe association between number of bedrooms ( ) and the probability of making a 20%+ down payment.\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nhomes['GT20DWN'] = np.where((homes['LPRICE'] - homes['AMMORT']) / homes['LPRICE'] &gt; 0.2, 1, 0)\n\nhomes_logreg = homes.copy()\nhomes_logreg = homes_logreg.dropna(subset=['LPRICE', 'AMMORT', 'BEDRMS', 'FRSTHO'])\npredictors = [col for col in homes_logreg.columns if col not in ['AMMORT', 'LPRICE', 'GT20DWN']]\n\nX_logreg = homes_logreg[predictors]\ny_logreg = homes_logreg['GT20DWN']\n\ncategorical_cols_logreg = X_logreg.select_dtypes(include=['object']).columns\nencoder_logreg = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nencoded_data_logreg = encoder_logreg.fit_transform(X_logreg[categorical_cols_logreg])\nencoded_df_logreg = pd.DataFrame(encoded_data_logreg, columns=encoder_logreg.get_feature_names_out(categorical_cols_logreg))\nX_logreg = X_logreg.drop(columns=categorical_cols_logreg)\nX_logreg = pd.concat([X_logreg, encoded_df_logreg], axis=1)\n\n\nmodel_logreg = LogisticRegression(max_iter=1000)\nmodel_logreg.fit(X_logreg, y_logreg)\n\n\nprint(model_logreg.coef_)\n\n\nfirst_home_coef = 0\ntry:\n  first_home_index = X_logreg.columns.get_loc(\"FIRSTHOME_Yes\")\n  first_home_coef = model_logreg.coef_[0][first_home_index]\n  print(f\"Coefficient for FIRSTHOME: {first_home_coef}\")\nexcept KeyError:\n  print(\"FIRSTHOME column not found after encoding or name has changed\")\n\n\nbedrms_coef = 0\ntry:\n    bedrms_index = X_logreg.columns.get_loc(\"BEDRMS\")\n    bedrms_coef = model_logreg.coef_[0][bedrms_index]\n    print(f\"Coefficient for BEDRMS: {bedrms_coef}\")\nexcept KeyError:\n    print(\"BEDRMS column not found after encoding or name has changed\")\n\n\n[[ 1.48955745e-06 -1.05744875e-07  2.38735318e-01 -2.34032641e-02\n  -1.30203374e-01  2.87119156e-02  4.14321959e-03 -1.90106435e-02\n  -1.65706073e-01 -6.59561841e-02  3.68467984e-03 -1.91667485e-02\n   1.69906895e-02 -2.46840055e-01 -6.68010459e-02 -4.65279383e-02\n   3.60294234e-02 -1.10822851e-01 -8.65047721e-02  3.43117032e-01\n  -3.99673128e-01 -2.98067750e-01 -3.67031315e-01  4.33408464e-01\n  -4.60995128e-01  5.34425144e-02 -8.44555914e-02  1.73650096e-01\n   1.61592492e-01  3.47565040e-01 -1.40567832e-01  1.81366961e-01\n   1.02153822e-02 -1.59841958e-01 -3.25920049e-04 -1.49392590e-01\n  -1.29210681e-01  7.48819623e-02  2.31204079e-01 -1.88321175e-01\n  -1.38272771e-01 -1.15534303e-02 -1.38164963e-01 -1.56462872e-01\n   6.74449175e-03 -4.33209186e-01  2.83489407e-01]]\nFIRSTHOME column not found after encoding or name has changed\nCoefficient for BEDRMS: -0.023403264146831273\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "Housing.html#logistic-regression-refit",
    "href": "Housing.html#logistic-regression-refit",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Logistic Regression Refit",
    "text": "Logistic Regression Refit\n\nRefit the logistic regression model, adding interaction terms:\n\nPredictors: all previously included predictors in Question 4 plus the interaction between FRSTHO and BEDRMS\n\nInterpret how the relationship between BEDRMS and the probability of a 20%+ down payment varies depending on whether the buyer is a first-time homeowner (FRSTHO).\n\n\n\nimport pandas as pd\nhomes_logreg['FRSTHO_BEDRMS_interaction'] = homes_logreg['FRSTHO'] * homes_logreg['BEDRMS']\n\npredictors = [col for col in homes_logreg.columns if col not in ['AMMORT', 'LPRICE', 'GT20DWN']]\nX_logreg = homes_logreg[predictors]\ny_logreg = homes_logreg['GT20DWN']\n\ncategorical_cols_logreg = X_logreg.select_dtypes(include=['object']).columns\nencoder_logreg = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\nencoded_data_logreg = encoder_logreg.fit_transform(X_logreg[categorical_cols_logreg])\nencoded_df_logreg = pd.DataFrame(encoded_data_logreg, columns=encoder_logreg.get_feature_names_out(categorical_cols_logreg))\nX_logreg = X_logreg.drop(columns=categorical_cols_logreg)\nX_logreg = pd.concat([X_logreg, encoded_df_logreg], axis=1)\n\nmodel_logreg = LogisticRegression(max_iter=1000)\nmodel_logreg.fit(X_logreg, y_logreg)\n\nprint(model_logreg.coef_)\n\ntry:\n    bedrms_index = X_logreg.columns.get_loc(\"BEDRMS\")\n    bedrms_coef = model_logreg.coef_[0][bedrms_index]\n    print(f\"Coefficient for BEDRMS: {bedrms_coef}\")\n\n    interaction_index = X_logreg.columns.get_loc('FRSTHO_BEDRMS_interaction')\n    interaction_coef = model_logreg.coef_[0][interaction_index]\n    print(f\"Coefficient for interaction term (FRSTHO*BEDRMS): {interaction_coef}\")\nexcept KeyError:\n    print(\"BEDRMS or interaction term column not found.\")\n\n\n\nprint(\"\\nInterpretation:\")\nprint(\"The coefficient for BEDRMS represents the change in the log-odds of a 20%+ down payment for a one-unit increase in BEDRMS, holding all other variables constant.\")\nprint(\"The coefficient for the interaction term (FRSTHO*BEDRMS) represents how the effect of BEDRMS on the log-odds of a 20%+ down payment changes depending on the first-time homeownership status.\")\n\nprint(\"\\nSpecifically:\")\nprint(f\"For non-first-time homeowners (FRSTHO = 0), the effect of an additional bedroom is approximately {bedrms_coef:.3f}.\")\nprint(f\"For first-time homeowners (FRSTHO = 1), the effect of an additional bedroom is approximately {bedrms_coef + interaction_coef:.3f}.\")\n\n[[ 1.15073444e-06 -1.25047456e-07  1.75996195e-01 -9.34307504e-03\n  -8.55094253e-02 -1.61955846e-02  9.75041552e-04 -5.19510804e-02\n  -1.32212172e-01 -3.29755676e-02  9.75541618e-03 -1.44271457e-02\n  -1.51026477e-02 -1.44115376e-01 -4.97090780e-02 -3.12271416e-02\n   4.06502035e-03 -1.25828080e-01 -1.03182804e-01  2.13145623e-01\n  -7.97032944e-02 -5.95545614e-02 -1.51349054e-01 -1.79056923e-01\n   2.08516409e-01 -1.90197206e-01  2.02440893e-02 -4.07148849e-02\n   7.12997834e-02  7.94414795e-02  1.56424353e-01 -6.42478857e-02\n   7.69113441e-02 -1.56559306e-03 -8.54431211e-02  4.16996894e-02\n  -1.41436974e-01 -6.55390680e-02  6.03899565e-02  1.43154002e-01\n  -1.78211264e-01 -5.95308194e-02 -2.39773988e-02 -7.57598273e-02\n  -1.07275833e-01  7.53856272e-03 -4.78016115e-01  3.78278756e-01]]\nCoefficient for BEDRMS: -0.009343075041082933\nCoefficient for interaction term (FRSTHO*BEDRMS): -0.05955456137963277\n\nInterpretation:\nThe coefficient for BEDRMS represents the change in the log-odds of a 20%+ down payment for a one-unit increase in BEDRMS, holding all other variables constant.\nThe coefficient for the interaction term (FRSTHO*BEDRMS) represents how the effect of BEDRMS on the log-odds of a 20%+ down payment changes depending on the first-time homeownership status.\n\nSpecifically:\nFor non-first-time homeowners (FRSTHO = 0), the effect of an additional bedroom is approximately -0.009.\nFor first-time homeowners (FRSTHO = 1), the effect of an additional bedroom is approximately -0.069.\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "Housing.html#separate-logistic-regression-models",
    "href": "Housing.html#separate-logistic-regression-models",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Separate Logistic Regression Models",
    "text": "Separate Logistic Regression Models\n\nFit separate logistic regression models (with the same model specification as in Question 4) for two subsets of home data:\n\n\nHomes worth VALUE &gt;= 175k.\nHomes worth VALUE &lt; 175k.\n\n\nCompare residual deviance, RMSE, and classification performance between the two models.\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, log_loss, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nhomes = pd.read_csv('https://bcdanl.github.io/data/american_housing_survey.csv')\n\nhomes['GT20DWN'] = np.where((homes['LPRICE'] - homes['AMMORT']) / homes['LPRICE'] &gt; 0.2, 1, 0)\n\nhomes_logreg = homes.copy()\nhomes_logreg = homes_logreg.dropna(subset=['LPRICE', 'AMMORT', 'BEDRMS', 'FRSTHO', 'VALUE']).reset_index(drop=True)\n\npredictors = [col for col in homes_logreg.columns if col not in ['AMMORT', 'LPRICE', 'GT20DWN', 'VALUE']]\n\nhigh_value_homes = homes_logreg[homes_logreg['VALUE'] &gt;= 175000].copy().reset_index(drop=True)\nlow_value_homes = homes_logreg[homes_logreg['VALUE'] &lt; 175000].copy().reset_index(drop=True)\n\ndef fit_and_evaluate_model(df, predictors):\n    X = df[predictors].copy()\n    y = df['GT20DWN'].copy()\n\n    categorical_cols = X.select_dtypes(include=['object']).columns\n    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n    encoded_data = encoder.fit_transform(X[categorical_cols])\n    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n    X = X.drop(columns=categorical_cols)\n    X = pd.concat([X, encoded_df], axis=1).reset_index(drop=True)\n\n    imputer = SimpleImputer(strategy='mean')\n    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns).reset_index(drop=True)\n\n    model = LogisticRegression(max_iter=1000)\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    y_prob = model.predict_proba(X)[:, 1]\n\n    rmse = np.sqrt(mean_squared_error(y, y_prob))\n    deviance = log_loss(y, y_prob)\n    accuracy = accuracy_score(y, y_pred)\n    precision = precision_score(y, y_pred, zero_division=0)\n    recall = recall_score(y, y_pred, zero_division=0)\n    f1 = f1_score(y, y_pred, zero_division=0)\n\n    return {\n        'RMSE': rmse,\n        'Deviance': deviance,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1,\n        'Model': model\n    }\n\nhigh_value_results = fit_and_evaluate_model(high_value_homes, predictors)\nlow_value_results = fit_and_evaluate_model(low_value_homes, predictors)\n\nprint(\"High Value Homes (VALUE &gt;= 175k):\")\nfor metric, value in high_value_results.items():\n    if metric != 'Model':\n        print(f\"{metric}: {value}\")\n\nprint(\"\\nLow Value Homes (VALUE &lt; 175k):\")\nfor metric, value in low_value_results.items():\n    if metric != 'Model':\n        print(f\"{metric}: {value}\")\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nHigh Value Homes (VALUE &gt;= 175k):\nRMSE: 0.45140461640279045\nDeviance: 0.5938783596732359\nAccuracy: 0.6852253862852765\nPrecision: 0.6095820591233435\nRecall: 0.413126079447323\nF1 Score: 0.49248507309038503\n\nLow Value Homes (VALUE &lt; 175k):\nRMSE: 0.39885217504215476\nDeviance: 0.4926438966145569\nAccuracy: 0.7832945435738299\nPrecision: 0.5630252100840336\nRecall: 0.039621525724423415\nF1 Score: 0.07403314917127071"
  },
  {
    "objectID": "Housing.html#conclusion",
    "href": "Housing.html#conclusion",
    "title": "Exploring Housing Data with Regression Analysis: A Deep Dive into Home Values",
    "section": "Conclusion",
    "text": "Conclusion\nThrough this analysis, we‚Äôve explored the relationships between key housing characteristics, such as the number of bedrooms, and house values. We also delved into predicting down payments with logistic regression, adding depth to our understanding by including interaction terms. Regression analysis is a powerful tool for uncovering insights in housing data, providing actionable insights for buyers, sellers, and real estate professionals alike."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#introduction-to-hadoop",
    "href": "posts/PySpark Basics/PySparkbasics.html#introduction-to-hadoop",
    "title": "PySpark Basics",
    "section": "Introduction to Hadoop",
    "text": "Introduction to Hadoop\n\nDefinition\n\nAn open-source software framework for storing and processing large data sets.\n\nComponents\n\nHadoop Distributed File System (HDFS): Distributed data storage.\nMapReduce: Data processing model.\n\nPurpose\n\nEnables distributed processing of large data sets across clusters of computers."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#hadoop-architecture---hdfs",
    "href": "posts/PySpark Basics/PySparkbasics.html#hadoop-architecture---hdfs",
    "title": "PySpark Basics",
    "section": "Hadoop Architecture - HDFS",
    "text": "Hadoop Architecture - HDFS\n\nHDFS\n\nDivides data into blocks and distributes them across different servers for processing.\nProvides a highly redundant computing environment\n\nAllows the application to keep running even if individual servers fail."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#hadoop-architecture--mapreduce",
    "href": "posts/PySpark Basics/PySparkbasics.html#hadoop-architecture--mapreduce",
    "title": "PySpark Basics",
    "section": "Hadoop Architecture- MapReduce",
    "text": "Hadoop Architecture- MapReduce\n\nMapReduce: Distributes the processing of big data files across a large cluster of machines.\n\nHigh performance is achieved by breaking the processing into small units of work that can be run in parallel across nodes in the cluster.\n\nMap Phase: Filters and sorts data.\n\ne.g., Sorting customer orders based on their product IDs, with each group corresponding to a specific product ID.\n\nReduce Phase: Summarizes and aggregates results.\n\ne.g., Counting the number of orders within each group, thereby determining the frequency of each product ID."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#how-hadoop-works",
    "href": "posts/PySpark Basics/PySparkbasics.html#how-hadoop-works",
    "title": "PySpark Basics",
    "section": "How Hadoop Works",
    "text": "How Hadoop Works\n\nData Distribution\n\n\nLarge data sets are split into smaller blocks.\n\n\nData Storage\n\n\nBlocks are stored across multiple servers in the cluster.\n\n\nProcessing with MapReduce\n\n\nMap Tasks: Executed on servers where data resides, minimizing data movement.\nReduce Tasks: Combine results from map tasks to produce final output.\n\n\nFault Tolerance\n\n\nData replication ensures processing continues even if servers fail."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#extending-hadoop-for-real-time-processing",
    "href": "posts/PySpark Basics/PySparkbasics.html#extending-hadoop-for-real-time-processing",
    "title": "PySpark Basics",
    "section": "Extending Hadoop for Real-Time Processing",
    "text": "Extending Hadoop for Real-Time Processing\n\nLimitation of Hadoop\n\nHadoop is originally designed for batch processing.\n\nBatch Processing: Data or tasks are collected over a period of time and then processed all at once, typically at scheduled times or during periods of low activity.\nResults come after the entire dataset is analyzed.\n\n\nReal-Time Processing Limitation:\n\nHadoop cannot natively process real-time streaming data (e.g., stock prices flowing into stock exchanges, live sensor data)\n\nExtending Hadoop‚Äôs Capabilities\n\nBoth Apache Storm and Apache Spark can run on top of Hadoop clusters, utilizing HDFS for storage."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#spark",
    "href": "posts/PySpark Basics/PySparkbasics.html#spark",
    "title": "PySpark Basics",
    "section": "Spark",
    "text": "Spark\n\nApache Spark: distributed processing system used for big data workloads. a unified computing engine and computer clusters\n\nIt contains a set of libraries for parallel processing for data analysis, machine learning, graph analysis, and streaming live data."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#spark-application-structure-on-a-cluster-of-computers",
    "href": "posts/PySpark Basics/PySparkbasics.html#spark-application-structure-on-a-cluster-of-computers",
    "title": "PySpark Basics",
    "section": "Spark Application Structure on a Cluster of Computers",
    "text": "Spark Application Structure on a Cluster of Computers\n\nDriver Process\n\nCommunicates with the cluster manager to acquire worker nodes.\nBreaks the application into smaller tasks if resources are allocated.\n\nCluster Manager\n\nDecides if Spark can use cluster resources (machines/nodes).\nAllocates necessary nodes to Spark applications.\n\nWorker Nodes\n\nExecute tasks assigned by the driver program.\nSend results back to the driver after execution.\nCan communicate with each other if needed during task execution."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#hadoop-mapreduce-the-challenge",
    "href": "posts/PySpark Basics/PySparkbasics.html#hadoop-mapreduce-the-challenge",
    "title": "PySpark Basics",
    "section": "Hadoop MapReduce: The Challenge",
    "text": "Hadoop MapReduce: The Challenge\n\nSequential Multi-Step Process:\n\nReads data from the cluster.\nProcesses data.\nWrites results back to HDFS.\n\nDisk Input/Output Latency:\n\nEach step requires disk read/write.\nResults in slower performance due to latency."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#apache-spark-the-solution",
    "href": "posts/PySpark Basics/PySparkbasics.html#apache-spark-the-solution",
    "title": "PySpark Basics",
    "section": "Apache Spark: The Solution",
    "text": "Apache Spark: The Solution\n\nIn-Memory Processing:\n\nLoads data into memory once.\nPerforms all operations in-memory.\n\nData Reuse:\n\nCaches data for reuse in multiple operations (ideal for iterative tasks like machine learning).\n\nFaster Execution:\n\nEliminates multiple disk I/O steps.\nDramatically reduces latency for interactive analytics and real-time processing."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#pyspark-spark-python",
    "href": "posts/PySpark Basics/PySparkbasics.html#pyspark-spark-python",
    "title": "PySpark Basics",
    "section": "PySpark = Spark + Python",
    "text": "PySpark = Spark + Python\n\npyspark is a Python API to Apache Spark.\n\nAPI: application programming interface, the set of functions, classes, and variables provided for you to interact with.\nSpark itself is coded in a different programming language, called Scala.\n\nWe can combine Python, pandas, and PySpark in one program.\n\nKoalas (now called pyspark.pandas) provides a pandas-like porcelain on top of PySpark."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#spark-dataframe-vs.-pandas-dataframe",
    "href": "posts/PySpark Basics/PySparkbasics.html#spark-dataframe-vs.-pandas-dataframe",
    "title": "PySpark Basics",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\n\nWhat makes a Spark DataFrame different from other DataFrame?\nSpark DataFrames are designed for big data and distributed computing.\nSpark DataFrame:\n\nData is distributed across a cluster of machines.\nOperations are executed in parallel on multiple nodes.\nCan process datasets that exceed the memory of a single machine.\n\nOther DataFrames (e.g., Pandas):\n\nOperate on a single machine.\nEntire dataset must fit into memory.\nLimited by local system resources."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#lazy-evalutation-and-optimization",
    "href": "posts/PySpark Basics/PySparkbasics.html#lazy-evalutation-and-optimization",
    "title": "PySpark Basics",
    "section": "Lazy Evalutation and Optimization",
    "text": "Lazy Evalutation and Optimization\n\nSpark DataFrame:\n\nUses lazy evaluation: transformations are not computed until an action is called.\nOptimize query execution.\n\nOther DataFrames:\n\nOperations are evaluated eagerly (immediately).\nNo built-in query optimization across multiple operations."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#scalability",
    "href": "posts/PySpark Basics/PySparkbasics.html#scalability",
    "title": "PySpark Basics",
    "section": "Scalability",
    "text": "Scalability\n\nSpark DataFrame:\n\nDesigned to scale to petabytes of data.\nUtilizes distributed storage and computing resources.\nIdeal for large-scale data processing and analytics.\n\nOther DataFrames:\n\nBest suited for small to medium datasets.\nLimited by the hardware of a single computer."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#fault-tolerance",
    "href": "posts/PySpark Basics/PySparkbasics.html#fault-tolerance",
    "title": "PySpark Basics",
    "section": "Fault Tolerance",
    "text": "Fault Tolerance\n\nSpark DataFrame:\n\nBuilt on Resilient Distributed Datasets (RDDs).\nAutomatically recovers lost data if a node fails.\nEnsures high reliability in distributed environments.\n\nOther DataFrames:\n\nTypically lack built-in fault tolerance.\nFailures on a single machine can result in data loss."
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#the-sparksession-entry-point",
    "href": "posts/PySpark Basics/PySparkbasics.html#the-sparksession-entry-point",
    "title": "PySpark Basics",
    "section": "The SparkSession Entry Point",
    "text": "The SparkSession Entry Point\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(‚Äúlocal[*]‚Äù).getOrCreate()"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#reading-a-csv-file-into-the-spark-framework",
    "href": "posts/PySpark Basics/PySparkbasics.html#reading-a-csv-file-into-the-spark-framework",
    "title": "PySpark Basics",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework\n\npath = ‚Äòhttps://bcdanl.github.io/data/df.csv‚Äô\ndf = spark.read.csv(path, inferSchema=True, header=True)\ndf.show()\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(‚Äúlocal[*]‚Äù).getOrCreate()\ndf_pd = pd.read_csv(‚Äòhttps://bcdanl.github.io/data/nba.csv‚Äô)\ndf = spark.createDataFrame(df_pd)"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#getting-a-summary-of-data",
    "href": "posts/PySpark Basics/PySparkbasics.html#getting-a-summary-of-data",
    "title": "PySpark Basics",
    "section": "Getting a Summary of Data",
    "text": "Getting a Summary of Data\n\ndf.printSchema(): prints the schema (column names and data types).\ndf.columns: returns the list of columns.\ndf.dtypes: returns a list of tuples (columnName, dataType).\ndf.count(): returns the total number of rows.\ndf.describe(): returns basic statistics of numerical/string columns (mean, count, std, min, max).\ndf.describe().show()"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#selecting-columns",
    "href": "posts/PySpark Basics/PySparkbasics.html#selecting-columns",
    "title": "PySpark Basics",
    "section": "Selecting Columns",
    "text": "Selecting Columns\n\nSingle column -&gt; returns a DataFrame with one column df.select(‚ÄúName‚Äù).show(5)\nMultiple columns -&gt; pass a list-like of column names df.select(‚ÄúName‚Äù, ‚ÄúTeam‚Äù, ‚ÄúSalary‚Äù).show(5)"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#counting-methods",
    "href": "posts/PySpark Basics/PySparkbasics.html#counting-methods",
    "title": "PySpark Basics",
    "section": "Counting Methods",
    "text": "Counting Methods"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#sorting-methods",
    "href": "posts/PySpark Basics/PySparkbasics.html#sorting-methods",
    "title": "PySpark Basics",
    "section": "Sorting Methods",
    "text": "Sorting Methods\n\nSort by a single column ascending\n\ndf.orderBy(‚ÄúName‚Äù).show(5)\n\nSort by descending\n\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(‚ÄúSalary‚Äù)).show(5)\n\nSort by multiple columns\n\ndf.orderBy([‚ÄúTeam‚Äù, desc(‚ÄúSalary‚Äù)]).show(5)\n\nSorting by One or More Variables\n\nnsmallest example:\n\ndf.orderBy(‚ÄúSalary‚Äù).limit(5).show()\n\nnlargest example:\n\ndf.orderBy(desc(‚ÄúSalary‚Äù)).limit(5).show()\n\n\nEquivalent of Pandas nsmallest or nlargest"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#indexing-and-row-access",
    "href": "posts/PySpark Basics/PySparkbasics.html#indexing-and-row-access",
    "title": "PySpark Basics",
    "section": "Indexing and Row Access",
    "text": "Indexing and Row Access\n\ndf.filter(‚ÄúTeam == ‚ÄòNew York Knicks‚Äô‚Äù).show()\ndf.limit(5).show()\ndf.take(5)\ndf.collect()"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#adding-removing-renaming-and-relocating-variables",
    "href": "posts/PySpark Basics/PySparkbasics.html#adding-removing-renaming-and-relocating-variables",
    "title": "PySpark Basics",
    "section": "Adding, Removing, Renaming, and Relocating Variables",
    "text": "Adding, Removing, Renaming, and Relocating Variables\n\ndf = df.withColumn(‚ÄúSalary_k‚Äù, col(‚ÄúSalary‚Äù) / 1000)\n\nAdding Columns with withColumn()\n\ndf = df.drop(‚ÄúSalary_k‚Äù) # remove a single column\ndf = df.drop(‚ÄúSalary_2x‚Äù, ‚ÄúSalary_3x‚Äù) # remove multiple columns\n\nRemoving Columns with drop()\n\ndf = df.withColumnRenamed(‚ÄúBirthday‚Äù, ‚ÄúDateOfBirth‚Äù)\n\nRenaming Columns with withColumnRenamed()\n\ndf = df.select(‚ÄúName‚Äù, ‚ÄúTeam‚Äù, ‚ÄúPosition‚Äù, ‚ÄúSalary‚Äù, ‚ÄúDateOfBirth‚Äù)\n\nRearranging Columns"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#mathematical-vectorized-operations",
    "href": "posts/PySpark Basics/PySparkbasics.html#mathematical-vectorized-operations",
    "title": "PySpark Basics",
    "section": "Mathematical & Vectorized Operations",
    "text": "Mathematical & Vectorized Operations\n\ndf.selectExpr( ‚Äúmean(Salary) as mean_salary‚Äù, ‚Äúmin(Salary) as min_salary‚Äù, ‚Äúmax(Salary) as max_salary‚Äù, ‚Äústddev_pop(Salary) as std_salary‚Äù ).show()\nfrom pyspark.sql import functions as F\nsalary_mean = df.select(F.avg(‚ÄúSalary‚Äù).alias(‚Äúmean_salary‚Äù)).collect()[0][‚Äúmean_salary‚Äù]\ndf2 = ( df .withColumn(‚ÄúSalary_2x‚Äù, F.col(‚ÄúSalary‚Äù) * 2) .withColumn( ‚ÄúName_w_Position‚Äù, Name and Position F.concat(F.col(‚ÄúName‚Äù), F.lit(‚Äù (‚Äú), F.col(‚ÄùPosition‚Äù), F.lit(‚Äú)‚Äù))) .withColumn( ‚ÄúSalary_minus_Mean‚Äù, salary F.col(‚ÄúSalary‚Äù) - F.lit(salary_mean)) )"
  },
  {
    "objectID": "posts/PySpark Basics/PySparkbasics.html#filtering-by-a-condition",
    "href": "posts/PySpark Basics/PySparkbasics.html#filtering-by-a-condition",
    "title": "PySpark Basics",
    "section": "Filtering by a Condition",
    "text": "Filtering by a Condition\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(‚Äúlocal[*]‚Äù).getOrCreate()\ndf_pd = pd.read_csv(‚Äúhttps://bcdanl.github.io/data/employment.csv‚Äù)\ndf_pd = df_pd.where(pd.notnull(df_pd), None)\ndf = spark.createDataFrame(df_pd)\ndf.filter(col(‚ÄúSalary‚Äù) &gt; 100000).show()\ndf.filter(( col(‚ÄúTeam‚Äù) == ‚ÄúFinance‚Äù ) & (col(‚ÄúSalary‚Äù) &gt;= 100000 )).show()\ndf.filter((col(‚ÄúTeam‚Äù) == ‚ÄúFinance‚Äù) | (col(‚ÄúTeam‚Äù) == ‚ÄúLegal‚Äù) | (col(‚ÄúTeam‚Äù) == ‚ÄúSales‚Äù)).show()"
  },
  {
    "objectID": "posts/Beer Markets/BeerMarkets.html#variable-description",
    "href": "posts/Beer Markets/BeerMarkets.html#variable-description",
    "title": "Beer Markets",
    "section": "Variable Description",
    "text": "Variable Description\n\nhh: an identifier of the household;\nX_purchase_desc: details on the purchased item;\nquantity: the number of items purchased;\nbrand: Bud Light, Busch Light, Coors Light, Miller Lite, or Natural Light;\ndollar_spent: total dollar value of purchase;\nbeer_floz: total volume of beer, in fluid ounces;\nprice_per_floz: price per fl.oz. (i.e., beer spent/beer floz);\ncontainer: the type of container;\npromo: Whether the item was promoted (coupon or otherwise);\nmarket: Scan-track market (or state if rural); demographic data, including gender, marital status, household income, class of work, race, education, age, the size of household, and whether or not the household has a microwave or a dishwasher."
  },
  {
    "objectID": "posts/Beer Markets/BeerMarkets.html#q1a",
    "href": "posts/Beer Markets/BeerMarkets.html#q1a",
    "title": "Beer Markets",
    "section": "Q1a üçª",
    "text": "Q1a üçª\n\nFind the top 5 markets in terms of the total beer_floz.\nFind the top 5 markets in terms of the total beer_floz of BUD LIGHT.\nFind the top 5 markets in terms of the total beer_floz of BUSCH LIGHT.\nFind the top 5 markets in terms of the total beer_floz of COORS LIGHT.\nFind the top 5 markets in terms of the total beer_floz of MILLER LITE.\nFind the top 5 markets in terms of the total beer_floz of NATURAL LIGHT.\n\n\nQ1a1 &lt;- beer_mkts %&gt;% \n   group_by(market) %&gt;% \n   summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %&gt;% \n   arrange(-beer_floz_tot) %&gt;% \n   slice(1:5)\n\nQ1a_bud &lt;- beer_mkts %&gt;% \n  filter(brand == \"BUD LIGHT\") %&gt;% \n  group_by(market) %&gt;% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %&gt;% \n  arrange(-beer_floz_tot) %&gt;% \n  slice(1:5)\n\nQ1a_busch &lt;- beer_mkts %&gt;% \n  filter(brand == \"BUSCH LIGHT\") %&gt;% \n  group_by(market) %&gt;% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %&gt;% \n  arrange(-beer_floz_tot) %&gt;% \n  slice(1:5)\n\nQ1a_coors &lt;- beer_mkts %&gt;% \n  filter(brand == \"COORS LIGHT\") %&gt;% \n  group_by(market) %&gt;% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %&gt;% \n  arrange(-beer_floz_tot) %&gt;% \n  slice(1:5)\n\nQ1a_miller &lt;- beer_mkts %&gt;% \n  filter(brand == \"MILLER LITE\") %&gt;% \n  group_by(market) %&gt;% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %&gt;% \n  arrange(-beer_floz_tot) %&gt;% \n  slice(1:5)\n\nQ1a_natural &lt;- beer_mkts %&gt;% \n  filter(brand == \"NATURAL LIGHT\") %&gt;% \n  group_by(market) %&gt;% \n  summarize(beer_floz_tot = sum(beer_floz, na.rm = T)) %&gt;% \n  arrange(-beer_floz_tot) %&gt;% \n  slice(1:5)"
  },
  {
    "objectID": "posts/Beer Markets/BeerMarkets.html#q1b",
    "href": "posts/Beer Markets/BeerMarkets.html#q1b",
    "title": "Beer Markets",
    "section": "Q1b üçª",
    "text": "Q1b üçª\n\nFor households that purchased BUD LIGHT at least once, what fraction of households did purchase only BUD LIGHT?\nFor households that purchased BUSCH LIGHT at least once, what fraction of households did purchase only BUSCH LIGHT?\nFor households that purchased COORS LIGHT at least once, what fraction of households did purchase only COORS LIGHT?\nFor households that purchased MILLER LITE at least once, what fraction of households did purchase only MILLER LITE?\nFor households that purchased NATURAL LIGHT at least once, what fraction of households did purchase only NATURAL LIGHT?\nWhich beer brand does have the largest proportion of such loyal consumers?\n\n\nq1b &lt;- beer_mkts %&gt;% \n  mutate(bud = ifelse(brand==\"BUD LIGHT\", 1, 0), # 1 if brand==\"BUD LIGHT\"; 0 otherwise\n         busch = ifelse(brand==\"BUSCH LIGHT\", 1, 0),\n         coors = ifelse(brand==\"COORS LIGHT\", 1, 0),\n         miller = ifelse(brand==\"MILLER LITE\", 1, 0),\n         natural = ifelse(brand==\"NATURAL LIGHT\", 1, 0),\n         .after = hh) %&gt;% \n  select(hh:natural) %&gt;%  # select the variables we need\n  group_by(hh) %&gt;% \n  summarise(n_transactions = n(), # number of beer transactions for each hh\n            n_bud = sum(bud), # number of BUD LIGHT transactions for each hh\n            n_busch = sum(busch), \n            n_coors = sum(coors), \n            n_miller = sum(miller), \n            n_natural = sum(natural) \n  ) %&gt;% \n  summarise(loyal_bud = sum(n_transactions == n_bud) / sum(n_bud &gt; 0), \n              # sum(n_transactions == n_bud) : the number of households that purchased BUD LIGHT only\n              # sum(n_bud &gt; 0) : the number of households that purchased BUD LIGHT at least once.\n            loyal_busch = sum(n_transactions == n_busch) / sum(n_busch &gt; 0),\n            loyal_coors = sum(n_transactions == n_coors) / sum(n_coors &gt; 0),\n            loyal_miller = sum(n_transactions == n_miller) / sum(n_miller &gt; 0),\n            loyal_natural = sum(n_transactions == n_natural) / sum(n_natural &gt; 0)\n  )\n\nq1b\n\n# A tibble: 1 √ó 5\n  loyal_bud loyal_busch loyal_coors loyal_miller loyal_natural\n      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1     0.660       0.473       0.639        0.631         0.510"
  },
  {
    "objectID": "posts/Beer Markets/BeerMarkets.html#q1c",
    "href": "posts/Beer Markets/BeerMarkets.html#q1c",
    "title": "Beer Markets",
    "section": "Q1c üçª",
    "text": "Q1c üçª\n\nFor each household, calculate the number of beer transactions.\nFor each household, calculate the proportion of each beer brand choice.\n\n\nq1c &lt;- beer_mkts %&gt;% \n  count(hh, brand) %&gt;% \n  group_by(hh) %&gt;% \n  mutate(n_tot = sum(n)) %&gt;%  # n_tot : the number of beer transactions\n  arrange(hh, brand) %&gt;% \n  mutate( prop = n / n_tot ) # prop: the proportion of each beer brand choice\n\nq1c\n\n# A tibble: 13,202 √ó 5\n# Groups:   hh [10,408]\n        hh brand           n n_tot  prop\n     &lt;int&gt; &lt;chr&gt;       &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 2000235 BUD LIGHT       1     1 1    \n 2 2000417 COORS LIGHT     8     8 1    \n 3 2000711 COORS LIGHT    13    13 1    \n 4 2000946 BUD LIGHT       1     2 0.5  \n 5 2000946 MILLER LITE     1     2 0.5  \n 6 2001521 BUD LIGHT       6    11 0.545\n 7 2001521 COORS LIGHT     3    11 0.273\n 8 2001521 MILLER LITE     2    11 0.182\n 9 2001531 BUSCH LIGHT     1     1 1    \n10 2001581 BUSCH LIGHT     5     5 1    \n# ‚Ñπ 13,192 more rows"
  },
  {
    "objectID": "posts/Spotify All/SpotifyAll.html#variable-description",
    "href": "posts/Spotify All/SpotifyAll.html#variable-description",
    "title": "Spotify All",
    "section": "Variable Description",
    "text": "Variable Description\n\npid: playlist ID; unique ID for playlist\nplaylist_name: a name of playlist\npos: a position of the track within a playlist (starting from 0)\nartist_name: name of the track‚Äôs primary artist\ntrack_name: name of the track\nduration_ms: duration of the track in milliseconds\nalbum_name: name of the track‚Äôs album"
  },
  {
    "objectID": "posts/Spotify All/SpotifyAll.html#q1a",
    "href": "posts/Spotify All/SpotifyAll.html#q1a",
    "title": "Spotify All",
    "section": "Q1a üé∂",
    "text": "Q1a üé∂\n\nFind the ten most popular song. üéµ\n\nA value of a song is defined as a combination of a artist_name value and a track_name value.\nWho are artists for those ten most popular song?\n\n\n\nQ1a &lt;- spotify_all %&gt;% \n  count(artist_name, track_name) %&gt;% \n  arrange(-n) %&gt;% \n  head(10)\n\nQ1a\n\n        artist_name                          track_name   n\n1             Drake                           One Dance 143\n2    Kendrick Lamar                             HUMBLE. 142\n3  The Chainsmokers                              Closer 129\n4              DRAM         Broccoli (feat. Lil Yachty) 127\n5       Post Malone                     Congratulations 119\n6             Migos Bad and Boujee (feat. Lil Uzi Vert) 117\n7              KYLE             iSpy (feat. Lil Yachty) 115\n8      Lil Uzi Vert                       XO TOUR Llif3 113\n9             Amin√©                            Caroline 107\n10           Khalid                            Location 102"
  },
  {
    "objectID": "posts/Spotify All/SpotifyAll.html#q1b",
    "href": "posts/Spotify All/SpotifyAll.html#q1b",
    "title": "Spotify All",
    "section": "Q1b üéº",
    "text": "Q1b üéº\n\nFind the five most popular artist in terms of the number of occurrences in the data.frame, spotify_all.\nWhat is the most popular song for each of the five most popular artist?\n\n\nQ1b &lt;- spotify_all %&gt;% \n  group_by(artist_name) %&gt;% \n  mutate(n_popular_artist = n()) %&gt;% \n  ungroup() %&gt;% \n  mutate( artist_ranking = dense_rank( desc(n_popular_artist) ) ) %&gt;% \n  filter( artist_ranking &lt;= 5) %&gt;% \n  group_by(artist_name, track_name) %&gt;% \n  mutate(n_popular_track = n()) %&gt;% \n  group_by(artist_name) %&gt;% \n  mutate(track_ranking = dense_rank( desc(n_popular_track) ) ) %&gt;% \n  filter( track_ranking &lt;= 2) %&gt;%   # I just wanted to see the top two tracks for each artist\n  select(artist_name, artist_ranking, n_popular_artist, track_name, track_ranking, n_popular_track) %&gt;% \n  distinct() %&gt;% \n  arrange(artist_ranking, track_ranking)\n\nQ1b\n\n# A tibble: 10 √ó 6\n# Groups:   artist_name [5]\n   artist_name    artist_ranking n_popular_artist track_name  track_ranking\n   &lt;chr&gt;                   &lt;int&gt;            &lt;int&gt; &lt;chr&gt;               &lt;int&gt;\n 1 Drake                       1             2715 One Dance               1\n 2 Drake                       1             2715 Jumpman                 2\n 3 Kanye West                  2             1065 Gold Digger             1\n 4 Kanye West                  2             1065 Stronger                2\n 5 Kendrick Lamar              3             1035 HUMBLE.                 1\n 6 Kendrick Lamar              3             1035 DNA.                    2\n 7 Rihanna                     4              915 Needed Me               1\n 8 Rihanna                     4              915 Work                    2\n 9 The Weeknd                  5              913 Starboy                 1\n10 The Weeknd                  5              913 The Hills               2\n# ‚Ñπ 1 more variable: n_popular_track &lt;int&gt;"
  },
  {
    "objectID": "posts/Spotify All/SpotifyAll.html#q1c",
    "href": "posts/Spotify All/SpotifyAll.html#q1c",
    "title": "Spotify All",
    "section": "Q1c üéπ",
    "text": "Q1c üéπ\nProvide both (1) ggplot codes and (2) a couple of sentences to describe the relationship between pos and the ten most popular artists.\n\nQ1c &lt;- spotify_all %&gt;% \n  group_by(artist_name) %&gt;% \n  mutate(n_popular_artist = n()) %&gt;% \n  ungroup() %&gt;% \n  mutate( artist_ranking = dense_rank( desc(n_popular_artist) ) ) %&gt;% \n  filter( artist_ranking &lt;= 10) \n  \n# boxplot\nggplot(Q1c,\n       aes(x = pos, y = fct_reorder(artist_name, -artist_ranking)) ) +\n  geom_boxplot() +\n  stat_summary(\n    fun = mean,\n    color = 'red'\n  )\n\n\n\n\n\n\n\n\nHistogram\n\n# histogram\nggplot(Q1c) +\n  geom_histogram(aes(x = pos), binwidth = 1) + \n  facet_grid(fct_reorder(artist_name, artist_ranking) ~ .  , switch = \"y\") +\n  theme(strip.text.y.left = element_text(angle = 0))\n\n\n\n\n\n\n\n\n\nAll are skewed right.\n\nUsers tend to locate these popular artists‚Äô songs early in their playlist.\n\nThe distribution of pos does not seem to vary a lot across the ten most popular artists.\nAnything noticeable can be mentioned."
  },
  {
    "objectID": "posts/Spotify All/SpotifyAll.html#q1d",
    "href": "posts/Spotify All/SpotifyAll.html#q1d",
    "title": "Spotify All",
    "section": "Q1d üéµ",
    "text": "Q1d üéµ\nCreate the data.frame with pid-artist level of observations with the following four variables:\n\npid: playlist id\nplaylist_name: name of playlist\nartist: name of the track‚Äôs primary artist, which appears only once within a playlist\nn_artist: number of occurrences of artist within a playlist\n\n\nQ1d &lt;- spotify_all %&gt;% \n  count(pid, playlist_name, artist_name) %&gt;% \n  rename(n_artist = n) %&gt;% \n  arrange(pid, -n_artist, artist_name)\n\n\nrmarkdown::paged_table(Q1d)"
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#variable-description",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#variable-description",
    "title": "Restaurant Inspection",
    "section": "Variable Description",
    "text": "Variable Description\n\nCAMIS:\n\nhis is an unique identifier for the entity (restaurant);\n10-digit integer\n\nDBA:\n\nThis field represents the name (doing business as) of the entity (restaurant); + Public business name, may change at discretion of restaurant owner\n\nBORO:\n\nBorough in which the entity (restaurant) is located.;\n‚Ä¢ 1 = MANHATTAN\n‚Ä¢ 2 = BRONX\n‚Ä¢ 3 = BROOKLYN\n‚Ä¢ 4 = QUEENS\n‚Ä¢ 5 = STATEN ISLAND\n‚Ä¢ 0 = Missing;\n\nCUISINE DESCRIPTION:\n\nThis field describes the entity (restaurant) cuisine.\n\nACTION:\n\nThis field represents the actions that is associated with each restaurant inspection. ;\n‚Ä¢ Violations were cited in the following area(s).\n‚Ä¢ No violations were recorded at the time of this inspection.\n‚Ä¢ Establishment re-opened by DOHMH\n‚Ä¢ Establishment re-closed by DOHMH\n‚Ä¢ Establishment Closed by DOHMH.\n‚Ä¢ Violations were cited in the following area(s) and those requiring immediate action were addressed.\n\nVIOLATION CODE:\n\nViolation code associated with an establishment (restaurant) inspection\n\nVIOLATION DESCRIPTION:\n\nViolation description associated with an establishment (restaurant) inspection\n\nCRITICAL FLAG:\n\nIndicator of critical violation;\n‚Ä¢ Critical\n‚Ä¢ Not Critical\n‚Ä¢ Not Applicable;\nCritical violations are those most likely to contribute to food-borne illness\n\nSCORE:\n\nTotal score for a particular inspection;\n\nGRADE:\n\nGrade associated with the inspection;\n‚Ä¢ N = Not Yet Graded\n‚Ä¢ A = Grade A\n‚Ä¢ B = Grade B\n‚Ä¢ C = Grade C\n‚Ä¢ Z = Grade Pending\n‚Ä¢ P = Grade Pending issued on re-opening following an initial inspection that resulted in a closure"
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#q1a",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#q1a",
    "title": "Restaurant Inspection",
    "section": "Q1a",
    "text": "Q1a\nWhat are the mean, standard deviation, first quartile, median, third quartile, and maximum of SCORE for each GRADE of restaurants?\n\nrestaurant %&gt;% group_by(GRADE) %&gt;% skim(SCORE) %&gt;% select(-n_missing)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n17633\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nGRADE\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nGRADE\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSCORE\nA\n1\n9.26\n3.42\n0\n7\n10\n12\n13\n‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñá\n\n\nSCORE\nB\n1\n21.03\n4.16\n0\n18\n21\n24\n36\n‚ñÅ‚ñÅ‚ñá‚ñá‚ñÅ\n\n\nSCORE\nC\n1\n38.56\n10.83\n0\n31\n36\n44\n86\n‚ñÅ‚ñá‚ñá‚ñÇ‚ñÅ"
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#q1b",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#q1b",
    "title": "Restaurant Inspection",
    "section": "Q1b",
    "text": "Q1b\n\nHow many restaurants with a GRADE of A are there in NYC?\nHow much percentage of restaurants in NYC are a GRADE of C?\n\n\nfreq &lt;- as.data.frame( table(restaurant$GRADE) )\n\nprop &lt;- as.data.frame( 100 * prop.table(table(restaurant$GRADE)) )"
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#q1c",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#q1c",
    "title": "Restaurant Inspection",
    "section": "Q1c",
    "text": "Q1c\nProvide both (1) ggplot code and (2) a simple comment to describe how the distribution of SCORE varies by GRADE and CRITICAL FLAG.\n\nBoxplots\n\n\nggplot(restaurant) +\n  geom_boxplot(aes(x = SCORE, y = GRADE, fill = GRADE) ) +\n  facet_grid( CRITICAL.FLAG ~ . )\n\n\n\n\n\n\n\n\n\nHistograms\n\n\nggplot(restaurant) +\n  geom_histogram(aes(x = SCORE), binwidth = 1 ) +\n  facet_grid( CRITICAL.FLAG ~ GRADE )\n\n\n\n\n\n\n\n\n\nMostly,\n\nThe values of SCORE for GRADE A ranges from 0 to 13.\nThe values of SCORE for GRADE B ranges 13 to 27.\nThe values of SCORE for GRADE C ranges 24 to 75.\n\nFor Not Critical type, two SCORE values around 1 and 12 are most common, while 12 is the single most common SCORE value for Critical type."
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#q1d",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#q1d",
    "title": "Restaurant Inspection",
    "section": "Q1d",
    "text": "Q1d\nProvide both (1) ggplot code and (2) a simple comment to describe how the proportion of CRITICAL FLAG varies by GRADE and BORO.\n\nggplot(restaurant) +\n  geom_bar(aes(x = CRITICAL.FLAG,\n               y = after_stat(prop), group = 1)) +\n  facet_grid( GRADE ~ BORO )\n\n\n\n\n\n\n\n\n\nFor GRADE A, the probability distribution of CRITICAL FLAG are similar across BOROs.\nFor GRADE B, the restaurants in Staten Island are more likely to be Critical than in other BOROs.\nFor GRADE C, the restaurants in Bronx are more likely to be Critical than in other BOROs."
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#q1e",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#q1e",
    "title": "Restaurant Inspection",
    "section": "Q1e",
    "text": "Q1e\nFor the 10 most common CUISINE DESCRIPTION values, find the CUISINE DESCRIPTION value that has the highest proportion of GRADE A.\n\nq2e &lt;- restaurant %&gt;% \n  group_by(CUISINE.DESCRIPTION) %&gt;% \n  mutate(n = n()) %&gt;% \n  ungroup() %&gt;% \n  filter(dense_rank(-n) &lt;= 10) %&gt;% \n  group_by(CUISINE.DESCRIPTION, GRADE) %&gt;% \n  count() %&gt;% \n  group_by(CUISINE.DESCRIPTION) %&gt;% \n  mutate(prop_A = n / sum(n)) %&gt;% \n  filter(GRADE == 'A') %&gt;% \n  arrange(-prop_A)"
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#q1f",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#q1f",
    "title": "Restaurant Inspection",
    "section": "Q1f",
    "text": "Q1f\n\nFind the 3 most common names of restaurants (DBA) in each BORO.\n\nIf the third most common DBA values are multiple, please include all the DBA values.\n\nOverall, which DBA value is most common in NYC?\n\n\nq2f &lt;- restaurant %&gt;% \n  select(DBA, BORO) %&gt;% \n  group_by(BORO, DBA) %&gt;% \n  summarize(n = n()) %&gt;% \n  mutate(ranking = dense_rank(-n)) %&gt;% \n  filter(ranking &lt;= 3) %&gt;% \n  arrange(BORO, ranking)\n\nq2f_ &lt;- restaurant %&gt;% \n  group_by(DBA) %&gt;% \n  count() %&gt;% \n  arrange(-n)\n\n\nNote that chipotle mexican grill and subway are both the third most popular franchise/chain in Manhattan. üåØ\nOverall, dunkin is the most popular franchise/chain in NYC. üç©"
  },
  {
    "objectID": "posts/Restaurant Inspection/RestaurantInspection.html#q1g",
    "href": "posts/Restaurant Inspection/RestaurantInspection.html#q1g",
    "title": "Restaurant Inspection",
    "section": "Q1g",
    "text": "Q1g\nFor all the DBA values that appear in the result of Q1f, find the DBA value that is most likely to commit critical violation.\n\nq2g &lt;- restaurant %&gt;% \n  filter(DBA %in% q2f$DBA) %&gt;% \n  group_by(DBA, CRITICAL.FLAG) %&gt;% \n  count() %&gt;% \n  group_by(DBA) %&gt;% \n  mutate(lag_n = lag(n),\n         tot = sum(n),\n         prop_crit = lag_n / tot) %&gt;% \n  select(DBA, prop_crit) %&gt;% \n  filter(!is.na(prop_crit)) %&gt;% \n  arrange(-prop_crit)\n\n\nAmong popular franchises/chains, subway is most likely to commit Critical violation in NYC. ü•™"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#variables-are-names-not-places",
    "href": "posts/Python Basics/PythonBasics.html#variables-are-names-not-places",
    "title": "Python Basics",
    "section": "Variables Are Names, Not Places",
    "text": "Variables Are Names, Not Places\n\nA value is datum (literal) such as a number or text\nThere are different types of values:\n\n352.3 is known as a float or double;\n22 is an integer;\n‚ÄúHello World!‚Äù is a string.\n\n\n\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)\n\n[10, 1.23, 'like this', True, None]\n\n\nlist\n\n\n\nThe most. basic built-in data types that we‚Äôll need to know about are:\n\nintegers, 10\nfloats, 1.23\nstrings, ‚Äúlike this‚Äù\nbooleans, True\nnothing, None\n\nPython also has a built-in type of data container called a list (ex. [10,15,20]) that can contain anything, even different types"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#values-variables-and-types",
    "href": "posts/Python Basics/PythonBasics.html#values-variables-and-types",
    "title": "Python Basics",
    "section": "Values, Variables, and Types",
    "text": "Values, Variables, and Types\n\na = 10\nprint(a)\n\n10\n\n\n\nA variable is a name that refers to a value.\n\nWe can think of a variable as a box that has a value, or multiple values, packed inside it\n\nA variable is just a name!\nSometimes you will hear variables referred to as objects.\nEverything that is not a literal value, such as 10, is an object"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#assignment",
    "href": "posts/Python Basics/PythonBasics.html#assignment",
    "title": "Python Basics",
    "section": "Assignment (=)",
    "text": "Assignment (=)\n\n# Here we assign the integer value 5 to the variable x.\nx = 5   \n\n# Now we can use the variable x in the next line.\ny = x + 12  \ny\n\n17\n\n\n\nIn Python, we use = to assign a value to a variable\nIn math, = means equality of both sides\nIn programs, = means assignment: assign the value on the right side to the variable on the left side.\nIn programming code, everything on the right side needs to have a value.\n\nThe right side can be a literal value, or a variable that has already been assigned a value, or a combination.\n\nWhen Python reads y= x + 12, it does the following:\n\nSees the = in the middle.\nKnows that this is an assignment.\nCalculates the right side (gets the value of the object referred to by x and adds it to 12).\nAssigns the result to the left-side variable, y."
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#code-and-comment-style",
    "href": "posts/Python Basics/PythonBasics.html#code-and-comment-style",
    "title": "Python Basics",
    "section": "Code and Comment Style",
    "text": "Code and Comment Style\n\nThe two main principles for coding and managing data are:\n\nMake things easier for your future self.\nDon‚Äôt trust your future self.\n\nThe # mark is Google Colab‚Äôs comment character\n\nThe # character has many names: hash, sharp, pound, or octothorpe.\n# indicates that the rest of the line is to be ignored.\nWrite comments before the line that you want the comment to apply to.\n\nConsider adding more comments on code cells and their results using text cells."
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#brackets",
    "href": "posts/Python Basics/PythonBasics.html#brackets",
    "title": "Python Basics",
    "section": "Brackets",
    "text": "Brackets\n\nThere are several kinds of brackets in Python, including [], {}, and ().\n\n\nvector = ['a', 'b']\nvector[0]\n\n'a'\n\n\n\n[] is used to denote a list or to signify accessing a position using an index\n\n\n{'a', 'b'}  # set\n{'first_letter': 'a', 'second_letter': 'b'}  # dictionary\n\n{'first_letter': 'a', 'second_letter': 'b'}\n\n\n\n{} is used to denote a set or a dictionary (with key-value pairs)\n\n\nnum_tup = (1, 2, 3)\nsum(num_tup)\n\n6\n\n\n\n() is used to denote\n\na tuple, or\nthe arguments to a function, ex. function(x) where x is the input passed to the function"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#question-1",
    "href": "posts/Python Basics/PythonBasics.html#question-1",
    "title": "Python Basics",
    "section": "Question 1",
    "text": "Question 1\n\nUsing Python operations only, calculate below:\n\n\n(2**5/(7*(4-2**3)))\n\n-1.1428571428571428"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#question-2",
    "href": "posts/Python Basics/PythonBasics.html#question-2",
    "title": "Python Basics",
    "section": "Question 2",
    "text": "Question 2\n\nFor each expression below, what is the value of the expression? Explain thoroughly.\n\n\n20 == '20'\n\nFalse\n\n\n\nThis is saying 20 is not equal to ‚Äò20‚Äô because they are different data types (int vs string)\n\n\nx = 4.0\ny = .5\nz = 3*y - x\n\nx &lt; y or 3*y &lt; x\n\nTrue\n\n\n\nThis says the expression is true since 3*.5 &lt; 4"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#question-3",
    "href": "posts/Python Basics/PythonBasics.html#question-3",
    "title": "Python Basics",
    "section": "Question 3",
    "text": "Question 3\n\nfare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\n\n\nWrite a Python code that uses slicing and the print() function to print out the following message:\n\nThe total trip cost is: $12.80\n\n\n\ntotal =fare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\ntotal = fare[0:2] + tip[0] + tax[3:6]\nprint(\"The total trip cost is:\", total)\n\n#The total trip cost is: $12.80The total trip cost is:\", total)\n\nThe total trip cost is: $12.80"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#question-4",
    "href": "posts/Python Basics/PythonBasics.html#question-4",
    "title": "Python Basics",
    "section": "Question 4",
    "text": "Question 4\n\nlist_variable = [100, 144, 169, 1000, 8]\n\n\nWrite a Python code that uses print() and max() functions to print out the largest value in the list, list_variable, as follows:\n\nThe largest value in the list is: 1000\n\n\n\nlist_variable = [100,144,169,1000,8]\nx =max(list_variable)\nprint('The largest value in the list is:',x)\n\n#The largest value in the list is: 1000\n\nThe largest value in the list is: 1000"
  },
  {
    "objectID": "posts/Python Basics/PythonBasics.html#question-5",
    "href": "posts/Python Basics/PythonBasics.html#question-5",
    "title": "Python Basics",
    "section": "Question 5",
    "text": "Question 5\n\nImport the pandas library as pd.\nInstall the itables package.\nFrom itables, import the function init_notebook_mode and show.\n\n\nimport pandas as pd\n!pip install itables\nfrom itables import init_notebook_mode\nfrom itables import show\n\nRequirement already satisfied: itables in /Users/owenellick/anaconda3/lib/python3.11/site-packages (1.6.3)\nRequirement already satisfied: IPython in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from itables) (8.15.0)\nRequirement already satisfied: pandas in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from itables) (2.0.3)\nRequirement already satisfied: numpy in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from itables) (1.24.3)\nRequirement already satisfied: backcall in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.2.0)\nRequirement already satisfied: decorator in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.1.6)\nRequirement already satisfied: pickleshare in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,&lt;3.1.0,&gt;=3.0.30 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (3.0.36)\nRequirement already satisfied: pygments&gt;=2.4.0 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (2.15.1)\nRequirement already satisfied: stack-data in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.2.0)\nRequirement already satisfied: traitlets&gt;=5 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (5.7.1)\nRequirement already satisfied: pexpect&gt;4.3 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (4.8.0)\nRequirement already satisfied: appnope in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from IPython-&gt;itables) (0.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from pandas-&gt;itables) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from pandas-&gt;itables) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from pandas-&gt;itables) (2023.3)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;IPython-&gt;itables) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;IPython-&gt;itables) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,&lt;3.1.0,&gt;=3.0.30-&gt;IPython-&gt;itables) (0.2.5)\nRequirement already satisfied: six&gt;=1.5 in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;itables) (1.16.0)\nRequirement already satisfied: executing in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from stack-data-&gt;IPython-&gt;itables) (0.8.3)\nRequirement already satisfied: asttokens in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from stack-data-&gt;IPython-&gt;itables) (2.0.5)\nRequirement already satisfied: pure-eval in /Users/owenellick/anaconda3/lib/python3.11/site-packages (from stack-data-&gt;IPython-&gt;itables) (0.2.2)"
  },
  {
    "objectID": "posts/New York City Dogs/newyorkdogs.html#load-necessary-libraries",
    "href": "posts/New York City Dogs/newyorkdogs.html#load-necessary-libraries",
    "title": "NYC Dogs",
    "section": "Load Necessary Libraries",
    "text": "Load Necessary Libraries\n\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(ggthemes)\nlibrary(ggplot2)\nlibrary(viridis)"
  },
  {
    "objectID": "posts/New York City Dogs/newyorkdogs.html#loading-the-data",
    "href": "posts/New York City Dogs/newyorkdogs.html#loading-the-data",
    "title": "NYC Dogs",
    "section": "Loading The Data",
    "text": "Loading The Data\n\nnyc_dog_license &lt;- read_csv(\n  'https://bcdanl.github.io/data/nyc_dog_license.csv')\nprint(nyc_dog_license)\n\n# A tibble: 222,328 √ó 9\n   animal_name animal_gender animal_birth_year breed_rc         borough zip_code\n   &lt;chr&gt;       &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;      &lt;dbl&gt;\n 1 Paige       F                          2014 Pit Bull (or Mi‚Ä¶ Manhat‚Ä¶    10035\n 2 Lola        F                          2009 Maltese          Manhat‚Ä¶    10028\n 3 Chewbacca   F                          2012 Labrador (or Cr‚Ä¶ Manhat‚Ä¶    10013\n 4 Lemmy       F                          2005 Yorkshire Terri‚Ä¶ Manhat‚Ä¶    10003\n 5 Ricky       M                          2014 German Shepherd‚Ä¶ Brookl‚Ä¶    11220\n 6 Murphy      M                          2012 Pit Bull (or Mi‚Ä¶ Bronx      10463\n 7 Avery       F                          2014 Pit Bull (or Mi‚Ä¶ Manhat‚Ä¶    10002\n 8 Bigs        M                          2004 Pit Bull (or Mi‚Ä¶ Brookl‚Ä¶    11208\n 9 Bess        F                          2010 Beagle           Brookl‚Ä¶    11218\n10 Apple       M                          2013 Havanese         Manhat‚Ä¶    10025\n# ‚Ñπ 222,318 more rows\n# ‚Ñπ 3 more variables: license_issued_date &lt;date&gt;, license_expired_date &lt;date&gt;,\n#   extract_year &lt;dbl&gt;\n\n\n\nnyc_zips_coord &lt;- read_csv(\n  'https://bcdanl.github.io/data/nyc_zips_coord.csv')\nprint(nyc_zips_coord)\n\n# A tibble: 11,175 √ó 3\n       X     Y objectid\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 -73.9  40.7        1\n 2 -73.9  40.7        1\n 3 -73.9  40.7        1\n 4 -73.9  40.7        1\n 5 -73.9  40.7        1\n 6 -73.9  40.8        1\n 7 -73.9  40.8        1\n 8 -73.9  40.8        1\n 9 -73.9  40.8        1\n10 -73.9  40.8        1\n# ‚Ñπ 11,165 more rows\n\n\n\nnyc_zips_df &lt;- read_csv(\n  'https://bcdanl.github.io/data/nyc_zips_df.csv')\nprint(nyc_zips_df)\n\n# A tibble: 262 √ó 11\n   objectid zip_code po_name     state borough st_fips cty_fips bld_gpostal_code\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1        1    11372 Jackson He‚Ä¶ NY    Queens       36 081                     0\n 2        2    11004 Glen Oaks   NY    Queens       36 081                     0\n 3        3    11040 New Hyde P‚Ä¶ NY    Queens       36 081                     0\n 4        4    11426 Bellerose   NY    Queens       36 081                     0\n 5        5    11365 Fresh Mead‚Ä¶ NY    Queens       36 081                     0\n 6        6    11373 Elmhurst    NY    Queens       36 081                     0\n 7        7    11001 Floral Park NY    Queens       36 081                     0\n 8        8    11375 Forest Hil‚Ä¶ NY    Queens       36 081                     0\n 9        9    11427 Queens Vil‚Ä¶ NY    Queens       36 081                     0\n10       10    11374 Rego Park   NY    Queens       36 081                     0\n# ‚Ñπ 252 more rows\n# ‚Ñπ 3 more variables: shape_leng &lt;dbl&gt;, shape_area &lt;dbl&gt;, x_id &lt;chr&gt;"
  },
  {
    "objectID": "posts/New York City Dogs/newyorkdogs.html#important-visualization",
    "href": "posts/New York City Dogs/newyorkdogs.html#important-visualization",
    "title": "NYC Dogs",
    "section": "Important Visualization",
    "text": "Important Visualization\n\nnyc_dog_license_cleaned &lt;- nyc_dog_license %&gt;%\n  filter(!is.na(breed_rc))\n\npit_bull_proportion &lt;- nyc_dog_license_cleaned %&gt;%\n  group_by(zip_code) %&gt;%\n  summarise(\n    total_dogs = n(),\n    pitbull_dogs = sum(breed_rc == \"Pit Bull (or Mix)\"),\n    pitbull_proportion = (pitbull_dogs / total_dogs) * 100\n  )\n\nmerged_data &lt;- nyc_zips_df %&gt;%\n  left_join(pit_bull_proportion, by = \"zip_code\") %&gt;%\n  left_join(nyc_zips_coord, by = \"objectid\")\n\nx_percentile &lt;- quantile(merged_data$X, 0.075, na.rm = TRUE)\ny_percentile &lt;- quantile(merged_data$Y, 0.60, na.rm = TRUE)\n\nggplot(merged_data, aes(x = X, y = Y, fill = pitbull_proportion)) +\n  annotate(\"richtext\",\n           x = x_percentile,  \n           y = y_percentile,  \n           label = \"&lt;img src='https://bcdanl.github.io/lec_figs/pitbull.png' width='750'/&gt;\",\n           fill = NA,\n           color = NA,\n           ) +\n  geom_polygon(aes(group = zip_code), color = \"black\", size = 0.1) +\n  scale_fill_viridis(name = \"Percent of All Licensed Pit Bull\", option = \"magma\", direction = 1, breaks = c(2.5, 5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5), limits = c(0,22.5),\n                     na.value = \"transparent\") +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  labs(title = \"New York City's Pit Bull\", subtitle = \"By Zip Code. Based on Licensing Data\") +\n  theme_void() +\n  guides(fill = guide_legend(barwidth = 2.5, nrow = 1, label.position = \"bottom\")) +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 20),\n        plot.subtitle = element_text(hjust = 0.5), size = 14,\n        legend.position = \"top\",\n        legend.justification = c(0.2,1),\n        legend.key.width = unit(2, \"cm\"),\n        legend.title.position = \"top\")\n\n\n\n\n\n\n\n\n\nGeographic Distribution: The map highlights variations in Licensed Pit Bull ownership across different zip codes. Darker areas indicate a lower percentage of licensed Pit Bulls, while lighter areas suggest a higher percentage."
  },
  {
    "objectID": "posts/New York City Dogs/newyorkdogs.html#follow-up-question",
    "href": "posts/New York City Dogs/newyorkdogs.html#follow-up-question",
    "title": "NYC Dogs",
    "section": "Follow Up Question",
    "text": "Follow Up Question\n\nWhich Zip Code has the highest proportion of Pit Bull (or Mix)?\n\n\nmax_proportion &lt;- max(pit_bull_proportion$pitbull_proportion, na.rm = TRUE)\n\nhighest_proportion_zips &lt;- pit_bull_proportion %&gt;%\n  filter(pitbull_proportion == max_proportion)\n\nprint(highest_proportion_zips)\n\n# A tibble: 22 √ó 4\n   zip_code total_dogs pitbull_dogs pitbull_proportion\n      &lt;dbl&gt;      &lt;int&gt;        &lt;int&gt;              &lt;dbl&gt;\n 1     1009          2            2                100\n 2     7065          1            1                100\n 3     8527          1            1                100\n 4     8721          2            2                100\n 5    10520          2            2                100\n 6    10536          1            1                100\n 7    11381          2            2                100\n 8    12010          1            1                100\n 9    12775          2            2                100\n10    13032          1            1                100\n# ‚Ñπ 12 more rows\n\n\n\nThere are 22 zip codes that all are tied for the highest proportion of Pit Bull (or Mix), the zip code that is shown at the top is 1009."
  },
  {
    "objectID": "posts/Spotify Favorite Artists/favoriteartists.html#variable-description",
    "href": "posts/Spotify Favorite Artists/favoriteartists.html#variable-description",
    "title": "Favorite Artists",
    "section": "Variable Description üë®‚Äçüé§",
    "text": "Variable Description üë®‚Äçüé§\n\npid: playlist ID; unique ID for playlist\nplaylist_name: a name of playlist\npos: a position of the track within a playlist (starting from 0)\nartist_name: name of the track‚Äôs primary artist\ntrack_name: name of the track\nduration_ms: duration of the track in milliseconds\nalbum_name: name of the track‚Äôs album"
  },
  {
    "objectID": "posts/Spotify Favorite Artists/favoriteartists.html#occurances",
    "href": "posts/Spotify Favorite Artists/favoriteartists.html#occurances",
    "title": "Favorite Artists",
    "section": "Occurances üéôÔ∏è",
    "text": "Occurances üéôÔ∏è\n\nartist_count = spotify['artist_name'].value_counts()\nartist_count\n\nDrake                2715\nKanye West           1065\nKendrick Lamar       1035\nRihanna               915\nThe Weeknd            913\n                     ... \nLuna City Express       1\nNinetoes                1\nRhemi                   1\nJamie 3:26              1\nCaleb and Kelsey        1\nName: artist_name, Length: 18866, dtype: int64\n\n\n\nThe above code counts the occurences of each artist\n\nas you can see, Drake appears the most in playlists"
  },
  {
    "objectID": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-dataframe",
    "href": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-dataframe",
    "title": "Favorite Artists",
    "section": "Favorite Artists DataFrame üéπ",
    "text": "Favorite Artists DataFrame üéπ\n\nfavorite_artists = spotify[spotify['artist_name'].isin(['Drake', 'Rihanna','Halsey'])]\nfavorite_artists\n\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\nartist_name\ntrack_name\nduration_ms\nalbum_name\n\n\n\n\n320\n5\nWedding\n22\nRihanna\nWe Found Love\n215226\nTalk That Talk\n\n\n409\n7\n2017\n15\nHalsey\nEyes Closed\n202438\nhopeless fountain kingdom\n\n\n522\n10\nabby\n8\nDrake\nPortland\n236614\nMore Life\n\n\n544\n10\nabby\n30\nDrake\nPreach\n236973\nIf You're Reading This It's Too Late\n\n\n570\n10\nabby\n56\nDrake\nHeadlines\n235986\nTake Care\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n197565\n999990\nDrake\n1\nDrake\nFake Love\n210937\nMore Life\n\n\n197566\n999990\nDrake\n2\nDrake\nPop Style\n212946\nViews\n\n\n197567\n999990\nDrake\n3\nDrake\nHotline Bling\n267066\nViews\n\n\n197568\n999990\nDrake\n4\nDrake\nLegend\n241853\nIf You're Reading This It's Too Late\n\n\n197714\n999992\nGB\n100\nRihanna\nBitch Better Have My Money\n219305\nBitch Better Have My Money\n\n\n\n\n3939 rows √ó 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nThe above code filters the DataFrame to show only the songs by my three favorite artists: Drake, Rihanna, and Halsey"
  },
  {
    "objectID": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-occurances",
    "href": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-occurances",
    "title": "Favorite Artists",
    "section": "Favorite Artists Occurances üé∂",
    "text": "Favorite Artists Occurances üé∂\n\nfavorite_count = favorite_artists['artist_name'].value_counts()\nfavorite_count\n\nDrake      2715\nRihanna     915\nHalsey      309\nName: artist_name, dtype: int64\n\n\n\nThis code shows the number of occurrences that my three favorite artists have in the data"
  },
  {
    "objectID": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-track-duration",
    "href": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-track-duration",
    "title": "Favorite Artists",
    "section": "Favorite Artist‚Äôs Track Duration üíø",
    "text": "Favorite Artist‚Äôs Track Duration üíø\n\nsorted_fav_artists = favorite_artists.sort_values(by = 'duration_ms', ascending = False)\nno_duplicates = sorted_fav_artists.drop_duplicates(subset=['artist_name', 'track_name'])\nlongest_tracks = no_duplicates[['artist_name', 'track_name', 'duration_ms']].head(10)\nlongest_tracks\n\n\n\n  \n    \n\n\n\n\n\n\nartist_name\ntrack_name\nduration_ms\n\n\n\n\n47554\nDrake\nCameras / Good Ones Go Interlude - Medley\n434960\n\n\n157169\nDrake\nPound Cake / Paris Morton Music 2\n433800\n\n\n65122\nDrake\nShut It Down\n419306\n\n\n145989\nRihanna\nSame Ol‚Äô Mistakes\n397093\n\n\n110780\nRihanna\nWhere Have You Been - Hardwell Club Mix\n394653\n\n\n122260\nDrake\nUptown\n381240\n\n\n38545\nDrake\nSince Way Back\n368035\n\n\n110850\nDrake\nTuscan Leather\n366400\n\n\n30644\nRihanna\nCold Case Love\n364520\n\n\n29273\nDrake\nForever\n357706\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nThe above code sorts the DateFrame containing only tracks from my favorite artists by the length of their tracks\nIt then gets rid of any duplicates so I only see one of each song\nThen I see the top 10 longest tracks by my three favorite artists\nDrake has 7 out of the 10 longest songs out of my three favorite artists\nRihanna has the other 3 longest songs\nHalsey doesn‚Äôt have any of them which tells me she keeps her songs relatively short"
  },
  {
    "objectID": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-average-position",
    "href": "posts/Spotify Favorite Artists/favoriteartists.html#favorite-artists-average-position",
    "title": "Favorite Artists",
    "section": "Favorite Artist‚Äôs Average Position üßë‚Äçüé§",
    "text": "Favorite Artist‚Äôs Average Position üßë‚Äçüé§\n\nfav_artist_name = favorite_artists['artist_name'].unique()\nartist_tracks = spotify[spotify['artist_name'].isin(fav_artist_name)]\navg_pos = artist_tracks.groupby('artist_name')['pos'].mean()\navg_pos\n\nartist_name\nDrake      57.143278\nHalsey     65.582524\nRihanna    47.174863\nName: pos, dtype: float64\n\n\n\nThe above code gathers the average position of tracks within a playlist for each of my three favorite artists within the Spotify DataFrame\n\nI hope you learned a little bit about my favorite artists within the Spotify DataFrame"
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#variable-description",
    "href": "posts/NFL in 2022/NFLin2022.html#variable-description",
    "title": "NFL in 2022",
    "section": "Variable Description üèà",
    "text": "Variable Description üèà\n\nplay_id: Numeric play identifier that when used with game_id and drive provides the unique identifier for a single play\ngame_id: Ten digit identifier for NFL game.\ndrive: Numeric drive number in the game.\nweek: Season week.\nposteam: String abbreviation for the team with possession.\nqtr: Quarter of the game (5 is overtime).\nhalf_seconds_remaining: Numeric seconds remaining in the half.\ndown: The down for the given play.\n\nBasically you get four attempts (aka downs) to move the ball 10 yards (by either running with it or passing it).\nIf you make 10 yards then you get another set of four downs.\n\npass: Binary indicator if the play was a pass play.\nwp: Estimated winning probability for the posteam given the current situation at the start of the given play."
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#q2a",
    "href": "posts/NFL in 2022/NFLin2022.html#q2a",
    "title": "NFL in 2022",
    "section": "Q2a üèà",
    "text": "Q2a üèà\n\nlibrary(tidyverse)\nlibrary(skimr)\n\nIn data.frame, NFL2022_stuffs, remove observations for which values of posteam is missing.\n\nq2a &lt;- NFL2022_stuffs %&gt;% \n  filter(!is.na(posteam))"
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#q2b",
    "href": "posts/NFL in 2022/NFLin2022.html#q2b",
    "title": "NFL in 2022",
    "section": "Q2b üèà",
    "text": "Q2b üèà\n\nSummarize the mean value of pass for each posteam when all the following conditions hold:\n\n\nwp is greater than 20% and less than 75%;\ndown is less than or equal to 2; and\nhalf_seconds_remaining is greater than 120.\n\n\nq2b &lt;- NFL2022_stuffs %&gt;% \n  filter(between(wp, 0.2, 0.75),\n         down &lt;= 2,\n         half_seconds_remaining &gt; 120)\n\nmeanvalq2b &lt;- q2b %&gt;% \n  group_by(posteam) %&gt;% \n  summarise(mean_pass = mean(pass, na.rm = TRUE))\n\nmeanvalq2b\n\n# A tibble: 32 √ó 2\n   posteam mean_pass\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 ARI         0.553\n 2 ATL         0.4  \n 3 BAL         0.520\n 4 BUF         0.604\n 5 CAR         0.458\n 6 CHI         0.420\n 7 CIN         0.657\n 8 CLE         0.491\n 9 DAL         0.474\n10 DEN         0.493\n# ‚Ñπ 22 more rows"
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#q2c",
    "href": "posts/NFL in 2022/NFLin2022.html#q2c",
    "title": "NFL in 2022",
    "section": "Q2c üèà",
    "text": "Q2c üèà\n\nProvide both (1) a ggplot code with geom_point() using the resulting data.frame in Q2b and (2) a simple comments to describe the mean value of pass for each posteam.\n\nIn the ggplot, reorder the posteam categories based on the mean value of pass in ascending or in descending order.\n\n\n\nmeanvaldesc &lt;- meanvalq2b %&gt;% \n  arrange(desc(mean_pass))\n\nggplot(meanvaldesc, aes(x = mean_pass, y = reorder(posteam, mean_pass))) +\n  geom_point()+\n  labs(x= \"Percentage of Pass Plays\",\n       y= \"Team with possesion\")\n\n\n\n\n\n\n\n\nComments:\n\nCincinnati, Kansas City, Los Angeles Chargers, Buffalo Bills, and Philadelphia eagles had the top 5 highest average percentage of pass plays during the 2022 season.\nAtlanta, Washington, Chicago, New Orleans, and Tennessee had the top 5 lowest average percentage of pass plays during the 2022 season."
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#q2d",
    "href": "posts/NFL in 2022/NFLin2022.html#q2d",
    "title": "NFL in 2022",
    "section": "Q2d üèà",
    "text": "Q2d üèà\n\nConsider the following data.frame, NFL2022_epa:\n\n\nNFL2022_epa &lt;- read.csv('https://bcdanl.github.io/data/NFL2022_epa.csv')\n\n\nVariable description for NFL2022_epa\n\nplay_id: Numeric play identifier that when used with game_id and drive provides the unique identifier for a single play\ngame_id: Ten digit identifier for NFL game.\ndrive: Numeric drive number in the game.\nposteam: String abbreviation for the team with possession.\npasser: Name of the player who passed a ball to a receiver by initially taking a three-step drop and backpedaling into the pocket to make a pass. (Mostly, they are quarterbacks)\nreceiver: Name of the receiver.\nepa: Expected points added (EPA) by the posteam for the given play.\n\nCreate the data.frame, NFL2022_stuffs_EPA, that includes\n\nAll the variables in the data.frame, NFL2022_stuffs;\nThe variables, passer, receiver, and epa, from the data.frame, NFL2022_epa. by joining the two data.frames.\n\nIn the resulting data.frame, NFL2022_stuffs_EPA, remove observations with NA in passer.\n\nAnswer:\n\nNFL2022_stuffs_EPA &lt;- left_join(NFL2022_stuffs, NFL2022_epa, by = c(\"play_id\", \"game_id\", \"drive\", \"posteam\"))\n\nNFL2022_stuffs_EPA &lt;- NFL2022_stuffs_EPA %&gt;%\n  filter(!is.na(passer))"
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#q2e",
    "href": "posts/NFL in 2022/NFLin2022.html#q2e",
    "title": "NFL in 2022",
    "section": "Q2e üèà",
    "text": "Q2e üèà\n\nProvide both (1) a single ggplot and (2) a simple comment to describe the NFL weekly trend of weekly mean value of epa for each of the following two passers,\n\n‚ÄúJ.Allen‚Äù\n‚ÄúP.Mahomes‚Äù\n\n\n\ntwo_passers &lt;- c(\"J.Allen\", \"P.Mahomes\")\n\nfiltered_twopassers &lt;- NFL2022_stuffs_EPA %&gt;% \n  filter(passer %in% two_passers)\n\nmean_epa_data &lt;- filtered_twopassers %&gt;% group_by(week, passer) %&gt;% \n  summarise(mean_epa = mean(epa, na.rm = TRUE))\n\nggplot(mean_epa_data, aes(x= week, y= mean_epa, color = passer))+\n  geom_line()+\n  scale_color_manual(values =c(\"J.Allen\" =\"blue\", \"P.Mahomes\"=\"red\"))+\n  labs(x= \"Week\",\n       y= \"Mean value of expected points added (EPA)\")\n\n\n\n\n\n\n\n\nComment: Patrick Mahomes generally had a higher mean value of epa. However, there were a few weeks that Josh Allen had a higher mean value of epa."
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#q2f",
    "href": "posts/NFL in 2022/NFLin2022.html#q2f",
    "title": "NFL in 2022",
    "section": "Q2f üèà",
    "text": "Q2f üèà\nCalculate the difference between the mean value of epa for ‚ÄúJ.Allen‚Äù the mean value of epa for ‚ÄúP.Mahomes‚Äù for each value of week.\n\ndifference_epa &lt;- mean_epa_data %&gt;% \n  pivot_wider(names_from = passer, values_from = mean_epa)\n\ndifference_epa$epa_differnce &lt;- difference_epa$'J.Allen' - difference_epa$'P.Mahomes'\n\nprint(difference_epa)\n\n# A tibble: 22 √ó 4\n# Groups:   week [22]\n    week J.Allen P.Mahomes epa_differnce\n   &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1     1   0.530    0.698        -0.169 \n 2     2   0.487    0.148         0.339 \n 3     3   0.169    0.246        -0.0763\n 4     4   0.191    0.271        -0.0803\n 5     5   0.627    0.302         0.325 \n 6     6   0.307    0.133         0.173 \n 7     7  NA        0.701        NA     \n 8     8   0.224   NA            NA     \n 9     9  -0.208    0.0965       -0.304 \n10    10   0.161    0.589        -0.429 \n# ‚Ñπ 12 more rows"
  },
  {
    "objectID": "posts/NFL in 2022/NFLin2022.html#q2g",
    "href": "posts/NFL in 2022/NFLin2022.html#q2g",
    "title": "NFL in 2022",
    "section": "Q2g üèà",
    "text": "Q2g üèà\n\nSummarize the resulting data.frame in Q2d, with the following four variables:\n\nposteam: String abbreviation for the team with possession.\npasser: Name of the player who passed a ball to a receiver by initially taking a three-step drop, and backpedaling into the pocket to make a pass. (Mostly, they are quarterbacks.)\nmean_epa: Mean value of epa in 2022 for each passer\nn_pass: Number of observations for each passer\n\nThen find the top 10 NFL passers in 2022 in terms of the mean value of epa, conditioning that n_pass must be greater than or equal to the third quantile level of n_pass.\n\n\nsummary_data &lt;- NFL2022_stuffs_EPA %&gt;%\n  group_by(posteam, passer) %&gt;%\n  summarise(\n    mean_epa = mean(epa, na.rm = TRUE),\n    n_pass = n()\n  )\n\nquantile_threshold_passer &lt;- quantile(summary_data$n_pass, 0.75)\nfiltered_summary_data &lt;- summary_data %&gt;%\n  filter(n_pass &gt;= quantile_threshold_passer)\n\ntop_10_passers &lt;- filtered_summary_data %&gt;%\n  arrange(desc(mean_epa)) %&gt;% \n  head(n=10)\n  \ntop_10_passers\n\n# A tibble: 10 √ó 4\n# Groups:   posteam [10]\n   posteam passer       mean_epa n_pass\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;  &lt;int&gt;\n 1 KC      P.Mahomes      0.286     880\n 2 MIA     T.Tagovailoa   0.234     453\n 3 SF      J.Garoppolo    0.200     348\n 4 BUF     J.Allen        0.172     785\n 5 DET     J.Goff         0.171     661\n 6 CIN     J.Burrow       0.153     854\n 7 DAL     D.Prescott     0.147     529\n 8 PHI     J.Hurts        0.138     672\n 9 JAX     T.Lawrence     0.128     764\n10 CLE     J.Brissett     0.0912    445"
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#load-necessary-library",
    "href": "posts/Ice Cream/icecream.html#load-necessary-library",
    "title": "Ice Cream",
    "section": "Load Necessary Library",
    "text": "Load Necessary Library\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(scales)"
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#loading-the-data",
    "href": "posts/Ice Cream/icecream.html#loading-the-data",
    "title": "Ice Cream",
    "section": "Loading the Data",
    "text": "Loading the Data\n\nice_cream &lt;- read_csv('https://bcdanl.github.io/data/ben-and-jerry-cleaned.csv')\n\nprint(ice_cream)\n\n# A tibble: 21,974 √ó 17\n   priceper1 flavor_descr        size1_descr household_id household_income\n       &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;              &lt;dbl&gt;            &lt;dbl&gt;\n 1      3.41 CAKE BATTER         16.0 MLOZ        2001456           130000\n 2      3.5  VAN CARAMEL FUDGE   16.0 MLOZ        2001456           130000\n 3      3.5  VAN CARAMEL FUDGE   16.0 MLOZ        2001456           130000\n 4      3    W-N-C-P-C           16.0 MLOZ        2001637            70000\n 5      3.99 AMERICONE DREAM     16.0 MLOZ        2002791           130000\n 6      3.89 ONE CSK BROWNIE     16.0 MLOZ        2002791           130000\n 7      3.89 PUMPKIN CSK         16.0 MLOZ        2002791           130000\n 8      2.14 CHC ALMOND NOUGAT   16.0 MLOZ        2001878           110000\n 9      3.5  PISTACHIO PISTACHIO 16.0 MLOZ        2002721           210000\n10      3    PISTACHIO PISTACHIO 16.0 MLOZ        2002721           210000\n# ‚Ñπ 21,964 more rows\n# ‚Ñπ 12 more variables: household_size &lt;dbl&gt;, usecoup &lt;lgl&gt;, couponper1 &lt;dbl&gt;,\n#   region &lt;chr&gt;, married &lt;lgl&gt;, race &lt;chr&gt;, hispanic_origin &lt;lgl&gt;,\n#   microwave &lt;lgl&gt;, dishwasher &lt;lgl&gt;, sfh &lt;lgl&gt;, internet &lt;lgl&gt;, tvcable &lt;lgl&gt;"
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#summary-statistics-for-key-variables",
    "href": "posts/Ice Cream/icecream.html#summary-statistics-for-key-variables",
    "title": "Ice Cream",
    "section": "Summary Statistics For Key Variables",
    "text": "Summary Statistics For Key Variables\n\nsummary(ice_cream %&gt;% select(priceper1, household_income, household_size))\n\n   priceper1     household_income household_size \n Min.   :0.000   Min.   : 40000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.: 80000   1st Qu.:2.000  \n Median :3.340   Median :110000   Median :2.000  \n Mean   :3.315   Mean   :125291   Mean   :2.456  \n 3rd Qu.:3.590   3rd Qu.:170000   3rd Qu.:3.000  \n Max.   :9.480   Max.   :310000   Max.   :9.000"
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#price-analysis-across-flavors",
    "href": "posts/Ice Cream/icecream.html#price-analysis-across-flavors",
    "title": "Ice Cream",
    "section": "Price Analysis Across Flavors",
    "text": "Price Analysis Across Flavors\n\n# Calculate average price and count for each flavor\nflavor_analysis &lt;- ice_cream %&gt;%\n  group_by(flavor_descr) %&gt;%\n  summarize(\n    avg_price = mean(priceper1, na.rm = TRUE),\n    count = n(),\n    sd_price = sd(priceper1, na.rm = TRUE)\n  ) %&gt;%\n  filter(count &gt; 50) %&gt;%  # Focus on flavors with sufficient data\n  arrange(desc(count))\n\n# Display top flavors by purchase count\nhead(flavor_analysis, 10)\n\n# A tibble: 10 √ó 4\n   flavor_descr               avg_price count sd_price\n   &lt;chr&gt;                          &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1 CHERRY GRCA                     3.44  2097    0.833\n 2 CHC FUDGE BROWNIE               3.34  1235    0.672\n 3 CHC CHIP C-DH                   3.47  1070    0.796\n 4 HEATH COFFEE CRUNCH             3.32  1070    0.625\n 5 CHUNKY MONKEY                   3.26  1064    0.662\n 6 PHISH FOOD                      3.33   968    0.638\n 7 NEW YORK SUPER FUDGE CHUNK      3.29   932    0.718\n 8 AMERICONE DREAM                 3.31   865    0.690\n 9 PB CUP                          3.29   828    0.631\n10 KARAMEL SUTRA                   3.28   738    0.649"
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#plot-1-price-vs.-popularity-of-top-flavors",
    "href": "posts/Ice Cream/icecream.html#plot-1-price-vs.-popularity-of-top-flavors",
    "title": "Ice Cream",
    "section": "Plot 1: Price vs.¬†Popularity of Top Flavors",
    "text": "Plot 1: Price vs.¬†Popularity of Top Flavors\n\n# Create plot for top 15 flavors showing price vs popularity\nflavor_analysis %&gt;%\n  top_n(15, count) %&gt;%\n  ggplot(aes(x = reorder(flavor_descr, count), y = avg_price, fill = count)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = avg_price - sd_price, ymax = avg_price + sd_price), \n                width = 0.2, color = \"darkred\", alpha = 0.7) +\n  coord_flip() +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Average Price vs. Popularity of Ben & Jerry's Flavors\",\n    subtitle = \"Top 15 flavors by purchase frequency with price variation\",\n    x = NULL,\n    y = \"Average Price per Unit ($)\",\n    fill = \"Purchase\\nCount\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"darkgrey\"),\n    axis.text.y = element_text(size = 11),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  ) +\n  guides(fill = guide_colorbar(barwidth = 1, barheight = 10))\n\n\n\n\n\n\n\n\n\nThis visualization reveals both the popularity and pricing strategy of Ben & Jerry‚Äôs top flavors.\n\nCHERRY GRCA and CHC CHIP C-DH appear to be among the most purchased flavors, though there‚Äôs significant price variation as shown by the error bars."
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#regional-preferences-analysis",
    "href": "posts/Ice Cream/icecream.html#regional-preferences-analysis",
    "title": "Ice Cream",
    "section": "Regional Preferences Analysis",
    "text": "Regional Preferences Analysis\n\n# Calculate regional preferences\nregional_preferences &lt;- ice_cream %&gt;%\n  filter(!is.na(region)) %&gt;%\n  group_by(region, flavor_descr) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  group_by(region) %&gt;%\n  mutate(proportion = count / sum(count),\n         rank_in_region = rank(-count, ties.method = \"min\")) %&gt;%\n  filter(rank_in_region &lt;= 5) %&gt;%\n  arrange(region, rank_in_region)\n\n# Display regional top flavors\nregional_preferences %&gt;%\n  select(region, flavor_descr, count, proportion, rank_in_region) %&gt;%\n  mutate(proportion = scales::percent(proportion, accuracy = 0.1))\n\n# A tibble: 20 √ó 5\n# Groups:   region [4]\n   region  flavor_descr               count proportion rank_in_region\n   &lt;chr&gt;   &lt;chr&gt;                      &lt;int&gt; &lt;chr&gt;               &lt;int&gt;\n 1 Central CHERRY GRCA                  403 8.2%                    1\n 2 Central CHC FUDGE BROWNIE            297 6.1%                    2\n 3 Central CHUNKY MONKEY                261 5.3%                    3\n 4 Central CHC CHIP C-DH                249 5.1%                    4\n 5 Central AMERICONE DREAM              236 4.8%                    5\n 6 East    CHERRY GRCA                  405 8.9%                    1\n 7 East    PHISH FOOD                   267 5.9%                    2\n 8 East    HEATH COFFEE CRUNCH          241 5.3%                    3\n 9 East    CHC FUDGE BROWNIE            207 4.5%                    4\n10 East    CHC CHIP C-DH                201 4.4%                    5\n11 South   CHERRY GRCA                  693 10.3%                   1\n12 South   CHC FUDGE BROWNIE            496 7.4%                    2\n13 South   CHUNKY MONKEY                368 5.5%                    3\n14 South   NEW YORK SUPER FUDGE CHUNK   332 4.9%                    4\n15 South   HEATH COFFEE CRUNCH          318 4.7%                    5\n16 West    CHERRY GRCA                  596 10.3%                   1\n17 West    CHC CHIP C-DH                322 5.5%                    2\n18 West    HEATH COFFEE CRUNCH          315 5.4%                    3\n19 West    NEW YORK SUPER FUDGE CHUNK   255 4.4%                    4\n20 West    CHUNKY MONKEY                240 4.1%                    5"
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#plot2-regional-flavor-preferences",
    "href": "posts/Ice Cream/icecream.html#plot2-regional-flavor-preferences",
    "title": "Ice Cream",
    "section": "Plot2: Regional Flavor Preferences",
    "text": "Plot2: Regional Flavor Preferences\n\n# Select top 3 flavors per region for visualization\ntop_regional &lt;- ice_cream %&gt;%\n  filter(!is.na(region)) %&gt;%\n  count(region, flavor_descr) %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  top_n(3, prop) %&gt;%\n  ungroup()\n\n# Create regional preferences plot\nggplot(top_regional, aes(x = reorder(flavor_descr, prop), y = prop, fill = region)) +\n  geom_col() +\n  facet_wrap(~ region, scales = \"free_y\", ncol = 2) +\n  coord_flip() +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Regional Preferences for Ben & Jerry's Flavors\",\n    subtitle = \"Top 3 flavors by proportion of purchases in each region\",\n    x = NULL,\n    y = \"Proportion of Regional Purchases\",\n    fill = \"Region\"\n  ) +\n  theme_light() +\n  theme(\n    strip.background = element_rect(fill = \"skyblue\"),\n    strip.text = element_text(face = \"bold\"),\n    legend.position = \"bottom\",\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 10),\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"darkgrey\")\n  ) +\n  guides(fill = guide_legend(nrow = 1))\n\n\n\n\n\n\n\n\n\nThis visualization shows how flavor preferences vary by region.\n\nInterestingly, while some flavors like CHERRY GRCA have broad appeal across regions, others show distinct regional popularity patterns."
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#demographics-and-purchasing-behavior",
    "href": "posts/Ice Cream/icecream.html#demographics-and-purchasing-behavior",
    "title": "Ice Cream",
    "section": "Demographics and Purchasing Behavior",
    "text": "Demographics and Purchasing Behavior\n\n# Income group and average price analysis\nincome_price &lt;- ice_cream %&gt;%\n  filter(!is.na(household_income)) %&gt;%\n  mutate(income_group = case_when(\n    household_income &lt; 30000 ~ \"Low\",\n    household_income &lt; 70000 ~ \"Middle\",\n    TRUE ~ \"High\"\n  )) %&gt;%\n  group_by(income_group) %&gt;%\n  summarize(\n    avg_price = mean(priceper1, na.rm = TRUE),\n    median_price = median(priceper1, na.rm = TRUE),\n    count = n(),\n    coupon_usage_rate = mean(usecoup == TRUE, na.rm = TRUE)\n  )\n\n# Display income group summary\nincome_price %&gt;%\n  mutate(coupon_usage_rate = percent(coupon_usage_rate, accuracy = 0.1))\n\n# A tibble: 2 √ó 5\n  income_group avg_price median_price count coupon_usage_rate\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;            \n1 High              3.31         3.34 20211 10.5%            \n2 Middle            3.39         3.39  1763 12.5%"
  },
  {
    "objectID": "posts/Ice Cream/icecream.html#plot-3-demographic-factors-and-purchase-behavior",
    "href": "posts/Ice Cream/icecream.html#plot-3-demographic-factors-and-purchase-behavior",
    "title": "Ice Cream",
    "section": "Plot 3: Demographic Factors and Purchase Behavior",
    "text": "Plot 3: Demographic Factors and Purchase Behavior\n\n# Create demographic factors plot\nice_cream %&gt;%\n  filter(!is.na(household_income), !is.na(household_size), !is.na(priceper1)) %&gt;%\n  mutate(\n    income_group = case_when(\n      household_income &lt; 30000 ~ \"Low\",\n      household_income &lt; 70000 ~ \"Middle\",\n      TRUE ~ \"High\"\n    ),\n    household_size = as.factor(household_size),\n    coupon_used = ifelse(usecoup == TRUE, \"Used Coupon\", \"No Coupon\")\n  ) %&gt;%\n  ggplot(aes(x = income_group, y = priceper1, fill = household_size)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  facet_wrap(~ coupon_used) +\n  scale_y_continuous(\n    limits = c(0, 6),\n    labels = dollar_format(),\n    name = \"Price per Unit ($)\"\n  ) +\n  scale_fill_brewer(\n    palette = \"YlOrRd\",\n    name = \"Household\\nSize\"\n  ) +\n  labs(\n    title = \"Ice Cream Pricing by Income Group, Household Size, and Coupon Usage\",\n    subtitle = \"How demographics affect Ben & Jerry's purchasing patterns\",\n    x = \"Household Income Group\",\n    caption = \"Note: Outliers beyond $6 are not shown\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"darkgrey\"),\n    legend.position = \"right\",\n    strip.background = element_rect(fill = \"lightpink\"),\n    strip.text = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    axis.title = element_text(face = \"bold\")\n  ) +\n  guides(fill = guide_legend(override.aes = list(alpha = 0.9)))\n\n\n\n\n\n\n\n\n\nHigher income households tend to pay more for Ben & Jerry‚Äôs ice cream, regardless of coupon usage\nLarger households (4+ members) in the middle-income bracket show the widest price variation\nCoupon usage appears to effectively reduce prices across all income groups, but especially for middle-income households"
  },
  {
    "objectID": "posts/welcome/Welcome.html",
    "href": "posts/welcome/Welcome.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my Quarto blog. Welcome!\nI am Owen Ellick and I am currently a Sophomore Data Analtytics major with a Minor in Finance. I play on the Men‚Äôs Lacrosse team here at Geneseo. I enjoy playing sports specifically football,lacrosse, and golf. My plan for when I graduate from Geneseo is to travel Europe for a short period of time and then get a job doing data analysis in the sports industry. My favorite football team is the New York Jets and my favorite hockey team is the Washington Capitals. I enjoy spending time with my friends and family as they are a huge part of my life.\n\nPicture of my favorite football team! Being of fan of this team has its ups and downs. It allows you to be apart of a culture and fanbase that is bigger than yourself. However recent struggles make being a fan of this team rough, but I know in the end one day they will get it done.\n The photo above is me playing in a lacrosse game for SUNY Geneseo against Nazareth University on February 25,2023. It was really cold and snowy, but we managed to come out with the win 13-11.\nI hope you learned a little bit about me!"
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#load-necessary-library",
    "href": "posts/ggplot Basics/ggplotbasics.html#load-necessary-library",
    "title": "ggplot Basics",
    "section": "Load Necessary Library",
    "text": "Load Necessary Library\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#grammar-of-graphics",
    "href": "posts/ggplot Basics/ggplotbasics.html#grammar-of-graphics",
    "title": "ggplot Basics",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\n\nA grammar of graphics is a tool that enables us to concisely describe the components of a graphic."
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#aesthetic-mappings",
    "href": "posts/ggplot Basics/ggplotbasics.html#aesthetic-mappings",
    "title": "ggplot Basics",
    "section": "Aesthetic Mappings",
    "text": "Aesthetic Mappings\n\nAn aesthetic is a visual property (e.g., size, shape, color) of the objects (e.g., class) in your plot.\nYou can display a point in different ways by changing the values of its aesthetic properties.\n\n\nAdding a color to the plot üü•\n\nggplot(data = mpg) + \n  geom_point(mapping = \n               aes(x = displ, \n                   y = hwy, \n                   color = class) )\n\n\n\n\n\n\n\n\n\n\nAdding a shape to the plot ‚¨ú\n\nggplot(data = mpg) + \n  geom_point(mapping = \n               aes(x = displ, \n                   y = hwy, \n                   shape = class) )\n\n\n\n\n\n\n\n\n\n\nAdding a size to the plot\n\nggplot(data = mpg) + \n  geom_point(mapping = \n               aes(x = displ, \n                   y = hwy, \n                   size = class) )\n\n\n\n\n\n\n\n\n\n\nAdding an alpha (transparency) to the plot\n\nggplot(data = mpg) + \n  geom_point(mapping = \n               aes(x = displ, \n                   y = hwy, \n                   alpha = class) )\n\n\n\n\n\n\n\n\n\n\nSpecifying a color to the plot üü¶\n\nggplot(data = mpg) + \n  geom_point(mapping = \n               aes(x = displ, \n                   y = hwy), \n             color = \"blue\")"
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#facets",
    "href": "posts/ggplot Basics/ggplotbasics.html#facets",
    "title": "ggplot Basics",
    "section": "Facets",
    "text": "Facets\n\nOne way to add a variable, particularly useful for categorical variables, is to use facets to split our plot into facets, subplots that each display one subset of the data.\n\n\nFacet plots using facet_wrap()\n\nggplot(data = mpg) + \n  geom_point(mapping = \n               aes(x = displ, \n                   y = hwy), \n             alpha = .5) + \n  facet_wrap( . ~ class, nrow = 2)"
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#geometric-objects",
    "href": "posts/ggplot Basics/ggplotbasics.html#geometric-objects",
    "title": "ggplot Basics",
    "section": "Geometric Objects",
    "text": "Geometric Objects\n\nA geom_*() is the geometrical object that a plot uses to represent data.\n\nBar charts use geom_bar();\nLine charts use geom_line();\nBoxplots use the geom_boxplot();\nScatterplots use the geom_point();\nFitted lines use the geom_smooth(); and many more!\n\nWe can use different geom_*() to plot the same data.\n\n\nScatterplot\n\nggplot(data = mpg) + \n  geom_point(mapping = \n               aes(x = displ, \n                   y = hwy),\n             alpha = .3)\n\n\n\n\n\n\n\n\n\n\nFitted Lines\n\nggplot(data = mpg) + \n  geom_smooth(mapping = \n                aes(x = displ, \n                    y = hwy))"
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#statistical-transformation",
    "href": "posts/ggplot Basics/ggplotbasics.html#statistical-transformation",
    "title": "ggplot Basics",
    "section": "Statistical Transformation",
    "text": "Statistical Transformation\n\nMany graphs, including bar charts, calculate new values to plot:\n\ngeom_bar(), geom_histogram(), and geom_freqpoly() bin our data and then plot bin counts, the number of observations that fall in each bin.\ngeom_boxplot() computes a summary of the distribution and then display a specially formatted box.\ngeom_smooth() fits a model to our data and then plot predictions from the model.\n\n\n\nHistogram\n\nggplot(diamonds) +\n  geom_histogram(aes(x = price))\n\n\n\n\n\n\n\n\n\n\nFreqpoly üìà\n\nggplot(diamonds) +\n  geom_freqpoly(aes(x = price), \n                 bins = 200)\n\n\n\n\n\n\n\n\n\n\nBar Chart üìä\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut))"
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#position-adjustment",
    "href": "posts/ggplot Basics/ggplotbasics.html#position-adjustment",
    "title": "ggplot Basics",
    "section": "Position Adjustment",
    "text": "Position Adjustment\n\nColor and fill aesthetic\n\nWe can color a bar chart using either the color aesthetic, or, more usefully, fill.\n\n\nggplot(data = diamonds) + \n  geom_bar(mapping = \n             aes(x = cut, \n                 color = cut))\n\n\n\n\n\n\n\n\n\n\nFill\n\nggplot(data = diamonds) + \n  geom_bar(mapping = \n             aes(x = cut, \n                 fill = cut))"
  },
  {
    "objectID": "posts/ggplot Basics/ggplotbasics.html#examples-of-ggplot-in-classwork",
    "href": "posts/ggplot Basics/ggplotbasics.html#examples-of-ggplot-in-classwork",
    "title": "ggplot Basics",
    "section": "Examples of ggplot in classwork üíØ",
    "text": "Examples of ggplot in classwork üíØ\n\nQ1c\n\nRecreate the R code necessary to generate plots\n\n\nQ1c(a)\n\nggplot(data=mpg, mapping=aes(x=displ, y=hwy)) + \n  geom_point(shape=19, alpha=0.4) +\n  geom_smooth(se=FALSE)\n\n\n\n\n\n\n\n\n\n\nQ1c(b)\n\nggplot(mpg, aes(x=displ, y=hwy, group=drv)) +\n  geom_point(alpha=0.4, shape=19) +\n  geom_smooth(show.legend = FALSE, se=FALSE) \n\n\n\n\n\n\n\n\n\n\nQ1c(c)\n\nggplot(mpg, aes(x=displ, y=hwy, color=drv)) +\n  geom_point(alpha=0.4, shape=19) +\n  geom_smooth(se=FALSE) \n\n\n\n\n\n\n\n\n\n\nQ1c(d)\n\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(aes(color=drv), alpha=0.4, shape = 19) + \n  geom_smooth(se=FALSE)\n\n\n\n\n\n\n\n\n\n\nQ1c(e)\n\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(aes(color=drv), alpha=0.4, shape=19) +\n  geom_smooth(aes(linetype = drv), se=FALSE)\n\n\n\n\n\n\n\n\n\n\nQ1c(f)\n\nggplot(mpg, aes(x=displ, y=hwy, color=drv)) +\n  geom_point(size=6, alpha=0.6, shape=19, color=\"transparent\") +\n  geom_point(size=3, alpha=0.4, shape=19)\n\n\n\n\n\n\n\n\n\nCongratulations on learning the ggplot basics! ‚úÖ"
  },
  {
    "objectID": "seaborn_basics.html",
    "href": "seaborn_basics.html",
    "title": "Seaborn Example",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 56, 78]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.set(style=\"whitegrid\")  # Optional: Set a clean grid style\nplt.figure(figsize=(8, 6))  # Set the figure size\nsns.barplot(data=df, x='Category', y='Values', palette='viridis')\n\n# Customize the plot\nplt.title(\"Bar Plot Example\", fontsize=16)\nplt.xlabel(\"Category\", fontsize=12)\nplt.ylabel(\"Values\", fontsize=12)\n\n# Show the plot\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=df, x='Category', y='Values', palette='viridis')"
  },
  {
    "objectID": "Danl-310-team-project.html#introduction",
    "href": "Danl-310-team-project.html#introduction",
    "title": "Shooting for Success: Analyzing NBA Player Performance and Value üèÄ",
    "section": "Introduction",
    "text": "Introduction\n\nBackground üíª\nBasketball is a data-rich sport where player performance directly impacts team success. Understanding key statistics can help teams make better decisions about player value, game strategy, and resource allocation. This project dives into the 2022-23 NBA player dataset to uncover insights that could influence team building and player development.\n\n\nStatement of the Project Interest\nThe goal of this project is to analyze the relationship between player performance metrics (offensive and defensive) and their salaries. We aim to identify what stats correlate most with high salaries and whether there are undervalued players based on their contributions."
  },
  {
    "objectID": "Danl-310-team-project.html#data-storytelling",
    "href": "Danl-310-team-project.html#data-storytelling",
    "title": "Shooting for Success: Analyzing NBA Player Performance and Value üèÄ",
    "section": "Data Storytelling üìñ",
    "text": "Data Storytelling üìñ\n\nQuestions and Objectives\n\nWhat performance metrics are most strongly correlated with player salaries?\nAre there any undervalued players based on performance statistics?\nHow do offensive and defensive metrics compare in their influence on salary?\nWhat patterns emerge among players with high Player Efficiency Ratings (PER) and Win Shares (WS)?\nHow does a player‚Äôs. scoring efficiency (True Shooting Percentage) compare to their salary?\n\n\n\n\nData Transformation and Descriptive Statistics\nTo prepare the data and highlight key takeaways, we performed the following steps:\n\nData Cleaning\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the NBA dataset\nnba_players &lt;- read_csv(\"http://bcdanl.github.io/data/nba_players.csv\")\n\nRows: 467 Columns: 51\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (3): PlayerName, Position, Team\ndbl (48): Salary, Age, GP, GS, MP, FG, FGA, FG_pct, 3P, 3PA, 3P_pct, 2P, 2PA...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Data cleaning: Remove outliers and invalid entries\nnba_players_clean &lt;- nba_players %&gt;%\n  filter(!is.na(Salary) & Salary &gt; 0 & GP &gt; 10) %&gt;%  # Remove players with missing or zero salary, and low game participation\n  mutate(Salary_Millions = Salary / 1e6)  # Convert salary to millions for easier readability\n\nFindings: This cleaning ensures our analysis focuses on active players with meaningful contributions, avoiding noise from incomplete or irrelevant data points.\n\n\n\nKey Summary Statistics\nThe following code calculates averages for key metrics like salary, points per game (PTS), Player Efficiency Rating (PER), and Win Shares (WS).\n\n# Calculate key descriptive statistics\nsummary_stats &lt;- nba_players_clean %&gt;%\n  summarize(\n    avg_salary = mean(Salary_Millions),\n    median_salary = median(Salary_Millions),\n    avg_pts = mean(PTS),\n    avg_per = mean(PER),\n    avg_ws = mean(WS)\n  )\n\nprint(summary_stats)\n\n# A tibble: 1 √ó 5\n  avg_salary median_salary avg_pts avg_per avg_ws\n       &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1       9.55          4.63    9.96    13.7   2.69\n\n\nFindings: The calculated averages provide a baseline for comparing individual players. For example: - Average salary: $9.55 million - Average points per game: 9.96 points\nThese values help identify outliers and standout players.\n\n\n\nCorrelation Analysis\nThis code computes the correlation matrix to explore relationships between salary and performance metrics. Higher correlations indicate stronger relationships.\n\n# Compute correlations between salary and performance metrics\ncorrelation_matrix &lt;- nba_players_clean %&gt;%\n  select(Salary_Millions, PER, WS, TS_pct, PTS, AST, STL, BLK) %&gt;%\n  cor()\nprint(correlation_matrix)\n\n                Salary_Millions       PER        WS     TS_pct       PTS\nSalary_Millions       1.0000000 0.5456509 0.5970353 0.20168943 0.7292814\nPER                   0.5456509 1.0000000 0.7409330 0.63499002 0.7011657\nWS                    0.5970353 0.7409330 1.0000000 0.51919903 0.7221934\nTS_pct                0.2016894 0.6349900 0.5191990 1.00000000 0.2745762\nPTS                   0.7292814 0.7011657 0.7221934 0.27457624 1.0000000\nAST                   0.6161033 0.4725905 0.5426286 0.04748020 0.7325362\nSTL                   0.4874090 0.4118194 0.5429013 0.09145927 0.6069206\nBLK                   0.2743947 0.4826649 0.4754230 0.38408129 0.2574327\n                       AST        STL        BLK\nSalary_Millions 0.61610328 0.48740903 0.27439469\nPER             0.47259052 0.41181939 0.48266485\nWS              0.54262859 0.54290135 0.47542296\nTS_pct          0.04748020 0.09145927 0.38408129\nPTS             0.73253620 0.60692061 0.25743267\nAST             1.00000000 0.68220450 0.02631061\nSTL             0.68220450 1.00000000 0.15878444\nBLK             0.02631061 0.15878444 1.00000000\n\n\nFindings: Metrics like PTS, AST, PER, and WS are likely to show strong correlations with salary, suggesting they are key indicators of player value.\n\n\n\n\nData Visualization\n\nSalary Distribution\nThis histogram shows how player salaries are distributed across the league, emphasizing the disparities between top earners and the rest.\n\nlibrary(ggplot2)\n\n# Histogram of salary distribution\nsalary_plot &lt;- ggplot(nba_players_clean, aes(x = Salary_Millions)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Distribution of NBA Player Salaries (in Millions)\",\n       x = \"Salary (in Millions)\",\n       y = \"Number of Players\") +\n  scale_x_continuous(breaks = seq(0, max(nba_players_clean$Salary_Millions), by = 5)) +\n  theme_minimal()\n\nprint(salary_plot)\n\n\n\n\n\n\n\n\nFindings: Most players earn less than $10 million per season, with a few outliers earning significantly more.\n\n\n\n\nCorrelation Heatmap\nThis heatmap visualizes the strength of relationships between salary and various performance metrics.\n\nlibrary(ggcorrplot)\n\nheatmap_plot &lt;- ggcorrplot(correlation_matrix, \n                           lab = TRUE, \n                           lab_size = 5,\n                           colors = c(\"red\", \"white\", \"blue\"),\n                           title = \"Correlation Between Salary and Performance Metrics\")\n\nheatmap_plot + \n  theme_minimal(base_size = 14) +\n  theme(aspect.ratio = 1,\n        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\nFindings: Strong correlations between salary and metrics like PTS PER, WS, and AST suggest these are key drivers of player compensation.\n\n\n\nScatterplot of Salary vs.¬†PER\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nscatter_salary_per &lt;- ggplot(nba_players_clean, aes(x = PER, y = Salary_Millions, color = PER)) +\n  geom_point(alpha = 0.7, size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  scale_color_gradient(low = \"lightblue\", high = \"darkgreen\") +\n  labs(\n    title = \"Salary vs. Player Efficiency Rating (PER)\",\n    x = \"Player Efficiency Rating (PER)\",\n    y = \"Salary (in Millions)\",\n    color = \"PER\"\n  ) +\n  scale_y_continuous(labels = dollar_format(prefix = \"$\", suffix = \"M\")) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.title.x = element_text(face = \"bold\"),\n    axis.title.y = element_text(face = \"bold\")\n  )\n\nprint(scatter_salary_per)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFindings: Some players are shown with high PER have relatively low salaries, highlighting potential undervalued players. This means that they are highly efficient yet are paid lower than they deserve.\n\n\n\nTop 10 Players by Win Shares ‚õπÔ∏è‚Äç‚ôÇÔ∏è\nThis bar chart compares the Win Shares (WS) of the top 10 players with their corresponding salaries.\n\n# Identify top 10 players by Win Shares\ntop_players_ws &lt;- nba_players_clean %&gt;%\n  arrange(desc(WS)) %&gt;%\n  slice(1:10) %&gt;% \n  relocate(WS, .after = Salary)\nprint(top_players_ws)\n\n# A tibble: 10 √ó 52\n   PlayerName    Salary    WS Position   Age Team     GP    GS    MP    FG   FGA\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Domantas Sab‚Ä¶ 2.11e7  12.6 C           26 SAC      79    79  34.6   7.3  11.9\n 2 Jimmy Butler  3.77e7  12.3 PF          33 MIA      64    64  33.4   7.5  13.9\n 3 Joel Embiid   3.36e7  12.3 C           28 PHI      66    66  34.6  11    20.1\n 4 Shai Gilgeou‚Ä¶ 3.09e7  11.4 PG          24 OKC      68    68  35.5  10.4  20.3\n 5 Jayson Tatum  3.04e7  10.5 SF          24 BOS      74    74  36.9   9.8  21.1\n 6 Jarrett Allen 2   e7   9.5 C           24 CLE      68    68  32.6   5.9   9.2\n 7 Damian Lilla‚Ä¶ 4.25e7   9   PG          32 POR      58    58  36.3   9.6  20.7\n 8 Anthony Davis 3.80e7   9   C           29 LAL      56    54  34     9.7  17.2\n 9 Donovan Mitc‚Ä¶ 3.09e7   8.9 SG          26 CLE      68    68  35.8  10    20.6\n10 Jalen Brunson 2.77e7   8.7 PG          26 NYK      68    68  35     8.6  17.6\n# ‚Ñπ 41 more variables: FG_pct &lt;dbl&gt;, `3P` &lt;dbl&gt;, `3PA` &lt;dbl&gt;, `3P_pct` &lt;dbl&gt;,\n#   `2P` &lt;dbl&gt;, `2PA` &lt;dbl&gt;, `2P_pct` &lt;dbl&gt;, eFG_pct &lt;dbl&gt;, FT &lt;dbl&gt;,\n#   FTA &lt;dbl&gt;, FT_pct &lt;dbl&gt;, ORB &lt;dbl&gt;, DRB &lt;dbl&gt;, TRB &lt;dbl&gt;, AST &lt;dbl&gt;,\n#   STL &lt;dbl&gt;, BLK &lt;dbl&gt;, TOV &lt;dbl&gt;, PF &lt;dbl&gt;, PTS &lt;dbl&gt;, TotalMinutes &lt;dbl&gt;,\n#   PER &lt;dbl&gt;, TS_pct &lt;dbl&gt;, `3PAr` &lt;dbl&gt;, FTr &lt;dbl&gt;, ORB_pct &lt;dbl&gt;,\n#   DRB_pct &lt;dbl&gt;, TRB_pct &lt;dbl&gt;, AST_pct &lt;dbl&gt;, STL_pct &lt;dbl&gt;, BLK_pct &lt;dbl&gt;,\n#   TOV_pct &lt;dbl&gt;, USG_pct &lt;dbl&gt;, OWS &lt;dbl&gt;, DWS &lt;dbl&gt;, WS_per_48 &lt;dbl&gt;, ‚Ä¶\n\n\n\nlibrary(ggplot2)\nlibrary(viridis)  # for prettier color gradients\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nlibrary(ggthemes) # optional, for additional theme options\n\n# Enhanced bar chart\nbar_top_ws &lt;- ggplot(top_players_ws, aes(x = reorder(PlayerName, WS), y = WS, fill = Salary_Millions)) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 0.7) +\n  coord_flip() +\n  geom_text(aes(label = round(WS, 1)), hjust = -0.2, size = 4, fontface = \"bold\") +  # Add data labels\n  scale_fill_viridis(option = \"plasma\", direction = -1, name = \"Salary ($M)\", guide = guide_colorbar(barwidth = 10, barheight = 8)) +\n  labs(\n    title = \"Top 10 NBA Players by Win Shares\",\n    subtitle = \"Colored by Salary (in Millions)\",\n    x = \"Player Name\",\n    y = \"Win Shares\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(face = \"italic\", size = 12),\n    axis.title.x = element_text(face = \"bold\", size = 13),\n    axis.title.y = element_text(face = \"bold\", size = 13),\n    axis.text = element_text(size = 11),\n    legend.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  ) +\n  ylim(0, max(top_players_ws$WS) + 2)  # Add space for text labels\n\nprint(bar_top_ws)\n\n\n\n\n\n\n\n\nFindings: The highest-performing players (by WS) tend to have high salaries, although there are a few exceptions. For example, Domantas Sabonis is the number one NBA player by win share, but his salary is the second lowest among top 10 players by win share. Jarret Allen is another example he is 6th in the NBA in win share yet he is being paid the lowest by any of the top 10 players by win share. This tells us that Sabonis and Allen are extremely undervalued and contribute to their teams winning percentage at a lower price compared to others in the NBA.\n\n\n\nOffensive vs.¬†Defensive Metrics and Salary üìà\nBelow is a chart showing the Offensive and Defensive Metrics for the top 10 highest paid NBA players. The goal is to see which performance metrics (offensive or defensive) contribute to a players salary more.\n\nlibrary(tidyverse)\n\n# Prepare data\nnba_off_def &lt;- nba_players_clean %&gt;%\n  slice_max(Salary_Millions, n = 10) %&gt;%   # Top 10 highest-paid players\n  select(PlayerName, PTS, AST, STL, BLK) %&gt;%\n  pivot_longer(cols = c(PTS, AST, STL, BLK),\n               names_to = \"Metric\",\n               values_to = \"Value\")\n\n# Create grouped bar chart\ngrouped_bar_chart &lt;- ggplot(nba_off_def, aes(x = reorder(PlayerName, -Value), y = Value, fill = Metric)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.7, color = \"black\", alpha = 0.9) +\n  labs(title = \"Top 10 Highest-Paid Players: *Offensive vs. Defensive Metrics*\",\n       subtitle = \"Comparing Points, Assists, Steals, and Blocks for Each Player\",\n       x = \"Player Name\",\n       y = \"Metric Value\",\n       fill = \"Metric\") +\n  scale_fill_manual(values = c(\"PTS\" = \"#1f77b4\", \"AST\" = \"#2ca02c\", \n                               \"STL\" = \"#ff7f0e\", \"BLK\" = \"#9467bd\")) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold.italic\", size = 15, hjust = 0.5),\n    plot.subtitle = element_text(face = \"italic\", size = 14, hjust = 0.5, margin = margin(b = 10)),\n    axis.title.x = element_text(face = \"bold\", size = 13, margin = margin(t = 10)),\n    axis.title.y = element_text(face = \"bold\", size = 13, margin = margin(r = 10)),\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"),\n    axis.text.y = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\"),\n    legend.position = \"right\",\n    panel.grid.major.y = element_line(color = \"grey80\"),\n    panel.grid.minor = element_blank()\n  ) +\n  guides(fill = guide_legend(title.position = \"top\", title.hjust = 0.5))\n\nprint(grouped_bar_chart)\n\n\n\n\n\n\n\n\nFindings 1. Offesive Metrics Dominate (PTS and AST)\n\nPoints (PTS): The PTS (light blue bars) are significantly higher for all players compared to other metrics. This suggests that scoring ability is a major factor contributing to high salaries.\n\nFor example, Giannis Antetokounmpo, Stephen Curry, and Damian Lillard have extremely high PTS values, indicating their scoring dominance.\nPlayers like Kevin Durant and Lebron James also exhibit high PTS, reinforcing the importance of offensive capabilities.\n\nAssists (AST): The AST (green bars) are the second-highest metric for most players, especially for those known as playmakers, such as LeBron James, Russell Westbrook, and Stephen Curry. This indicates that playmaking ability is another key factor in high salaries.\n\n\nDefensive Metrics Are Less Prominent (BLK and STL)\n\n\nBlocks (BLK): The BLK (purple bars) are relatively low for all players, even for those known for their defensive prowess, like Kawhi Leonard and Giannis Antetokounmpo. This suggests that blocking is not a significant determinant of salary for these top players.\nSteals (STL): The STL (orange bars) are also consistently low across all players, indicating that steals, like blocks, have less influence on salary when compared to offensive metrics.\n\n\nBalanced Players Are More Valued\n\n\nSome players demonstrate a balance between offensive (PTS and AST) and defensive (STL and BLK) metrics. For example:\n\nGiannis Antetokounmpo and LeBron James show relatively higher values in both offensive and defensive categories, making them versatile players.\nKawhi Leonard shows moderate contributions in all metrics, aligning with his reputation as a two-way player.\n\n\n\nOutliers in Specific Metrics\n\n\nStephen Curry: Has the highest PTS value but relatively lower defensive metrics, emphasizing his role as a scoring specialist.\nRussell Westbrook: Shows higher AST values compared to most other players, highlighting his playmaking ability.\n\nKey Conclusions:\n\nOffensive contributions (PTS and AST) have a stronger influence on salary than defensive contributions (BLK and STL).\n\nPlayers who excel in scoring (e.g., Curry, Durant, James) or playmaking (e.g., Westbrook, James) are more highly compensated.\n\nDefensive metrics, while still important are less impactful on salary for these top players, due to the heightened difficulty to achieve higher defensive statistics although balanced players like Giannis and LeBron are exceptions.\n\nThis analysis ties the metrics back to salary by showing that offensive capabilities are the primary drivers of high compensation. - In the NBA, while being a great defensive player is important, todays fast paced, high scoring game favors players who can score a lot of points, hence the top 10 paid players all having high offensive statistics.\n\n\n\nInteractive Scatterplot of Salary vs.¬†True Shooting Percentage (TS%) üìä\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Set a minimum threshold for total minutes played\nmin_minutes &lt;- 1000\n\n# Filter dataset to include players with at least 'min_minutes'\nfiltered_nba_data &lt;- nba_players_clean %&gt;%\n  filter(TotalMinutes &gt;= min_minutes)  # Replace 'TotalMinutes' with your actual column name\n\n# Scatterplot: Salary vs. True Shooting Percentage (TS%)\nscatter_salary_ts &lt;- ggplot(filtered_nba_data, aes(\n  x = TS_pct, y = Salary_Millions, \n  text = paste(\n    \"&lt;b&gt;Player:&lt;/b&gt;\", PlayerName, \n    \"&lt;br&gt;&lt;b&gt;TS%:&lt;/b&gt;\", scales::percent(TS_pct, accuracy = 0.1),\n    \"&lt;br&gt;&lt;b&gt;Salary:&lt;/b&gt;\", scales::dollar(Salary_Millions, prefix = \"$\", suffix = \"M\"),\n    \"&lt;br&gt;&lt;b&gt;Total Minutes Played:&lt;/b&gt;\", TotalMinutes\n  )\n)) +\n  geom_point(color = \"#1f78b4\", alpha = 0.8, size = 2) +  # Blue points with transparency\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#ff7f00\", size = 1.2) +  # Orange trend line\n  labs(\n    title = \"Interactive Scatterplot: Salary vs. True Shooting Percentage (TS%)\",\n    subtitle = paste(\"Players with at least\", min_minutes, \"Total Minutes Played\"),\n    x = \"True Shooting Percentage (TS%)\",\n    y = \"Salary (in Millions)\"\n  ) +\n  scale_x_continuous(\n    labels = scales::percent_format(), \n    limits = c(0.45, 0.75),\n    breaks = seq(0.45, 0.75, by = .05), # Assuming TS% ranges between 40% and 70%\n    expand = c(0, 0)\n  ) +\n  scale_y_continuous(\n    labels = scales::dollar_format(prefix = \"$\", suffix = \"M\"), \n    limits = c(0, 50),  # Assuming salary ranges up to $50M\n    expand = c(0, 0)\n  ) +\n  theme_minimal(base_size = 14, base_family = \"Arial\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(face = \"italic\", size = 12, hjust = 0.5, color = \"gray40\"),\n    axis.title.x = element_text(face = \"italic\", size = 12),\n    axis.title.y = element_text(face = \"italic\", size = 12),\n    axis.text = element_text(size = 10),\n    panel.grid.major = element_line(color = \"gray80\", size = 0.5),\n    panel.grid.minor = element_blank(),\n    legend.position = \"none\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n\n# Convert the ggplot object to an interactive plot\ninteractive_scatter_salary_ts &lt;- ggplotly(\n  scatter_salary_ts, \n  tooltip = \"text\"  # Display the custom tooltip text\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n# Save the interactive plot as an HTML file\nhtmlwidgets::saveWidget(\n  interactive_scatter_salary_ts, \n  \"interactive_scatter_salary_ts_filtered_minutes.html\", \n  selfcontained = TRUE\n)\n\n# Print the interactive plot to view in RStudio\ninteractive_scatter_salary_ts\n\n\n\n\n\nAbove is an interactive plot showing the relationship between True Shooting Percentage and Player Salaries."
  },
  {
    "objectID": "Danl-310-team-project.html#significance-of-the-project",
    "href": "Danl-310-team-project.html#significance-of-the-project",
    "title": "Shooting for Success: Analyzing NBA Player Performance and Value üèÄ",
    "section": "Significance of the Project üîä",
    "text": "Significance of the Project üîä\n\nWhy These Findings Matter\nThe insights uncovered in this project go beyond basketball and can be applied to several real-world scenarios:\n\nInfluencing Business Decisions in Sports:\n\nSalary Optimization: Teams can use the findings to identify undervalued players whose contributions (e.g., high PER or WS) exceed their current salaries. This can guide better salary negotiations and resource allocation.\nPlayer Evaluation: By understanding which metrics (e.g., offensive contributions like PTS and AST) strongly influence salaries, teams can refine their scouting and recruitment strategies to focus on high-impact players.\nData-Driven Contracts: Teams can use this analysis to structure contracts that better reflect a player‚Äôs overall value, balancing offensive and defensive contributions.\n\nBroader Applications Beyond Sports:\n\nWorkforce Compensation Analysis: Just as we analyzed player salaries, organizations in other industries can use data to ensure fair and performance-based compensation for employees.\nInfluencing Public Policy: The methodology of linking measurable performance metrics to pay could also be applied in sectors like education, healthcare, and public services to assess resource allocation and equity.\n\nEnhancing Fan Engagement:\n\nFans love debating whether players are underpaid or overpaid. This analysis provides a data-backed framework for fans, sports journalists, and commentators to discuss player value and team-building strategies.\n\n\n\n\nConnecting to Broader Themes\n\nData-Driven Decision-Making: This project underscores how data can lead to more informed, fair, and efficient decisions in any industry.\nValue Assessment: By identifying inefficiencies and patterns in compensation, we can promote better allocation of resources, whether in sports or other fields."
  },
  {
    "objectID": "Danl-310-team-project.html#references",
    "href": "Danl-310-team-project.html#references",
    "title": "Shooting for Success: Analyzing NBA Player Performance and Value üèÄ",
    "section": "References",
    "text": "References\n\nData Sources\n\nNBA Dataset: Source\n\nContains player performance metrics and salary data for the 2022-23 NBA season.\n\n\n\n\nAI Assistance\nThis project benefited from the use of GitHub Copilot, a code-generation and AI assistant tool. GitHub Copilot was employed to: - Refine code aesthetics, such as improving themes, scales, and formatting for better presentation. - Provide insights and guidance on implementing project-specific requirements (e.g., filtering datasets, embedding interactive visualizations). - Answer technical questions and suggest best practices for reproducible and professional code. - Help to refine errors and troubleshoot incorrect code.\nGitHub Copilot facilitated faster implementation and enhanced the overall quality of the project by offering suggestions and reducing development time.\nNote: All outputs from GitHub Copilot were reviewed and validated by the project team to ensure accuracy and relevance to the project goals."
  },
  {
    "objectID": "pandas_basics.html#creating-a-series",
    "href": "pandas_basics.html#creating-a-series",
    "title": "Pandas Basics",
    "section": "Creating a Series",
    "text": "Creating a Series\n\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40, 50]\nseries = pd.Series(data)\nseries\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30\n\n\n3\n40\n\n\n4\n50\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "pandas_basics.html#creating-a-dataframe",
    "href": "pandas_basics.html#creating-a-dataframe",
    "title": "Pandas Basics",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\n\n# Creating a DataFrame from a dictionary\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\ndf = pd.DataFrame(data)\ndf\n\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAlice\n25\nNew York\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#exploring-data",
    "href": "pandas_basics.html#exploring-data",
    "title": "Pandas Basics",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n\n# Display the first few rows\ndf.head()\n\n# Display the shape of the DataFrame\nprint(\"Shape:\", df.shape)\n\n# Display summary statistics\ndf.describe()\n\nShape: (3, 3)\n\n\n\n\n  \n    \n\n\n\n\n\n\nAge\n\n\n\n\ncount\n3.0\n\n\nmean\n30.0\n\n\nstd\n5.0\n\n\nmin\n25.0\n\n\n25%\n27.5\n\n\n50%\n30.0\n\n\n75%\n32.5\n\n\nmax\n35.0"
  },
  {
    "objectID": "pandas_basics.html#selecting-data",
    "href": "pandas_basics.html#selecting-data",
    "title": "Pandas Basics",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n# Selecting a single column\ndf[\"Name\"]\n\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nAlice\n\n\n1\nBob\n\n\n2\nCharlie\n\n\n\n\ndtype: object\n\n\n\n\n# Selecting multiple columns\ndf[[\"Name\", \"City\"]]\n\n\n\n  \n    \n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAlice\nNew York\n\n\n1\nBob\nLos Angeles\n\n\n2\nCharlie\nChicago\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n# Selecting rows by index\ndf.iloc[0]\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\nName\nAlice\n\n\nAge\n25\n\n\nCity\nNew York\n\n\n\n\ndtype: object"
  },
  {
    "objectID": "pandas_basics.html#filtering-data",
    "href": "pandas_basics.html#filtering-data",
    "title": "Pandas Basics",
    "section": "Filtering Data",
    "text": "Filtering Data\n\n# Filtering rows where Age is greater than 25\nfiltered_df = df[df[\"Age\"] &gt; 25]\nfiltered_df\n\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#adding-a-new-column",
    "href": "pandas_basics.html#adding-a-new-column",
    "title": "Pandas Basics",
    "section": "Adding a New Column",
    "text": "Adding a New Column\n\n\n# Adding a new column\ndf[\"Salary\"] = [50000, 60000, 70000]\ndf\n\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\nSalary\n\n\n\n\n0\nAlice\n25\nNew York\n50000\n\n\n1\nBob\n30\nLos Angeles\n60000\n\n\n2\nCharlie\n35\nChicago\n70000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n    ## Conclusion\n\n    This notebook covers the basic operations of pandas. You can explore more advanced features like merging,\n    joining, and working with time series data in pandas documentation: https://pandas.pydata.org/docs/"
  },
  {
    "objectID": "Icecream.html",
    "href": "Icecream.html",
    "title": "Ice Cream Blog",
    "section": "",
    "text": "Scooping Into Ben & Jerry‚Äôs: An Analysis of Ice Cream Data\n\n\n\nIcecream2.png\n\n\n\n\nIntroduction\nHello fellow ice cream enthusiasts! Today, we‚Äôre diving into a dataset of Ben & Jerry‚Äôs ice cream purchases to explore pricing, flavors, and consumer demographics. We‚Äôll use Python and some data analysis techniques to uncover interesting insights.\n\n\nLoading The Data\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nice_cream = pd.read_csv('https://bcdanl.github.io/data/ben-and-jerry-cleaned.csv')\n\n\nprint(ice_cream.head())\n\n   priceper1       flavor_descr size1_descr  household_id  household_income  \\\n0       3.41        CAKE BATTER   16.0 MLOZ       2001456            130000   \n1       3.50  VAN CARAMEL FUDGE   16.0 MLOZ       2001456            130000   \n2       3.50  VAN CARAMEL FUDGE   16.0 MLOZ       2001456            130000   \n3       3.00          W-N-C-P-C   16.0 MLOZ       2001637             70000   \n4       3.99    AMERICONE DREAM   16.0 MLOZ       2002791            130000   \n\n   household_size  usecoup  couponper1   region  married   race  \\\n0               2     True         0.5  Central    False  white   \n1               2    False         0.0  Central    False  white   \n2               2    False         0.0  Central    False  white   \n3               1    False         0.0     West    False  white   \n4               3    False         0.0    South     True  white   \n\n   hispanic_origin  microwave  dishwasher    sfh  internet tvcable  \n0            False       True       False  False      True    True  \n1            False       True       False  False      True    True  \n2            False       True       False  False      True    True  \n3            False       True        True   True     False    True  \n4            False       True        True   True      True    True  \n\n\n\n\nDescriptive Statistics:\n\n\nprint(ice_cream.info())\nprint(ice_cream.describe(include='all'))\nprint(ice_cream['priceper1'].mean())\nprint(ice_cream['flavor_descr'].nunique())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 21974 entries, 0 to 21973\nData columns (total 17 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   priceper1         21974 non-null  float64\n 1   flavor_descr      21974 non-null  object \n 2   size1_descr       21974 non-null  object \n 3   household_id      21974 non-null  int64  \n 4   household_income  21974 non-null  int64  \n 5   household_size    21974 non-null  int64  \n 6   usecoup           21974 non-null  bool   \n 7   couponper1        21974 non-null  float64\n 8   region            21974 non-null  object \n 9   married           21974 non-null  bool   \n 10  race              21974 non-null  object \n 11  hispanic_origin   21974 non-null  bool   \n 12  microwave         21974 non-null  bool   \n 13  dishwasher        21974 non-null  bool   \n 14  sfh               21974 non-null  bool   \n 15  internet          21974 non-null  bool   \n 16  tvcable           21940 non-null  object \ndtypes: bool(7), float64(2), int64(3), object(5)\nmemory usage: 1.8+ MB\nNone\n           priceper1 flavor_descr size1_descr  household_id  household_income  \\\ncount   21974.000000        21974       21974  2.197400e+04      21974.000000   \nunique           NaN           50           2           NaN               NaN   \ntop              NaN  CHERRY GRCA   16.0 MLOZ           NaN               NaN   \nfreq             NaN         2097       21605           NaN               NaN   \nmean        3.314627          NaN         NaN  1.661201e+07     125290.798216   \nstd         0.665626          NaN         NaN  1.168595e+07      57188.363223   \nmin         0.000000          NaN         NaN  2.000358e+06      40000.000000   \n25%         3.000000          NaN         NaN  8.142253e+06      80000.000000   \n50%         3.340000          NaN         NaN  8.401573e+06     110000.000000   \n75%         3.590000          NaN         NaN  3.018389e+07     170000.000000   \nmax         9.480000          NaN         NaN  3.044069e+07     310000.000000   \n\n        household_size usecoup    couponper1 region married   race  \\\ncount     21974.000000   21974  21974.000000  21974   21974  21974   \nunique             NaN       2           NaN      4       2      4   \ntop                NaN   False           NaN  South    True  white   \nfreq               NaN   19629           NaN   6713   13276  18968   \nmean          2.456403     NaN      0.125579    NaN     NaN    NaN   \nstd           1.336821     NaN      0.517889    NaN     NaN    NaN   \nmin           1.000000     NaN      0.000000    NaN     NaN    NaN   \n25%           2.000000     NaN      0.000000    NaN     NaN    NaN   \n50%           2.000000     NaN      0.000000    NaN     NaN    NaN   \n75%           3.000000     NaN      0.000000    NaN     NaN    NaN   \nmax           9.000000     NaN      8.980000    NaN     NaN    NaN   \n\n       hispanic_origin microwave dishwasher    sfh internet tvcable  \ncount            21974     21974      21974  21974    21974   21940  \nunique               2         2          2      2        2       2  \ntop              False      True       True   True     True    True  \nfreq             20919     21567      16983  16076    18529   13954  \nmean               NaN       NaN        NaN    NaN      NaN     NaN  \nstd                NaN       NaN        NaN    NaN      NaN     NaN  \nmin                NaN       NaN        NaN    NaN      NaN     NaN  \n25%                NaN       NaN        NaN    NaN      NaN     NaN  \n50%                NaN       NaN        NaN    NaN      NaN     NaN  \n75%                NaN       NaN        NaN    NaN      NaN     NaN  \nmax                NaN       NaN        NaN    NaN      NaN     NaN  \n3.314627108010766\n50\n\n\n\nThe mean price per serving is approximately $3.31.\nThere are 50 unique flavors of Ben & Jerry‚Äôs Ice Cream in this Data Frame\n\n\n\nCounting\n\nprint(ice_cream['flavor_descr'].value_counts().head(10))\n\nflavor_descr\nCHERRY GRCA                   2097\nCHC FUDGE BROWNIE             1235\nCHC CHIP C-DH                 1070\nHEATH COFFEE CRUNCH           1070\nCHUNKY MONKEY                 1064\nPHISH FOOD                     968\nNEW YORK SUPER FUDGE CHUNK     932\nAMERICONE DREAM                865\nPB CUP                         828\nKARAMEL SUTRA                  738\nName: count, dtype: int64\n\n\n\nThis shows the top 10 flavors purchased.\n\n\n\nprint(ice_cream['race'].value_counts())\n\n\nprint(ice_cream['household_income'].value_counts())\n\n\nprint(ice_cream['household_size'].value_counts())\n\nrace\nwhite    18968\nblack     1522\nother      887\nasian      597\nName: count, dtype: int64\nhousehold_income\n80000     4758\n70000     2727\n110000    2337\n130000    2199\n180000    1265\n160000    1208\n170000    1198\n190000    1132\n210000    1021\n150000    1018\n60000      681\n50000      616\n230000     504\n40000      466\n260000     271\n240000     256\n280000     117\n300000     104\n310000      96\nName: count, dtype: int64\nhousehold_size\n2    8472\n1    5273\n3    3775\n4    2883\n5    1040\n6     282\n9     110\n7      91\n8      48\nName: count, dtype: int64\n\n\n\nThis shows the breakdown of demographic informations such as race, household income, and household size.\n\n\n\nFiltering and Grouping\n\nincome_flavor_counts = ice_cream.groupby(['household_income', 'flavor_descr']).size().reset_index(name='counts')\ntop_flavors_by_income = income_flavor_counts.sort_values(by=['household_income', 'counts'], ascending=[True, False]).groupby('household_income').head(3)\nprint(top_flavors_by_income)\n\n     household_income                flavor_descr  counts\n9               40000                 CHERRY GRCA      47\n11              40000               CHUNKY MONKEY      36\n7               40000               CHC CHIP C-DH      34\n54              50000                 CHERRY GRCA      56\n56              50000               CHUNKY MONKEY      46\n52              50000               CHC CHIP C-DH      44\n96              60000                 CHERRY GRCA     105\n88              60000             AMERICONE DREAM      33\n113             60000  NEW YORK SUPER FUDGE CHUNK      30\n140             70000                 CHERRY GRCA     226\n138             70000               CHC CHIP C-DH     163\n150             70000         HEATH COFFEE CRUNCH     143\n186             80000                 CHERRY GRCA     417\n197             80000         HEATH COFFEE CRUNCH     264\n184             80000               CHC CHIP C-DH     239\n234            110000                 CHERRY GRCA     239\n233            110000           CHC FUDGE BROWNIE     160\n232            110000               CHC CHIP C-DH     128\n279            130000                 CHERRY GRCA     220\n281            130000               CHUNKY MONKEY     143\n270            130000             AMERICONE DREAM     135\n325            150000                 CHERRY GRCA      88\n340            150000  NEW YORK SUPER FUDGE CHUNK      65\n323            150000               CHC CHIP C-DH      56\n367            160000                 CHERRY GRCA      99\n388            160000                  PHISH FOOD      77\n377            160000                HEATH CRUNCH      61\n409            170000           CHC FUDGE BROWNIE     124\n410            170000                 CHERRY GRCA     121\n419            170000         HEATH COFFEE CRUNCH      80\n456            180000                 CHERRY GRCA      96\n473            180000  NEW YORK SUPER FUDGE CHUNK      93\n466            180000         HEATH COFFEE CRUNCH      74\n500            190000                 CHERRY GRCA     112\n499            190000           CHC FUDGE BROWNIE      73\n510            190000         HEATH COFFEE CRUNCH      70\n545            210000                 CHERRY GRCA     138\n544            210000           CHC FUDGE BROWNIE     129\n567            210000                  PHISH FOOD      47\n588            230000                 CHERRY GRCA      53\n590            230000               CHUNKY MONKEY      50\n605            230000  NEW YORK SUPER FUDGE CHUNK      42\n629            240000           CHC FUDGE BROWNIE      28\n630            240000                 CHERRY GRCA      20\n647            240000                      PB CUP      14\n669            260000                 CHERRY GRCA      21\n681            260000               KARAMEL SUTRA      21\n682            260000              MAGIC BROWNIES      19\n728            280000                   W-N-C-P-C      34\n708            280000                 CHERRY GRCA      13\n707            280000           CHC FUDGE BROWNIE      12\n760            300000                   W-N-C-P-C      11\n737            300000                 CHERRY GRCA      10\n736            300000           CHC FUDGE BROWNIE       9\n768            310000                 CHERRY GRCA      16\n767            310000           CHC FUDGE BROWNIE       8\n789            310000                         VAN       7\n\n\n\nThis code shows the top 3 flavors purchased by each income bracket, revelaing potential income based preferences.\n\n\ncoupon_price_diff = ice_cream.groupby('usecoup')['priceper1'].agg(['mean', 'std'])\nprint(coupon_price_diff)\n\n             mean       std\nusecoup                    \nFalse    3.307155  0.649671\nTrue     3.377174  0.783962\n\n\n\nThis shows the average price and standard deviation for purchases made with and without coupons, indicating potential price sensitivity.\n\n\nregional_flavor_counts = ice_cream.groupby(['region', 'flavor_descr']).size().reset_index(name='counts')\ntop_flavors_by_region = regional_flavor_counts.sort_values(by=['region', 'counts'], ascending=[True, False]).groupby('region').head(3)\nprint(top_flavors_by_region)\n\n      region         flavor_descr  counts\n10   Central          CHERRY GRCA     403\n9    Central    CHC FUDGE BROWNIE     297\n12   Central        CHUNKY MONKEY     261\n58      East          CHERRY GRCA     405\n80      East           PHISH FOOD     267\n68      East  HEATH COFFEE CRUNCH     241\n104    South          CHERRY GRCA     693\n103    South    CHC FUDGE BROWNIE     496\n106    South        CHUNKY MONKEY     368\n152     West          CHERRY GRCA     596\n150     West        CHC CHIP C-DH     322\n162     West  HEATH COFFEE CRUNCH     315\n\n\n\nThis reveals the top 3 flavors purcahsed in each region\n\n\n\nVisualiations\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ntop_flavors = ice_cream['flavor_descr'].value_counts().nlargest(5).index\n\ntop_flavors_df = ice_cream[ice_cream['flavor_descr'].isin(top_flavors)]\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='flavor_descr', y='priceper1', data=top_flavors_df)\nplt.title('Price Distribution by Top 5 Flavors')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis visualization will show the distribution of prices for the top 5 most popular flavors, helping us understand if certain flavors tend to be priced higher or lower.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create the scatter plot with regression line\nplt.figure(figsize=(10, 6))\nsns.regplot(x='household_income', y='priceper1', data=ice_cream, scatter_kws={'alpha': 0.5})\nplt.title('Household Income vs. Price (with Regression Line)')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis visualization will show the relationship between household income and the price of ice cream, with a regression line to indicate the trend.\n\n\n\nLinear Regression\n\n\nX = ice_cream[['household_income', 'household_size']].fillna(0) # Filling na values with 0.\ny = ice_cream['priceper1']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n\ny_pred = model.predict(X_test)\n\n\nprint(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred)}\")\n\nMSE: 0.4228710318984661\nR-squared: 0.004728162675762815\n\n\n\nThe very low R-squared value suggests that your linear regression model is a poor fit for the data.\nHousehold income and household size, as used in this model, are not strong predictors of the price of Ben & Jerry‚Äôs ice cream.\nOther factors, not included in your model, are likely to have a much stronger influence on price.\nThis model is not useful for predicting the price of the ice cream."
  },
  {
    "objectID": "danl_proj_nba.html#salary-distribution-among-teams",
    "href": "danl_proj_nba.html#salary-distribution-among-teams",
    "title": "Data Analysis Project",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet‚Äôs start with the salary distribution among teams using seaborn for visualization. ‚Äã‚Äã\n\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n/var/folders/_m/d6jf0jhd2zzdfd5kzdhl_24w0000gn/T/ipykernel_79892/1671011424.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  nba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 16))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams.\nNotice that Portland Trail Blazers has the highest total salary followed by Golden State Warriors and Philadelphia 76ers, and Memphis Grizzlies has the lowest total salary."
  },
  {
    "objectID": "danl_proj_nba.html#player-age-distribution",
    "href": "danl_proj_nba.html#player-age-distribution",
    "title": "Data Analysis Project",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let‚Äôs explore the Player Age Distribution across the NBA. We‚Äôll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ‚Äã‚Äã\n\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\n# nba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\n# nba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n/Users/bchoe/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players.\nNotice that the majority of players fall within an age range from 24 to 34. There are few players whose age is above 40."
  },
  {
    "objectID": "danl_proj_nba.html#position-wise-salary-insights",
    "href": "danl_proj_nba.html#position-wise-salary-insights",
    "title": "Data Analysis Project",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we‚Äôll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let‚Äôs create a box plot to visualize the salary distribution for each position. ‚Äã‚Äã\n\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. PG-SG has the highest median salary."
  },
  {
    "objectID": "danl_proj_nba.html#top-10-highest-paid-players",
    "href": "danl_proj_nba.html#top-10-highest-paid-players",
    "title": "Data Analysis Project",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we‚Äôll identify the Top 10 Highest Paid Players in the NBA. Let‚Äôs visualize this information.\n\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'PlayerName',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  }
]