{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PySpark Basics\"\n",
        "author: \"Owen Ellick\"\n",
        "date: \"2025-02-14\"\n",
        "categories: [Data Analysis]\n",
        "image: \"PySpark.png\"\n",
        "execute: \n",
        "  warning: false\n",
        "  message: false\n",
        "\n",
        "toc: true\n",
        "---"
      ],
      "id": "f0d30c9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ![](PySpark.png)\n",
        "\n",
        "# Apache Hadoop\n",
        "\n",
        "## Introduction to Hadoop\n",
        "\n",
        "-   Definition\n",
        "    -   An open-source software framework for storing and processing large data sets.\n",
        "-   Components\n",
        "    -   Hadoop Distributed File System (HDFS): Distributed data storage.\n",
        "    -   MapReduce: Data processing model.\n",
        "- Purpose\n",
        "  - Enables distributed processing of large data sets across clusters of computers.\n",
        "  \n",
        "## Hadoop Architecture - HDFS\n",
        "- HDFS\n",
        "  - Divides data into blocks and distributes them across different servers for processing.\n",
        "  - Provides a highly redundant computing environment\n",
        "    - Allows the application to keep running even if individual servers fail.\n",
        "    \n",
        "## Hadoop Architecture- MapReduce\n",
        "\n",
        "- MapReduce: Distributes the processing of big data files across a large cluster of machines.\n",
        "  - High performance is achieved by breaking the processing into small units of work that can be run in parallel across nodes in the cluster.\n",
        "- Map Phase: Filters and sorts data.\n",
        "  - e.g., Sorting customer orders based on their product IDs, with each group corresponding to a specific product ID.\n",
        "- Reduce Phase: Summarizes and aggregates results.\n",
        "  - e.g., Counting the number of orders within each group, thereby determining the frequency of each product ID.\n",
        "\n",
        "## How Hadoop Works\n",
        "\n",
        "1. Data Distribution\n",
        "  - Large data sets are split into smaller blocks.\n",
        "2. Data Storage\n",
        "  - Blocks are stored across multiple servers in the cluster.\n",
        "3. Processing with MapReduce\n",
        "  - Map Tasks: Executed on servers where data resides, minimizing data movement.\n",
        "  - Reduce Tasks: Combine results from map tasks to produce final output.\n",
        "4. Fault Tolerance\n",
        "  - Data replication ensures processing continues even if servers fail.\n",
        "  \n",
        "## Extending Hadoop for Real-Time Processing\n",
        "\n",
        "- Limitation of Hadoop\n",
        "  - Hadoop is originally designed for batch processing.\n",
        "    - Batch Processing: Data or tasks are collected over a period of time and then processed all at once, typically at scheduled times or during periods of low activity.\n",
        "    - Results come after the entire dataset is analyzed.\n",
        "- Real-Time Processing Limitation:\n",
        "  - Hadoop cannot natively process real-time streaming data (e.g., stock prices flowing into stock exchanges, live sensor data)\n",
        "- Extending Hadoopâ€™s Capabilities\n",
        "  - Both Apache Storm and Apache Spark can run on top of Hadoop clusters, utilizing HDFS for storage.\n",
        "\n",
        "# Apache Spark\n",
        "\n",
        "## Spark\n",
        "\n",
        "- Apache Spark: distributed processing system used for big data workloads. a unified computing engine and computer clusters\n",
        "  - It contains a set of libraries for parallel processing for data analysis, machine learning, graph analysis, and streaming live data.\n",
        "  \n",
        "## Spark Application Structure on a Cluster of Computers\n",
        "\n",
        "- Driver Process\n",
        "  - Communicates with the cluster manager to acquire worker nodes.\n",
        "  - Breaks the application into smaller tasks if resources are allocated.\n",
        "- Cluster Manager\n",
        "  - Decides if Spark can use cluster resources (machines/nodes).\n",
        "  - Allocates necessary nodes to Spark applications.\n",
        "- Worker Nodes\n",
        "  - Execute tasks assigned by the driver program.\n",
        "  - Send results back to the driver after execution.\n",
        "  - Can communicate with each other if needed during task execution.\n",
        "\n",
        "# Spark vs. Hadoop\n",
        "\n",
        "## Hadoop MapReduce: The Challenge\n",
        "- Sequential Multi-Step Process:\n",
        "  - Reads data from the cluster.\n",
        "  - Processes data.\n",
        "  - Writes results back to HDFS.\n",
        "- Disk Input/Output Latency:\n",
        "  - Each step requires disk read/write.\n",
        "  - Results in slower performance due to latency.\n",
        "\n",
        "## Apache Spark: The Solution\n",
        "- In-Memory Processing:\n",
        "  - Loads data into memory once.\n",
        "  - Performs all operations in-memory.\n",
        "- Data Reuse:\n",
        "  - Caches data for reuse in multiple operations (ideal for iterative tasks like machine learning).\n",
        "- Faster Execution:\n",
        "  - Eliminates multiple disk I/O steps.\n",
        "  - Dramatically reduces latency for interactive analytics and real-time processing.\n",
        "\n",
        "# PySpark on Google Colab\n",
        "## PySpark = Spark + Python\n",
        "\n",
        "- pyspark is a Python API to Apache Spark.\n",
        "  - API: application programming interface, the set of functions, classes, and variables provided for you to interact with.\n",
        "  - Spark itself is coded in a different programming language, called Scala.\n",
        "- We can combine Python, pandas, and PySpark in one program.\n",
        "  - Koalas (now called pyspark.pandas) provides a pandas-like porcelain on top of PySpark.\n",
        "  \n",
        "## Spark DataFrame vs. Pandas DataFrame\n",
        "\n",
        "-  What makes a Spark DataFrame different from other DataFrame?\n",
        "  - Spark DataFrames are designed for big data and distributed computing.\n",
        "- Spark DataFrame:\n",
        "  - Data is distributed across a cluster of machines.\n",
        "  - Operations are executed in parallel on multiple nodes.\n",
        "  - Can process datasets that exceed the memory of a single machine.\n",
        "- Other DataFrames (e.g., Pandas):\n",
        "  - Operate on a single machine.\n",
        "  - Entire dataset must fit into memory.\n",
        "  - Limited by local system resources.\n",
        "  \n",
        "## Lazy Evalutation and Optimization\n",
        "- Spark DataFrame:\n",
        "  - Uses lazy evaluation: transformations are not computed until an action is called.\n",
        "  - Optimize query execution.\n",
        "- Other DataFrames:\n",
        "  - Operations are evaluated eagerly (immediately).\n",
        "  - No built-in query optimization across multiple operations.\n",
        "\n",
        "## Scalability\n",
        "- Spark DataFrame:\n",
        "  - Designed to scale to petabytes of data.\n",
        "  - Utilizes distributed storage and computing resources.\n",
        "  - Ideal for large-scale data processing and analytics.\n",
        "- Other DataFrames:\n",
        "  - Best suited for small to medium datasets.\n",
        "  - Limited by the hardware of a single computer.\n",
        "  \n",
        "## Fault Tolerance\n",
        "- Spark DataFrame:\n",
        "  - Built on Resilient Distributed Datasets (RDDs).\n",
        "  - Automatically recovers lost data if a node fails.\n",
        "  - Ensures high reliability in distributed environments.\n",
        "- Other DataFrames:\n",
        "  - Typically lack built-in fault tolerance.\n",
        "  - Failures on a single machine can result in data loss.\n",
        "  \n",
        "# PySpark Basics\n",
        "\n",
        "## The SparkSession Entry Point"
      ],
      "id": "f81dcfb4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "id": "77c5ecf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading a CSV file into the Spark Framework\n"
      ],
      "id": "d247b334"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path = 'https://bcdanl.github.io/data/df.csv'\n",
        "df = spark.read.csv(path, \n",
        "                    inferSchema=True,\n",
        "                    header=True)\n",
        "df.show()"
      ],
      "id": "633b2291",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df_pd = pd.read_csv('https://bcdanl.github.io/data/nba.csv')\n",
        "df = spark.createDataFrame(df_pd)"
      ],
      "id": "83b14bac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting a Summary of Data\n",
        "- df.printSchema(): prints the schema (column names and data types).\n",
        "- df.columns: returns the list of columns.\n",
        "- df.dtypes: returns a list of tuples (columnName, dataType).\n",
        "- df.count(): returns the total number of rows.\n",
        "- df.describe(): returns basic statistics of numerical/string columns (mean, count, std, min, max).\n"
      ],
      "id": "df1999e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.describe().show()"
      ],
      "id": "f92a2cef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting Columns"
      ],
      "id": "c3a43353"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single column -> returns a DataFrame with one column\n",
        "df.select(\"Name\").show(5)\n",
        "\n",
        "# Multiple columns -> pass a list-like of column names\n",
        "df.select(\"Name\", \"Team\", \"Salary\").show(5)"
      ],
      "id": "e0bca287",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counting Methods"
      ],
      "id": "c866a5fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Counting how many total rows\n",
        "nba_count = df.count()\n",
        "\n",
        "# Count distinct values in one column\n",
        "from pyspark.sql.functions import countDistinct\n",
        "num_teams = df.select(countDistinct(\"Team\")).collect()[0][0]\n",
        "\n",
        "# GroupBy a column and count occurrences\n",
        "df.groupBy(\"Team\").count().show(5)"
      ],
      "id": "ec58d84c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sorting Methods"
      ],
      "id": "f07c6cd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sort by a single column ascending\n",
        "df.orderBy(\"Name\").show(5)\n",
        "\n",
        "# Sort by descending\n",
        "from pyspark.sql.functions import desc\n",
        "df.orderBy(desc(\"Salary\")).show(5)\n",
        "\n",
        "# Sort by multiple columns\n",
        "df.orderBy([\"Team\", desc(\"Salary\")]).show(5)"
      ],
      "id": "e1af0af6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Sorting by One or More Variables\n"
      ],
      "id": "9c3a89b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# nsmallest example:\n",
        "df.orderBy(\"Salary\").limit(5).show()\n",
        "\n",
        "# nlargest example:\n",
        "df.orderBy(desc(\"Salary\")).limit(5).show()"
      ],
      "id": "0b960fdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Equivalent of Pandas nsmallest or nlargest\n",
        "\n",
        "## Indexing and Row Access"
      ],
      "id": "fc491e61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: filter by condition\n",
        "df.filter(\"Team == 'New York Knicks'\").show()\n",
        "df.limit(5).show()\n",
        "df.take(5)\n",
        "df.collect()"
      ],
      "id": "c59a8c87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding, Removing, Renaming, and Relocating Variables\n"
      ],
      "id": "300f2368"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add a column \"Salary_k\" using a column expression col()\n",
        "df = df.withColumn(\"Salary_k\", col(\"Salary\") / 1000) "
      ],
      "id": "2f43b87a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Adding Columns with withColumn()\n"
      ],
      "id": "c32666bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = df.drop(\"Salary_k\")  # remove a single column\n",
        "df = df.drop(\"Salary_2x\", \"Salary_3x\")  # remove multiple columns"
      ],
      "id": "36a51dc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Removing Columns with drop()\n"
      ],
      "id": "08c8c8b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = df.withColumnRenamed(\"Birthday\", \"DateOfBirth\")"
      ],
      "id": "3cfa181d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Renaming Columns with withColumnRenamed()\n"
      ],
      "id": "1a061bee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = df.select(\"Name\", \"Team\", \"Position\", \"Salary\", \"DateOfBirth\")"
      ],
      "id": "7b0ea85f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Rearranging Columns\n",
        "\n",
        "## Mathematical & Vectorized Operations\n"
      ],
      "id": "7e974d27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Summaries for numeric columns\n",
        "df.selectExpr(\n",
        "    \"mean(Salary) as mean_salary\",\n",
        "    \"min(Salary) as min_salary\",\n",
        "    \"max(Salary) as max_salary\",\n",
        "    \"stddev_pop(Salary) as std_salary\"\n",
        ").show()"
      ],
      "id": "6a544882",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Pre-compute the average salary (pulls it back as a Python float)\n",
        "salary_mean = df.select(F.avg(\"Salary\").alias(\"mean_salary\")).collect()[0][\"mean_salary\"]\n",
        "\n",
        "df2 = (\n",
        "    df\n",
        "    .withColumn(\"Salary_2x\", F.col(\"Salary\") * 2)    # Add Salary_2x\n",
        "    .withColumn(\n",
        "        \"Name_w_Position\",           # Concatenate Name and Position\n",
        "        F.concat(F.col(\"Name\"), F.lit(\" (\"), F.col(\"Position\"), F.lit(\")\")))\n",
        "    .withColumn(\n",
        "        \"Salary_minus_Mean\",        # Subtract mean salary\n",
        "        F.col(\"Salary\") - F.lit(salary_mean))\n",
        ")"
      ],
      "id": "664f11ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filtering by a Condition"
      ],
      "id": "9a871a3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df_pd = pd.read_csv(\"https://bcdanl.github.io/data/employment.csv\")\n",
        "df_pd = df_pd.where(pd.notnull(df_pd), None)  # Convert NaN to None\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "df.filter(col(\"Salary\") > 100000).show()"
      ],
      "id": "9d1f7fd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.filter(\n",
        "    ( col(\"Team\") == \"Finance\" ) & \n",
        "    ( col(\"Salary\") >= 100000 )\n",
        ").show()"
      ],
      "id": "ab5ec202",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.filter(\n",
        "    (col(\"Team\") == \"Finance\") | \n",
        "    (col(\"Team\") == \"Legal\")   | \n",
        "    (col(\"Team\") == \"Sales\")\n",
        ").show()"
      ],
      "id": "fb7dd269",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}